

Page 1
 
 
 
POLISH ACADEMY OF SCIENCE
BIOCYBERNETIC INSTITUTE
AND BIOMEDICAL ENGINEERING
 
 
 
 
Adam Jóźwik
 
 
 
 
UNPARAMETRIC SUPERVISED CLASSIFICATION METHODS
 
 
 
 
 
 
 
 
 
 
 
WORKS OF BIOCYBERNETIC INSTITUTE NR
WARSAW 2013 AND BIOMEDICAL ENGINEERING ISSN

Page 2
 
 
 
I extend my heartfelt thanks and expressions of gratitude:
 
 
 
 
Prof. Wojciech Zmysłowski is very insightful
Read the work, submit your comments and suggestions, thanks to
Many issues have been presented in a more transparent form.
 
 
 
Mr. Dominik Sankowski for his kindness and conveyance
Their comments in particular regarding the final separation of work.
  
 
 
Mr. Professor Mark Darowski for the encouragement to write this
Monograph.
 
 
 
Co-authors of my publications, thanks to which presented in monograph
Methods have been applied
 
 
 
To all those who have shown constant interest in the state
Work progress.
 
 
 
Adam Jóźwik

Page 3
 
 
 
SUMMARY
Monographs, for the most part of their volume, contain a description of the methods
Developed by its author. First general principles were presented
Constructing classifiers and methods of evaluating them. Much work is involved
Methods for determining the hyperplanes separating pairs of classes for use with
Structural multi-decision classifiers with a parallel structure composed of
Two-decision classifiers.
A well-known algorithm for error correction, iteration, is presented
Algorithm for determining the hyperplane that divides two sets and recursive
Algorithm for linear distributions of two sets published previously
By the author of this monograph.
The new result published for the first time in this paper is
Classifier constructed on the basis of a learning set, edited for linear
Separation of pairs of sets representing different classes. Another new result
Is an extension of the recursive algorithm to study the two linear distributions
A set that allows you to verify that the test files are linearly separated from
Set "clearance".
Most attention was devoted to the minimum distance methods, including
Fuzzy rules k nearest neighbors and construction method of classifiers
It consists in class area designation. A reduction algorithm was presented
Reference set consisting of pairs of objects coming closest to each other
And a new algorithm that uses an already reduced set of conversions
Artificial space of the characteristics, to further reduce it. Also a new proposal
Is an algorithm for condensing a set of references using artificial features.
The proposed revised, unpublished,
The nearest neighbors for the collection of learners with deficiencies
Values ​​of some characteristics.

Page 4
 
 
TABLE OF CONTENTS
 
1. INTRODUCTION............................................... ................................................. 5
1.1 General information ............................................. ........................ K ...... K ...... KR =
1.O Classifier Structure ....................................... .............................. S =
1.P Discriminatory Functions .......................................... ........................ KT =
The ideal classifier .................................................................. V =
1.R Problem of standardization of characteristics ......................................................... 10 =
2. METHODS OF CLASSIFICATION QUALITY ASSESSMENT ............................................................ .... NN =
2.N Estimation of probability of misclassification ................................ KKNN =
2.Obvious Matrix .................................... .............................. K .. .... NO =
2.P Fraction of errors as a criterion for selection of features ...... .. ........................ ......... NP =
3. DESCRIPTION OF HIPHBREAK DISTRIBUTOR ..................... NT =
3.N Error Correction Algorithm ............................................ NV =
3.O Iterative Algorithm for Determining the Optimal Hyperplane ................... OV =
3.P Recursive Linear Regression Algorithm ........................ ..PS =
3.4 Addition of the recursive algorithm .................................... 4O =
3.R Editing the learning set for linear division of sets ...... 4S =
4. MINIMUM-DISTANCE METHODS ................................................... .... RO =
4.N Minimum distance separator ........................................... RO =
4.O rule k nearest neighbors ........................................... .......................... RR =
4.P Construction of the classifier k-NS in the case of deficiencies of the characteristic values ​​..K ...... RV =
4.4 Blade rule k-NS ......................................................... ......... SR =
4.R Classification using class areas ................................. .KTN =
4.S Reduction of reference sets ..................... ... ..................... ............ TS =
4.T Condensation of reference sets ................................................KUR =
4.U Method of potential functions ............................................. ....... KKVN =
4.V New classification assessment scheme ....................................... .96 =
5. Summary and Perspectives ............... ........................................ .............. K ...... VT =
6. Cited literature ... .......................................... ........................ 104 =

Page 5
 
1. INTRODUCTION
 
1.1 General information
Classification of objects is the subject of research in the most commonly known field
Known as image recognition. Often, it is often said about recognizing patterns,
Whether to recognize objects. Objects are understood in a very general sense. They can
They are different objects, alphanumeric characters, sounds, optical images, states
Health, plants, animals, or people. It is assumed that the objects are described by a set
Features, also known as feature traits, can be numeric, qualitative or binary.
However, in this monograph, unless it is separately marked, it assumes
Be sure that features take numerical values, real or total. Vector features
Corresponds to points in the feature space, usually Euclidean. In the further
Considerations of the concept: object described by the set of features x, vector of the characteristic x, point x w
Feature space, will be used interchangeably for convenience. About points in the feature space
It will be most often referred to when referring to an interpretation
Geometrically, in the case of algebraic operations we will refer to feature vectors.
In this monograph, only classifications will be considered
Supervised, whose decision rules are derived from the set of learners to whom
Is a collection of objects with known class membership. Also, in this collection of counts
Classes should be consistent with the statistics of their occurrence. There is another kind
Classification, which is most often based on the function of similarity or distance
Between objects and uses a collection of objects whose affiliation is not
known. Classification task consists in dividing the set of objects into a certain
The number of subsets, set in advance or not, in such a way that objects from different
Subsets differed as strongly as possible, and objects within these subsets
Should differ from each other as little as possible. This is an unattended classification, which
Will be omitted in monograph.
In the recognition system, you can distinguish two basic blocks: block
Attribute extraction, ie feature extractor and classifier, as illustrated in Figure 1.1.
 
 
Fig. 1.1. General scheme of the recognition system
 
Extractor features
 
 
Object
Classifier
 
 
Set of Characteristics Decision
(Class) (Vector Features)

Page 6
The considerations in this paper will relate exclusively to
Classifier. The classifier implements a certain decision-making rule on the basis of which
One of the nc classes in question is indicated. The decision rule may relate
To many classes or only to two classes. Where used in
Classifier decision rule applies to only two classes or when applied
Multi-decision rule, but limited to pairs of classes, the classifier will be determined
As a two-decision.
 
1.2 Classifiers Structure
Two-decision classifiers can be used to construct classifiers
Multi-decision. One of the solutions is a parallel classifier folded back
Two-way classifiers are possible pairs of classes, and of course they are
Nc (nc-1) / 2. The structure of this classifier is shown in Figure 1.2.
 
 
Fig. 1.2. Parallelizer
 
Each component of the two-decision classifiers gives one vote
Two classes for which it was constructed. Parallel classifier, on the other hand
Points to the class that gets the most votes. Inconsistent cases, determined
Further for convenience as a draw, they can be settled in favor of the most common class
Occurring if a priori such information is available, random or to class with
A smaller index if the classes are numbered.
 In the case of more classes than two classes, the structure is possible
Hierarchical, eg two-step. In a two-step classification class can be
Grouped in macro-classes. In the first stage the classification is then made to
Macro-classes, and in the second stage the decision is made between classes of this macro-
The class to which the object was classified in the first stage. Especially
A macro-class can consist of only one class, which is sure to take place in
G
L
about
S
about
in
and
N
and
E
 
 
Pair of classes (nc-1) and nc
 
 
Pair of classes 1 and 3
 
 
Pairs of classes 1 and 2
 
 
 What?
Vector
Features x
Grade

Page 7
Case classification task involving three classes. Then some of the objects
It is already recognized in the first stage, and part is only in the second.
Classification steps in a hierarchical structure can be more than two. number
Possible hierarchical structures grow rapidly as the number increases
Considered classes. The nuisance of this effect will be less if in the first stage
A small number of macro-classes will be created, but will consist of more classes.
The next stage may have more macro-classes then, but less classes. IN
Only one class is present in the final stage.
It is easy to see that a hierarchical sequence of classification can be obtained from
Parallel structure. All you have to do is arrange the vote hierarchy.
It is also possible to have a multi-step structure in which each stage is resolved
Between a single class and the remaining classes of a given stage. For example, in
If the task for three classes, we can decide in the first step whether
The object belongs to class 1 or to a macro class composed of classes 2 and 3. Indication on
Class 1 finishes classifying, and macro-class indication activates the second stage, which
It only settles between classes 2 and 3. This case is often found in
Medical applications when class 1 is a healthy person and a class 2 and 3 person
Sick It is worth noting that for groups of sick people we have available to others
Sets of characteristics than the group of healthy people. Against the classifiers
Using one multi-decision rule, classifiers consisting of multiple two-
Decisive classifiers have the advantage that each classifier classifier
It may work on other sets of features, which may result in higher quality
Classification as measured by fraction or percentage of correct decisions. Also this multi-
A stepwise decision-making scheme can be implemented in the structure
Parallel, shown in Figure 1.2, if the vote of the classifier classifiers
Will be organized in a multi-stage manner. Multi-stage classification is devoted a lot
Places in the book [Kurzyński M., 1997].
 
1.3 Discriminatory Functions
The principle of the classifier is explained in Fig. 1.3., On which it is presented
The general construction of a classifier, which contains as many blocks of discriminatory functions as many
There are classes.
The object, or vector of attributes x, will be assigned to that class for which
The discriminatory function achieves the maximum value. Possible case of tie,
That is equal to the value of the discriminative function can be resolved in a similar
The manner in which constituent voting takes place
Parallel Classifier. In the simplest case, discriminatory functions
Can be linear.

Page 8
 
 
Fig. 1.3. General construction of the classifier
 
Each of the discriminative functions gj (x), j = 1,2, ..., nc, can be interpreted
As a function of the similarity of an object described by the x-vector of objects of class j
If the discriminative functions are linear, the classifier has the same structure
Shown in Figure 1.3, is called a linear machine.
 If the discussed classification problem concerns only two classes, then it is not
It is necessary to use two blocks of discriminatory functions. Instead of two
One function g12 (x) = g1 (x) -g2 (x) can be used for functions g1 (x) and g2 (x). The object will remain
Classified in class 1 when g12 (x) ≥0 and to class 2 when g12 (x) <0. Structure
Classifier then simplifies to the figure as in Figure 1.4.
 
 
Fig. 1.4. Structure of the two-decision classifier
 
The first block calculates the value of the discriminative function and the second determines the value of the discriminative function
The sign, that is, counts the signum function (g12 (x)) and if it accepts the value of +1, then the object
Is classified in class 1 and otherwise in class 2.
 From such two-decision classifiers you can build multi-
Decision making using the structure of Fig. 1.2. Then use the function gi, j (x),
I = 1,2, ..., nc-1, j = i + 1, and + 2, ..., nc. In the case of gi, j (x) 0 classifier
Cast a vote on class and, in case gi, j (x) <0, then on class j.
Considerations in tasks where the number of classes is greater than two classes,
The major focus will be on classifiers that use one rule
Multi-decision or classifier with parallel structure, as in Figure 1.2.
      Block g12
 +1
1
Object g12 (x) Class
M
and
K
S
and
M
at
M
 
 
 
F. Discrimination. Gnc (x)
 
 
F. Discrimination. G2 (x)
 
 
F. Discrimination. G1 (x)
 
 



Vector
Features x
Grade

Page 9
 
1.4 Classifier ideal
    Basic criterion for assessment of classification quality, excluding classification
The fuzzy, which will be described later in the paper, is the probability of misleading
Decision, estimated by fraction or percentage of misleading decisions. Against such an assumption
The classifier should indicate the most probable class, ie
Indicates the class and for which the condition is fulfilled:
                                                       P (i / x) = maxj p (j / x), (1.1)
Where p (j / x) is the probability of occurrence of class j, j = 1,2, ..., nc, under
The condition that the object is described is the attribute vector x. However, distributions
The probabilities p (j / x) for the classes under consideration j are not usually a priori known.
However, they can be directly estimated based on the set of objects that they are
Affiliation is known. Such a collection as mentioned above was mentioned in subsection
1.1, is called a training set or training set and serves to
Construction of the classifier, ie the decision rule.
If the density distributions of probabilities f (x / j)
Considered classes and probability a priori p (j) class instances were known, then
The probability p (j / x) could be calculated from the formula [Bayes T., 1973]:
                                                  P (j / x) = p (j) f (x / j) / f (x). (1.2)
The value of the function f (x) can be computed from the known pattern for probability
Total:
                                                  F (x) = p (j) f (x / j), (1.3)
However, this is not necessary because f (x) in the denominator of formula 1.2 can be omitted,
Because only the counter decides which of the probabilities p (j / x) will reach
Maximum value. However, in some tasks, the calculation of probabilities
P (j / x) may be desirable, and then f (x) must be estimated. Probability p (j)
And the density of probability distributions f (x / j), as was the case in
For conditional probabilities p (j / x), they can be estimated at
Based on the learning set. To estimate the probabilities p (j), it is necessary,
Classes in the teaching set are represented in proportions consistent with
Frequency of their occurrence.
So wishing to construct a decision rule, offering as little as possible
The percentage of mistakes can be estimated directly from the learning set
Probability p (j / x) or probability p (j) and distribution density f (x / j) and
Based on them, enumerate p (j / x). For features whose values ​​are purely
Integers, functions of the probability distribution function f (x / j) and f (x),
Nc
J1

Page 10
Occurring in formulas 1.2 and 1.3, should be replaced by the corresponding decomposition functions
Probabilities p (x / j) and p (x).
It is easy to see that a classifier operating according to the rule defined by relation 1.1 has
Structure as illustrated in Figure 1.3.
Not all of the methods that will be outlined in the further parts of this
Work, fall into the schemes described above. So far it has been assumed that
An object can only belong to one of the nc classes under consideration. However, one method,
Which will be proposed, concerns a classification called fuzzy. It consists in that,
The object may belong to varying degrees in each class. The result of the classification is
Then the blurred vectors of affinity v = [v1, v2, ..., vnc], where vj, j = 1,2, ..., nc,
Means the degree of belonging to the class j
In this case, it will accept values ​​in the range [0,0-1,0] and is calculated
By the formula:
             E = (, (1.4)
Where v = [v1, v2, ..., vnc] is the actual vector of the object belonging to
W = [w1, w2, ..., wnc] is assigned the membership vector,
Produced by the classifier. In a special case the vector components
Affiliates can take binary values, ie 0 or 1. In that case
We will deal with acute (not fuzzy) classification. The classification is acute
So special case of fuzzy classification. In further consideration, if
Will not mark what type of classification is considered, it is assumed that it is about
Acute classification.
 
1.5 Standardization problem
Features describing recognized objects can be expressed in different units
Measure. The type of units does not create any problems because they are still in the calculation
They do not take part, and only the abundance of these units represent the values ​​of the characteristics.
However, when some size is expressed in small units, it will
Represented in the emperor more than if expressed in large
Units. In the calculation, the meaning of those qualities whose values ​​are expressed
Larger numbers will usually be larger. For this reason, standardization is recommended
Characteristics. In addition, usually the results of classification using standardization features
Should be better. Two basic types will be considered below
Standardization, classical standardization and median standardization. Let the teaching set
Where xi = xi, xi, x, xi, n, and xi, j is the value of the jth i-th feature
Object. Then classical standardization follows the formula:
 2 /) ||
1

Nc
J
Jjwv m
I1

Page 11
         , (1.5)
Where mvj is the mean value of the jth characteristic and sdj its standard deviation. Both
These values ​​are calculated from the learning set. Characteristics j for which sdj is
Equal to zero are not standardized and should be omitted during construction
Classifier.
 Another kind of standardization expresses a similar pattern, with the difference that the mean
The value of the feature is replaced by the median mdj and the standard deviation of the median
Mddj of absolute values ​​of deviations from the median, ie:
        . (1.6)
Standardization will still be discussed in the further part of the work and though
The symbols of the mean value of the features and their standard deviations will be used
The applications received will also apply to median standardization, if not
This will be marked separately.
 
 
2. METHODS OF CLASSIFICATION QUALITY ASSESSMENT
 
2.1 Estimation of probability of misclassification
 Method of the test set
 The most natural method for estimating the probability of misleading decisions
Classifier, is the use of a separate test set T, ie set of objects,
Whose affiliation is known. All objects from the set T of which mT are
Classified by the decision rule derived from the teaching set U. Number of eT
Misclassified objects divided by the mT number of all objects
The test set T, ie the error fraction erT = eT / mT is the estimate
Probability of misclassification. A fair assessment of the classifier requires that
The test set was used exactly once. Usually we have one
A collection of objects of known class affiliation and use of this method requires
Divide this set of objects into the learning part U and the test part T. The problem
In this method there is a ratio between the size of the learning part U and the test piece T.
1x validation method
 This is probably the most commonly used method of probability estimation
Misclassification. The set U, having the population of mU, is divided into as equal as possible
Parts of Tl. Each of these l parts plays the role of the test set and the sum
The remaining parts, ie the set UTl is used to construct the classifier.
In this method, which requires the construction of a classifier of 1-fold, each of jjjisjisdmvxx /) (" jjjjjddmdxx /) (" 

Page 12
The objects of the learning set U are classed once. If eU is wrong
Classified as erU = eU / mU is the probability estimate
Mistaken decision of the classifier.
As we can see, this method consists in applying multiple methods of collection
Tester. The l parameter is smaller, the smaller the difference between
The number of collections UTl and the collection U. One can expect that
Classifier based on the set U will offer a smaller fraction
Wrong decisions than a classifier constructed using any of its
Subset UTl. It is expected that the fraction of errors calculated by this method is expected
It will be inflated, in this more significant degree the number l will be smaller. FROM
On the other hand, large values ​​of l require more calculations, because they are
Construct more classifiers. When choosing l should be taken into account
Computing cost of constructing the classifier once. Classification of new
Objects, ie objects outside the learning set, will be based on
Classifier built using the whole set U. Most commonly accepted
The value is l = 10 [Kohavi R., 1995]. You can also divide the dataset into 5 less
More equal parts and each one once used as a learning set and a second time as
Test set.
 Method minus one element
 This method is a special case of the 1-fold validation method if
Let us assume l = 1. It consists in classifying each object u and from the set U,
Containing mU objects, using a derived classifier
U {ui}. If we assume that the number of errors is eU, the fraction of erU = eU / mU is
Estimate the probability of the wrong decision. Use this method recommended
It is especially in cases of small learning sets, especially when
Construction of the selected type of classifier requires only a small amount of effort
Calculations.
Its drawback is the need for multiple construction of the classifier. Method
Minus one element has become popular since 1965 [Lachenbruch
P.A., 1965]. However, the authors of [Devijver P.A., Kittler J., 1982] note that
This method was previously used by Russian authors, but they did not succeed
Determine who was the originator of this method.
 
2.2 Misalignment matrices
The error fraction is the standard criterion for rating the classifier, but in some cases
Applications of the type of mistakes are also desirable. More accurate
The information would include a one-liner erode error matrix where eri} {1inc
Ier

Page 13
Denotes the fraction of mistakes made among objects of class i. From these fractions, knowing
The number of classes I can calculate is the global error fraction er = mjeri / m, where m
Number of objects classified.
You can also use the above method of classification quality assessment to designate
A numerical matrices of R = where ri, j is the number
Objects in class and classed in class j. The definition of matrix R shows that
The relation of numbers to classes and in the set of learners with elements r and j is as follows:
Mi =, and the number of objects l classified into class and can be determined from the formula:
Li =. Based on the R matrix, it is easy to estimate the probability pi, j,
That the object in class i will be classed as j and the probability qi, j that
Object classed to class and actually derived from class j. Compound matrix
P = and matrix Q = with the matrix R are: pi, j = ri, j / mi and
Qi, j = rj, and / li.
The probability matrix Q has a greater practical significance than the matrix P,
Because it tells us the reliability of qi, j with which classification is made, if the object is
Class M, P, and Q will be referred to as the matrices,
I, II and III kind.
     
2.3 Fraction of Errors as Criteria Selection
The error fraction is the most commonly used criterion during construction
Classifier, which should also include selection of features. Some features may not
Have no relation to the classes under consideration and the part may contain information
Redundant. Such characteristics not only irritate the synthesizer of the classifier, but moreover
They may deteriorate the quality of the classification. This fact explains the simple example given in Fig.
2.1. In the situation as in the drawing, it is assumed that each class occupies the square area and is
Represented by 3 points in the feature space. In this example and in others, in
The rest of the work, the crosses will be represented objects of class 1, and the wheels with
Class 2
To build a classifier, it is rational to assume as hypermarket
Separating, the broken line being the geometric point of the points evenly
Distant from each of these classes. If two x and y are used, then it is obtained
There will be a broken line H2 separating the two classes. Then points in class 1
(Crosses) in Area A will be misclassified to Class 2 (circles). 
Nc
J1} {1, jinc
Jir 
Nc
Jjir1, 
Nc
Jijr1,} {1, jinc
Jip} {1, jinc
Jiq

Page 14
            
  Fig. 2.1. Illustration of a situation in which redundant features may harm
 
Points from Class 2 lying in areas B1 and B2 will be wrong
It would be much better to use only one feature in this case
X. Then, in the one-dimensional space, which is the x-axis,
The hyperfunctional partition would be the point of intersection of the straight H1 with the x axis.
The presence of superfluous features does not compromise the quality of classification, unless it is
Used for the construction of the classifier. This unfavorable effect,
The redundant traits may be detrimental because of the insufficient number of learner sets.
If areas A were filled with multiple points in class 1 and areas B1 and B2 large
The number of points in the class 2, the redundant characteristics at most do not improve the quality
Classification, but also could not hurt her significantly.
Another interesting situation is that the features that are used
Individually, they have no meaning in the set, they can even be ideal
Combination of features, as illustrated in Figure 2.2.
 
                     
Fig. 2.2. Non-singular features can individually create a good set
 
X
X
X
about
O o
X
X
about
about
Y
X
about
X
X
X
X o
about
about
X x x o o
Y
X
about
AND
B1
B2 Class 1
Class 2
H2 H1

Page 15
After scoring the points on both the x-axis and the y-axis of the class, it is very difficult
Separating and using any of these features singly does not make sense. Use
Both of these characteristics simultaneously offer very good quality of classification.
These simple examples confirm the desirability of taking into account the selection of features during
Construct a classifier. The originator of both drawings, Fig. 2.1 and Fig. 2.2 is
Mgr Wojciech Siedlecki, former charge of the author of this monograph, as
Master thesis promoter.
The perfect selection of features could consist in reviewing all the possible
A combination of n features, estimating for each of them the probability of misleading
Decide and choose this combination that offers the smallest value of the above
Probability.
However, it should be noted that the number of possible combinations of features quickly
Grows with increasing number of available attributes and it is 2n-1. For this reason
Use more computationally intensive review procedures only some
Combination of features [Devijver P.A., Kittler J., 1982]. They relate to the next attachment
Features, rejection of features, or combination of attachment and rejection.
 In the case of the procedure of the next attachment of the characteristics must first be determined
The characteristic for which the error fraction reaches the minimum value. Then to this feature on
All possible ways include the other feature and the verified ones
This way couples of characteristics are chosen by the couple, offering the smallest fraction of errors. IN
A similar way is created by three characteristics, four and larger sets of features, ending in
A full set of n features available. Example of the course of selection of features from
The application of the procedure for the subsequent attachment of features is shown in Table 2.1.
 
Tab. 2.1. An example of the procedure of the next attachment of features
(1) 0.50 (4.1) 0.40 (4.3.1) 0.20 (4.3.5,1) 0.10 * (4,3,5,1,2) 0, 20*
(2) 0.30 (4,2) 0,50 (4,3,2) 0,25 (4,3,5,2) 0,15 
(3) 0.40 (4.3) 0.15 * (4.3.5) 0.10 *  
(4) 0.20 * (4.5) 0.30   
(5) 0.30    
 
Character sets are given in parentheses, and next to them fractions of errors that these
Sets they offer. Stars have marked the best feature sets in the phases
Subsequent additions. Of all the combinations of features, the one for whom
The fraction of errors is the smallest. In the example given, this will be a combination of
Features 3, 4 and 5 offering error fractions er = 0.1. The same fraction of errors was

Page 16
Calculated for Character Set 1, 3, 4 and 5, but it is better to choose a smaller set
Number of features.
 Using the procedure of the next rejection of features, the first verified set
Features include all n features. Then on all possible ways thrown out
Is one of the features, ie it evaluates combinations of n-1 features and is selected
Such a combination for which the smallest fraction error value is obtained. IN
A similar way of the selected combination is made up of combinations of n-2 features, etc.,
Ending with a combination of only one feature. Operation of this procedure
Shown in Tab. 2.2. This time the selected set of features turned out to be a combination
Composed of two traits, 3 and 5, which offers error fractions er = 0.06. Examples of both
The presented procedures for viewing feature sets come from the work [Jóźwik A.,
2005]
 
Tab. 2.2. An example of the procedure for the next rejection of features
(1,2,3,4,5) 0,20 * (2,3,4,5) 0,15 (3,4,5) 0,10 * (3,4) 0,15 (3) 0 , 40
 (1,3,4,5) 0,10 * (1,4,5) 0,15 (3,5) 0,06 * (5) 0,30 *
 (1,2,4,5) 0,20 (1,3,5) 0,25 (4,5) 0,30 
 (1,2,3,5) 0,25 (1,3,4) 0,20  
 (1,2,3,4) 0,15   
 
It is also possible that the procedure is a combination of both of the above described
Procedure review features. Its course begins with the same procedure as the procedure
Another feature attachment.
Starting with the selection of the best three traits, the procedure is followed
Rejection features and continue as long as the kits are obtained
Features, containing the same number of features as the sets received so far, but offering
Smaller fractions of errors. Because this procedure is initially the same
The way, like the procedure for adding features, is just to illustrate its course
Only when a set of three features is temporarily selected.
A description of this procedure is provided in Table 2.3.
 
Tab. 2.3. Example of a combined procedure
(4.5) 0.30 (3.5.1) 0.25 (2.3) 0.08 (1.2.5) 0.15 (1.2.4.5) 0.20 , 2,3,4,5) 0,20 *
(3,5) 0,06 * (3,5,2) 0,05 * (2,5) 0,04 * (2,3,5) 0,20 (2,3,4,5) 15 * 
(4,3) 0,15 (3,4,5) 0,10 (3,5) 0,06 (2,4,5) 0,10 * 

Page 17
The first column of Tab. 2.3 contains sets of two attributes obtained after
Discard one feature from the set of features 3, 4 and 5 obtained by a sequential procedure
Include features as shown in the first three columns Tab.
2.1. The best of these two qualities is a pair of qualities 3 and 5. To this pair for all
Possible ways include one of the other features. The best of yes
The three characteristic traits obtained consist of characteristics 2, 3 and 5 (column 2). Now again
The rejection of the traits is carried out and as a result, a set of traits 2 and 5 are found,
For which the smallest fraction of errors has been achieved so far.
Joining the two features of the third did not improve because they received
The best of these triangles, featuring features 2, 4 and 5, is not better than before
Obtained triple of features 3, 4 and 5, because both these three offer error fractions
Er = 0.1. If it were otherwise, then another rejection would be required
Characteristics.
 There are no general rules indicating which of the two basic procedures
You must apply, include, or reject features. In cases where the number
Available n features are large, comparable to the size of the learning set,
The use of the procedure for rejection of features may not be possible because they may
Numerous sets of features are offered that offer the same error fractions.
It can happen because when the number of features is large, the rejection of one,
Whatever feature, it may not change the value of the error fraction at all.
Then it is more rational to apply the next attachment of features or
Combined procedure, although the latter is much more complex than the next
Attaching features. Ambiguous situations can be settled in different ways, for example
The advantage of a feature that individually offers fewer fractions of error or rejection
It would cause the largest increase of error fractions.
 
 
3. DESCRIPTION OF THE DISTRIBUTION HIPHOP
The linear discriminative function can be used in a decision rule
Referring to two classes, it can also be a very useful tool
To construct multi-decision classifiers using structure
Parallel, as shown in Figure 1.2.
Works on algorithms for determining hyperplanes separating two
The sets X1 and X2 in the feature space have been running for a long time [e.g. Nilsson N.,
In 1965, one of the simplest algorithms was proposed
This is a heuristic algorithm which, assuming linear distribution of sets,
Provides a divisible hyperplane in a finite number of steps.

Page 18
Particular attention should also be paid to the algorithm presented in the paper
[Koziniec B.N., 1973], consisting in looking for a pair of nearest points, of which
One belongs to the convex shell of the set X1 and the other to the convex shell of the set X2.
Yet other algorithms are recursive and rely on rotation
Hyperplanes in the n dimensional space of features around the n-1 dimensional axis [Jóźwik
A., 1983a; Jóźwik A., 1998a; Cendrowska D., 2005; Sturgulewski L., 2008]. Big
A group of algorithms for determining the dividing hyperplane is of character
Gradient as shown in the works [Ho Y., C., Kashyap R. L., 1965; Booby
R., O., Hart P.E., Stork D., G., 2001].
Algorithms for determining the dividing hyperplane are used most often
Linear minimization approach [Bobrowski L., Niemiro W., 1984],
A square goal function [Mangasarian O.L., 2000; Vapnik W.N., 2000], z
Linear constraints. Among these methods are particularly popular
Nowadays, the methods referred to as support vectors machines (SVM) by
Vapnika. In his earlier work similar approach was determined by the method
Generalized portraits [Vapnik W.N., Chervonenkis A. J., 1974]. To find
Divide the hyperplane, by minimizing the quadratic function of the goal, in
The proposed method was the conjugate gradient method. Method
Generalized portraits let you decide whether or not collections are linear
Separate, and if so, to designate an optimal hyperplane in the sense of its
Distance from the nearest point from the separated harvest.
 Currently, in conjunction with the method of supporting vectors, it is used
Lagrange multipliers are more often used. In the most difficult case, when the harvest fails
Are linearly separable, the penalty coefficient is introduced into the objective function, which is
Selected experimentally. Another option that can be applied
Also with other methods of determining the separation hyperplane, is
Transformation of distributed collections into a new feature space in which these collections
They may already be linear. A similar way was already suggested at work
Nilsson [Nilsson N., 1965] using the function i (x), i = 1,2, ..., N. Vectors x describing
Objects in the space of n dimensional features, by applying the function i (x) are
Transformed into vectors  (x) = [1 (x), 2 (x), ..., N (x)] in the dimension N space.
The functions i (x) should be chosen in such a way that the linear features in the new space
Separation of collections was easier.
Ideal algorithm for constructing a linear discriminative function
Would be an algorithm that would type the minimum subset of points from a set
X = X1X2, after which the subtracted sets X1 and X2 would be
Already divide the hyperplane, which is equivalent to constructing such
Hyperplanes, so that the number of points from the set X = X1X2 lying on its wrong

Page 19
The site was minimal. The solution of this task need not be of course
Unambiguous
In this monograph the reflections will be devoted to the chosen, less
Complex methods of determining the dividing hyperplane from which
Some have been suggested by the author of this monograph.
 
3.1. Error Correction Algorithm
Linear distribution of subsets of the learning set representing different classes,
It can very simplify the design of the classifier. The hyperplane can be separated
Only two collections, but this fact need not be an obstacle to construct a classifier
Multi-decision, as already explained earlier in Figure 1.2.
In addition, linearly inseparable sets X1 and X2 can be transformed into a new one
Space of features, usually of a higher dimension, in which they can already be separated
Hyperplane.
The linear (strict) separation of two sets X1 and X2 in the feature space means
Meeting the following conditions:
G (x)> 0 when xX1
And (3.1)
 G (x) <0, where xX2,
Where g (x) = + wn + 1 and x = [x1, x2, ..., xn]. The discriminative function g (x) can be
Express it succinctly that w = [w1, w2, w3, ..., wn]. Then g (x) = wx + wn + 1, a
The conditions (3.1) can be written as:
In ° x + wn + 1> 0, where xX1
                                                                   And (3.2)
In ° x + wn + 1 <0, when xX2
If the set X = X1X2 from the space En is transformed into a set Y in space
En + 1 according to the formula:
Y = h (x) = [x1, x2, ..., xn, 1] when xX1
And (3.3)
Y = h (x) = [- x1, -x2, ..., - xn, -1], g dx xX2
And a (n + 1) -dimensional vector v = [w, wn + 1] = [w1, w2, w3, ..., wn, wn + 1]
The inequality (3.2) will be transformed into:
When yYY, (3.4)
Where Y = h (X). The transformation operation (3.3) is illustrated in Figure 3.1. 


N
J
Jjxw
1

Page 20
 
 
 
                
Fig. 3.1. Illustration of transformations 3.3
 
Objects x1, x2, x3 and x4 lying on a straight line, ie in a one dimensional space,
Have been converted into two-dimensional space. Linear distribution of sets X1
And X2 means that y = y1, y2, y3, y4} lies on one side of a certain
A hyperplane passing through the origin of the coordinate system in space
Two-dimensional.
The vector v for which the relation (3.4) holds, uniquely defines a linear function
Discriminatory g (x) meeting the conditions (3.1). In the work of Nilsson [Nilsson N., 1965]
The transformation of (3.3) of the set X = X1X2 into the set Y and in the set is given
The simple, lower-cited, algorithm for finding the vector v being the solution
Inequality system (3.4).
 Based on the set Y, an infinite sequence (yk) is created, in which
Each element of set Y is infinite number of times. If
Y = {yk}, then the sequence (yk) can take the form: (y1, y2, ..., ym, ym, y2, ..., y2 • m, ...)
Where yi • m + k = yk, for any integer i. Based on the string (yk)
A vector string (vk) is created according to the following rule.
Definition of the algorithm:
 V0 = [01.02, ..., 0n + 1], ie it is a (n + 1) dimensional zero vector;
 Vk = vk-1, when vk-1kk> 0; (3.5)
 Vk = vk-1 + a ° yk, g d y vk-1 ° yk≤0,
Where a is a positive number, which is called the correction factor and is
Selected experimentally so as not to exceed the permissible range of numbers,
Usually, however, it is sufficient to accept in the definition of (3.5) the algorithm that a = 1.
-3
-2
-1
0
1
-2 -1 0 1 2 3 4 5
X x
X2 x1
X x
V = [1, -3]
O o
O o
X4 x3

Y4 y3
Y1 y2 1k m
K1 1k 1k 1k

Page 21
Also note is the version of this algorithm with correction factor
Absolute, in which the correction factor changes at every iteration step by
Ak = -vk-1 ° yk / (yk) 2. It may also be the smallest integer greater than the number
-vk-1 ° yk / (yk) 2. The factor ak is positive because vk-1k is negative.
The absolute correction factor guarantees that after the correction of the considered point yk da
Non-negative scalar product vk ° yk.
The principle of the error correction algorithm follows from the observation that
That is, after correction, the value of the scalar product,
Corresponding to the vector yk, is magnified. Such a change goes in desirable
Direction because the purpose of using the algorithm is to find such a vector v, which
With each element of the set Y gives a positive scalar product as defined by the condition
(3.4). Correction increases the value of the product from vk-1 ° yk to vk ° yk, but still the new value
This product for yk can be negative if the coefficient is not applied
Absolute correction. The absolute correction factor brings this product
Scalar at least to zero, and if before the correction was equal to zero, then to value
Positive.
In the aforementioned book, the following statement is given.
Theorem 3.1
If the sets X1 and X2 are linearly separable, then the algorithm will find a solution
Inequality (3.4) in the finite number of correction steps.
It is worth pointing out the proof of this claim, as in the later part of monograph
A modification will be described, the convergence of which will be proven in a similar way
way.
Evidence
For the sake of simplicity consider that a = 1. In addition, it is convenient
Assume that there are only elements in (yk) that need to be corrected. While
Assumption: vk = y1 + y2 +. . . + Yk, and multiplying both sides by the vector v being
Solution of the system (3.4) whose existence results from the assumed linear distribution
The sets X1 and X2 show that
                                   V v = = v y, (3.6)
Where b = v> 0. Cauchy's inequality shows the relationship v2 ° v (v ° vk) 2, and after
Taking into account (3.6) the following inequality follows:
V (v ° vk) 2 / v2k2 ° b2 / v2. (3.7)
On the other hand, it is clear from the definition of the algorithm that 1k Yymin 2k 2k

Page 22
V = (vk-1 + yk) 2 = v + 2 ° vk-1 ° yk + y, but 2 ° vk-1 ° yk0, since by assumption vector yk
Required correction, therefore
       Vv + y v + y + y ... yk ° c, (3.8)
Where c = y2.
By associating with (3.7) and (3.8) it is found that k2 ° b2 / v2 v k ° c, ie k2 ° b2 / v2k ° c, and hence
You can already get a restriction on k:
 K c ° v2 / b2, (3.9)
What ends the proof of the theorem.
A priori is not given any indication whether the sets X1 and X2 are linearly separable,
or not. Therefore, the algorithm could have been stopped for some time
Limit the number of iterations. Iterations of this algorithm are very costly
Computable, so it's worth doing a number of experiments with increasing acceptance
Larger numbers limit the number of steps, and the number of objects y
From the set Y that satisfy the condition (3.4). As restrictions can be taken
Numbers are a multiple of the number m of objects in the set Y. If, several times
Increasing the number of iterations, in another experiment, will not bring positive
The result, that is to obtain the solution of the system (3.4), we can assume that the sets are
Linearly inseparable or difficult to separate, ie too close to each other to achieve
success.
A case of linear non-separation
Although the convergence of this algorithm was only shown for sets X1 and X2
Linearly distributed, it can be useful also in the case of their linear
Inseparability Just after every correction in step k check for how many
The elements yi, i = 1,2, ..., m, from the set Y true is the inequality vk ° yi> 0 and
Remember this vector v * from the vectors v i, i = 1,2, ..., k, for
Whose number of such vectors is the largest. It strives to receive this
The hyperplane, defined by the components of the vector v *, which correctly distributes
Possibly the largest number of objects from the sets X1 and X2. Use of such
Proceed to the known Iris Data collection (http://archive.ics.uci.edu/ml/datasets.html,
3 classes, 4 features and 150 objects), along with the parallel structure as in Figure 1.2. from
In Chapter 1, it results in a classifier who in this set is confused only 3 times what
Means the error fraction e = 0.02. The number of iterations for pairs of classes (1,2), (1,3) and (2,3) was
201, 201 and 8701, respectively. The accepted restriction on the number of iterations, ie, the presentation
The vectors from the set Y for each pair of classes were 10000. 2k 21k 2k 2k 21k 2k 22k 21k 2k

K
J1 2j Yymax 2k

Page 23
  However, it may happen that the designated separating hyperplane, o
By the equation g (x) = 0, it will run at a very short distance as shown in
Fig. 3.2a, from objects from the set X = X1X2, which is not indicated.
 
 
 
 
 
 
                    X o
    X x o
X o x o
           O x o
   X o x
         O o x o o
 X o
                              
                              
 
Fig. 3.2. Illustration of the modification of the error correction algorithm
 
However, the error correction algorithm can, however, be modified to be compatible with its
To find a pair of parallel split hyperplanes, gA (x) = 0 and
GB (x) = 0, 2 ° apart from each other, as illustrated in Figure 3.2a.
This generalization, other than described below, has been suggested in the book
[Duda R., O., Hart P.E., Stork D., G., 2001].
 This type of resolution is referred to as the luminous separation ps = 2 °  or
As separation with margin . Intuition suggests that after the two-
Defined by the discriminative function gC (x) = [gA (x) + gB (x)] / 2 (division
By 2 is not necessary) you can expect smaller fractional errors
Classification than when the discriminative function g (x) is used. If you assume that
GC (x) = w ° x + wn + 1 = 0, it is easy to show that both parallel hyperplanes,
The hyperplane gC (x) = 0 o , is defined by the equations: gA (x, ) = w ° x + wn + 1 ° | and
GB (x, ) = w ° x + wn + 1 +  ° | w | where | w | Means the length of the vector in.
Indeed, let the points a, b in Figure 3.2b correspond to the hyperplanes
Passing through points a and b parallel to the hyperplane gC (x) = 0,
Passing through point c, they express themselves equations of form:
GA (x) = in x + wn + 1 + a = 0 and gB (x) = in x + wn + 1 + b = 0. Vector a = c +  ° w, hence | a-c | =  ° | w | = ,
Ie  =  / | w |. Similarly b = c- ° w. The vector a satisfies the equation gA (a) = w ° a + wn + 1 + a = 0,
Because the corresponding point a lies on the hyperplane gA (x) = 0, that is
GA (x, ) = 0 gB (x, ) = 0
 
  
G (x) = 0 gC (x) = 0
2
A) b) w
in
GA (x, ) = 0 gB (x, ) = 0
 
  
2
GC (x) = 0



and
B c

Page 24
GA (a) = w ° (c +  ° w) + wn + 1 + a = w ° c + wn + 1 + a +  w2 = a +  ° w2 = 0 because the first two
Components are the number of gC (c), and this is zero because the point c lies on
Hyperplane gC (x) = 0. Therefore a = - ° w2 = - ( / | w |) ° w2 = - ° | w |.
So also gB (b) = w ° (c- w) + wn + 1 + b = w ° c + wn + 1 + b- w2 = b- w2 = 0,
Hence b =  ° w2 = ( / w |) ° w2 =  ° w |, since w ° c + wn + 1 = gC (c) = 0.
A pair of functions gA (x, ) and gB (x, ) should satisfy the following systems
Inequality:
GA (x, ) = in ° x + wn + 1- ° | when xX1
And (3.10)
    gB (x, ) = w ox + wn + 1 +  ° | w |    gdyxX2
After multiplying both sides with the above inequality by -1, the system (3.10) will be
Had the form:
                                          In x + wn + 1- ° | w | when xX1
                                                      And (3.11)
                                          In ° (-x) + wn + 1 ° (-1) - ° | w |    gdyxX2 w
 
Now using the mapping (3.3) of the set X = X1X2 in the set in Y and remembering that
V = [w, wn + 1], the inequality (3.11) will take the form:
     When yyy. (3.12)
Modification I
 V0 = [01.02, ..., 0n + 1], ie it is a (n + 1) dimensional zero vector;
 Vk = vk-1, when vk-1 kkk-k k k-1 0 0; (3.13)
 Vk = vk-1 + a ° yk, g d y vk-1 ° yk- ° | in k-1 | ≤0.
Theorem on this modification, equivalent to Theorem 3.1
It will now sound:
Theorem 3.2
If the sets X1 and X2 are separated with the ps = 2 ° prze clearance, that is, the margins , then
Algorithm acting modifying I, finds the vector v satisfying the system
Unequal (3.12) in finite steps, and components of this vector
They explicitly specify the functions gA (x, ) and gB (x, ) satisfying the conditions (3.10).
Proof of this assertion is very easy and similar to that already mentioned above
Proof of original version assertion. However, for comparison of this
Modifications with the original version of the algorithm will be listed below.

Page 25
Evidence
 In place of the relation (3.5) vector v, satisfying the arrangement (3.12), under the assumption that
Sets X1 and X2 are separated with a ps = 2 °  clearance, exists. This time instead of the relationship
(3.6) it fulfills the following condition:
                               V (b +  ° | w |), (3.14)
Where as in the original version b = v? Y. It is obvious that b>  ° w, because of
3.12 shows that v y y-    0 when yYY. Hence, in relation 3.9 in place of the number b
You must substitute the number (b +  ° w), resulting in a new limit on the number
Steps k:
         K c ° v2 / (b +  ° | w |) 2. (3.15)
 You can therefore assume that the number b is the same number that occurs in the relation
(3.9). However, it is worthwhile to realize that in the strict sense this is not true.
In the case of the original version, it was assumed that each product was identical
Unknown vector v, whose existence results from the assumption of linear distribution
The sets X1 and X2, with each object in the set Y greater than zero. And in this
Modification of this threshold is not zero but the number of  ° w. In other words, at
Assuming the same distance points of the set X from the hyperplane
Separating for the original version of the algorithm as for the distance of these points from
The closest of the two divider planes in the modified version, the numbers
B in relation (3.9) and (3.15) would indeed be the same. Right side value
The relation (3.15) would be, with the above assumption less than in relation (3.9),
Because the denominator is larger by  ° | w | which means less correction
Algorithm. This observation is intuitively understandable, as the larger clearance
Between sets X1 and X2 implies a faster resolution of the solution.
 Let v0 be a solution of the system (3.4) or (3.12), whichever one
The algorithm version will be applied, original or modified. Applying
The modified version does not guarantee that the smallest scalar product v0,
Where yYY will be larger than the original version but with a linear assumption
The distribution of the ground clearance ps = 2 °  is a guarantee that it will be larger than
 ° | w |.
After obtaining solution 3.12, the two-decision classifier will be defined
Finally, by the hyperplane gC (x) = [gA (x, ) + gB (x, )] / 2 = 0, illustrated in Fig.
3.2a. However, there are no obstacles to using both divider planes.
Objects x for which gA (x, )> 0 would be qualified for class 1, and objects for
Whose gB (x, ) <0 to class 2 and the remaining one between
The hyperplanes gA (x, ) = 0 and gB (x, ) = 0, which satisfies the conditions Yymin

Page 26
GA (x, ) 0 and gB (x, ) 0 can be assigned a decision I do not know or a fuzzy decision
Corresponding to the participation of objects of both classes in the belt between
The hyperplanes gA (x, ) = 0 and gB (x, ) = 0.
 Likewise, one can consider the linear distribution of sets with a 2 ° overlay, like this
Shown in Figure 3.2b. This time the functions gA (x, ) and gB (x) should satisfy
The following inequalities:
GA (x, ) = w ° x + wn + 1 +  ° | w | when xX1
And (3.16)
gB (x, ) = w ox + wn + 1 °  | w |    gdyxX2
Using the set Y, the relation v vector v with the vector w, the relation (3.16) is given
Transform into a more compact form:
   V y    0 0 0 when yYY. (3.17)
Modification of the error correction algorithm, referring to linear resolution
Sets X1 and X2 with a 2 ° nakład overlay, can be written as follows:
Modification II
 V0 = [01.02, ..., 0n + 1], ie it is a (n + 1) dimensional zero vector;
 Vk = vk-1, when vk-1kk + k | (3.18)
 Vk = vk-1 + a ° yk, g d y vk-1 ° yk +  ° | in k-1 | ≤ 0.
This time, the theorem on the convergence of the algorithm could be: if
Sets X1 and X2 are linearly separated with a 2 ° overlay, then the algorithm acting according to II
The modifier will find the vector v satisfying the inequality (3.17) in finite order
Number of steps. Its components would unambiguously define the hyperplanes gA (x, ) = 0
And gB (x, ) = 0 satisfying conditions (3.16).
Unfortunately, this statement is only a hypothesis at present. But there is no
The obstacles to this modification do not apply practically. Iterations of this algorithm are
Very simple, so restrictions on their number can be very large
Numbers in the order of several hundred thousand and more, and in the case of small collections of even order
A few million. As was the case with the original version
Algorithm, after each correction can remember the best score, measured by the number
Y vectors satisfying the arrangement (3.17).
Example
 Below will illustrate the operation of the error correction algorithm in its
Original version. Let X1 = {x1, x2} and X2 = {x3, x4}, where x1 = [1,3], x2 = [3,1]
X3 = [3,4], x4 = [5,2]. These collections are shown in Figure 3.3.

Page 27
After applying the transformation (3.3) from the set X = X1X2 we obtain the following
Y = {y1 = [1,3,1], y2 = [3,1,1], y3 = [- 3,4-4,1], y4 = [-5,5 . Based on this
You should now create an infinite string that fulfills the assumption that the algorithm
Each element of this set occurs in infinite number of times.
 
                                   
Fig. 3.3. Data set for illustrating the operation of the error correction algorithm
 
This string can be in the form: (y1, y2, y3, y4, y1, y2, y3, y4, ...), ie yk4 + i = yi, where k is
Any non-negative integer, and a string element number. Algorithm run
Error correction for correction factor a = 1 is shown in Tab. 3.1.
It shows the entire course of the algorithm, despite the large number of iterations, after
This is to be able to see that it is not characterized by a gradual approach to
Solution, ie a systematic increase in product values ​​vk-1 ° yk after execution
Another correction. In column labeled as ordinal number, normal font
The number of the next correction is given and the bold number of the sequence is indicated in bold
Presenting an object from the set Y. The solution was reached after 44 corrections, ie
After 65 presentations of objects. Slides 66 to 69 were required for
Verifying that the obtained vector v = v69 is a solution of the system (3.4).
Organization Tab 3.1 is as follows. The start of the algorithm starts with
K = 1. Vector vk-1 from line k-1 is multiplied by yk vector from row k
This is not positive, then in the k line there is a new vk vector. If, however,
Vk-1 ° yk is positive, then vector v does not change, ie vk = vk-1. So on
The base of the string (yk) generates successive string elements (vk).
Beginning with step 66, four consecutive vk-1kgs are positive, which means that
A vector v = v66 is obtained, with each element of the set Y giving a positive product
scalar. Therefore, the continuation of the algorithm no longer makes sense, because vk vector does not
It will change. A solution of the inequality (3.4) has been obtained,
X
about
0 1 2 3 4 5
4
3
2
1
3
2
4
X
O 

1 1k 1k

Page 28
Which is the vector v = [- 3, -1, 11], and hence the discriminative function g (x) = - 3 ° x1-x2 + 11
Meeting conditions (3.1).
 
Tab. 3.1. The algorithm of the error correction algorithm for the set of Fig. 3.3.
L.p. Vector y Vector product v
L.p. Vector y Vector product v v1 v2 v3
0 y1 y2 y3 vk-1yk 0 0 0 y1 y2 y3 vk-1yk v1 v2 V3
1 1 3 1 0 1 3 1 25 1 3 1 -10 -2 -1 6
2 -3 -4 -1 -16 -2 -1 0 26 3 1 1 -1 1 0 7
3 1 3 1 -5 -1 2 1 27 -3 -4 -1 -10 -2 -4 6
4 3 1 1 0 2 3 2 28 1 3 1 -8 -1 -1 7
5 -3 -4 -1 -20 -1 -1 1 29 -3 -4 -1 -4 -4 6
6 1 3 1 -3 0 2 2 30 1 3 1 -13 -3 -2 7
7 -3 -4 -1 -10 -3 -2 1 31 3 1 1 -4 0 -1 8
8 1 3 1 -8 -2 1 2 32 -3 -4 -1 -4 -3 -5 7
9 3 1 1 -3 1 2 3 33 1 3 1 -11 -2 -2 8
10 -3 -4 -1 -14 -2 -2 2 34 3 1 1 0 1 -1 9
11 1 3 1 -6 -1 1 3 35 -3 -4 -1 -8 -2 -5 8
12 -3 -4 -1 -4 -4 -3 2 36 1 3 1-9 -1 -2 9
13 -5 -2 -1 24 -4 -3 2 37 -5 -2 -1 0 -6 -4 8
14 1 3 1 -11 -3 0 3 38 1 3 1 -10 -5 -1 9
15 3 1 1 -6 0 1 4 39 3 1 1 -7 -2 0 10
16 -3 -4 -1 -8 -3 -3 3 40 -3 -4 -1 -4 -5 -4 9
17 1 3 1 -9 -2 0 4 41 1 3 1 -8 -4 -1 10
18 3 1 1 -2 1 1 5 42 3 1 1 -3 -1 0 11
19 -3 -4 -1 -12 -2 -3 4 43 -3 -4 -1 -8 -4 -4 10
20 1 3 1 -7 -1 0 5 44 1 3 1 -6 -3 -1 11
21 -3 -4 -1 -2 -4 -4 4 66 3 1 1 1 -3 -1 11
22 1 3 1 -12 -3 -1 5 67 -3 -4 -1 2 -3 -1 11
23 3 1 1 -5 0 0 6 68 -5 -2 -1 6 -3 -1 11
24 -3 -4 -1 -6 -3 -4 5 69 1 3 1 5 -3 -1 11
 
For objects x1 and x2 of e with b and o r in X1, it assumes g (x1) = 5 and g (x2) = 1, and for
Objects x3 and x4 of g (x3) = - 2 and g (x4) = - 6. Simple g (x) = - 3 ° x1-x2 + 11 = 0
Passes through points [3; 2] and [2,5; 3,5], which are marked in Fig. 3.3 and
Lie in the field of this figure. Hence, it is easy to plot a dividing hyperplane,
In this case straight line.
 In the example shown, vectors first appeared in vectors
Corresponding to objects in class 1 and then in class 2. alternate presentation
Yk vectors from different classes in this example did not reduce the number
Iteration steps.
 1k

Page 29
 
3.2. Iterative algorithm for determining the optimal hyperplane
The fault of the error correction algorithm is the inability to resolve,
Whether the analyzed sets X1 and X2 are linearly separable. In addition, the hyperplane
Distributing, which with its use is possibly designated, may be
Far from optimal and then repeatedly experimented with
Modification I, proposed in the previous subsection. As has been
Marked in subsection (3.1) above, it is desirable for the separating hyperplane
It was as far away from the nearest object from the set X = X1X2.
Presented will now be an algorithm that finds the closest pair
Points a and b, where aCo (X1) and bCo (X2), where Co (Xi), i = 1,2,
Means the convex shell of the set Xi. Based on such a pair is easy
Determine the equation of the optimal partition hyperplane. The idea of ​​this algorithm
It is shown in Figure 3.4.
 
                                       
Fig. 3.4. The idea of ​​an iterative algorithm for determining the dividing hyperplane
For convenience, under the vectors symbols will also be understood as points in
Space of features with the same coordinates as the components of the vectors.
 As in the error correction algorithm, from the xj objects belonging to the set
X = X1X2 = {xj}, create an infinite sequence (zj) of vectors, this time n
Dimensional, such that every object x of the set X has an infinite number in it
Times and vectors z1 and z2 come from different classes. It is most convenient to assume that zj = xj,
Zj-m = zj, where j> m, z1X1 and z2Z2, ie the string is as follows:
(X, x2, ..., xm, xm + 1, xm + 2, ..., x2 • m, ...), where x1X1 and x2X2.
 As the first approximation of the search vectors a and b, vectors z1 will be adopted
And z2, ie a1 = z1 and b2 = z2. You just have to show how knowing the i-th approximation of the above
Determine the vectors of the next, with an i + 1 index. So, let the couple ai and bi be
about
about
X
about
X
X
X o
about
X x
O o
about
about
X
X

B a
G (x) = (a-b)  (x-a / 2-b / 2) = 0 mj1 1j 1j

Page thirty
I-the approximation of the search vectors a and b, and the vector of zj shown here comes from
Class 1. It may occupy different positions, as shown in Figure 3.5.
  
 
Fig. 3.5. Illustration of the idea of ​​an iterative algorithm
 
The situation in which it would come from the opposite class is not worth analyzing, because
Considerations would be symmetrical, ie, the vector's place and would take the vector bi and vice versa.
Vectors ai and bi designate a pair of parallel hyperplanes A and B
Orthogonal to the vector (ai-bi) and passing respectively points a and b.
The vector shown zj may lie to the left of or on the hyperplane A, ie
On the side indicated by the vector (ai-bi), between the hyperplanes A and B and on
Right from the B hyperplane or on it, ie on the opposite side than the one it is
Indicates the vector (ai-bi). The points a and b, which would define such a, are also sought
A pair of hyperplanes A and B, so that inside the belt between these hyperplanes does not
There was none of the string vectors (zj), ie no set object X = X1X2.
The point zj comes from class 1 as it is accepted, so it should lie to the left
The hyperplane A side, and more precisely from the opposite side than the hyperplane B. If
It is not, that is, it is located to the right of the hyperplane A in the middle belt
The hyperplanes A and B, on the hyperplane B or to the right of it, are pairs
Vectors ai and bi need to be upgraded, ie designate a new pair of vectors,
Ie ai + 1 and bi + 1. By definition to the right of the hyperplane B is to be understood
The opposite side to hyperplane A. The diagnosis on which side
The hyperplane A is present at the point z, can be made
By examining the value of scalar product (zj-ai)  (ai-bi). If it is positive or equal to zero,
Ai x bi
X
X
X
X ai + 1 (ie <1)
  A C E D B
Zj
Zj
Zj
Zj
X
(Zj-ai)  (ai-bi) 0
(Zj-ai)  (ai-bi) <0
(Zj-ai)  (ai-bi) <0
Ai + 1 (ie = 1) 1j

Page 31
There is no need for correction, ie ai + 1 = ai. When it is negative, ai + 1 is negative
Change.
Referring to Fig. 3.5, it is easy to see that the entire stretch connecting the points
Corresponding to the vectors i and i lies within the domes Co (X1). On this episode
You should find the point ai + 1, which will be closest to the point b. If you throw a point bi on
The straight line passing through the points zj and ai will be outside the section joining those
Points are ai + 1 = zj, because indeed, then the point z is from the whole of the above episodes is
Closest point bi. If the projection of the point of bi lies on the connecting section zj z
Ai, the closest point for bi in this episode will be just this throw,
That he will be the ai + 1 vector. So, both of these situations can be covered by one relationship:
Ai + 1-ai) = tj (zj-ai), where tj (0,1), ie ai + 1 = ai + no
Requires correction, because then the vector zj would be on the hyperplane A
Tj = 1, then ai + 1 = zj, and if 0 <tj <1, then ai + 1 is the bi projection of the segment that connects
Point zj with point ai. In order to calculate ai + 1, a fraction,
 Vectors (zj-ai) and (ai + 1-bi) are orthogonal, so (zj-ai)  (ai + 1-bi) = 0, and
Consider that ai + 1 = ai + tj (zj-ai) will result in the equation (zj-ai)  [(ai + tj (zj-ai) -bi] = 0,
Where tj = (zj-ai)  (bi-ai) / (zj-ai) 2. Algorithm acting as described above
Further approximations of vectors a and b may require an infinite number of steps
Iteration, as illustrated in the example shown in Figure 3.6.
 
 
Fig. 3.6. Example illustrating the need for an infinite number of steps
 
The set X1 contains four objects, ie, X1 = {z1, z3, z4}, and the set X2 has only one object,
Ie X2 = {z2}. The string (zj) has the following form: (z1, z2, z3, z4, z1, z2, z3, z4, ...). IN
First iteration a1 = z1 and b1 = z2. Point a2 is the projection of the point b1 for the connecting section
Point a1 with point z3. In turn, point a3 is the projection of the point b1 for the section connecting the point
X o
X
X
A1 = z1 b1 = z2
 
Z3
 
Z4
 
A3
A2
A4 1j

Page 32
A2 with point z4. Point a4 is the projection of the point b1 for the segment that connects the point a3 with
Point z3. Not shown in Fig. 3.6, point a5 would be a projection of point b1 for the segment
Combining point a4 with point z4. Continuation would result in correction of approximations only
Point a and these corrections would be performed only after the presentation of objects z3 and z4.
       One of the natural conditions for stopping an algorithm is when d (a, b) = 0, where
D (, ) is a function of distance, usually Euclidean. Then the sets X1 and X2 would be recognized
They were linearly inseparable. For numerical reasons, the condition d (a, b) = 0 belongs
Replace condition d (a, b) ≤ , where  is a small positive real number.
On the other hand, for linear distributions of sets, the condition (zj-ai)  (ai-bi) 0, for
All the vectors from the set Z may not be satisfied in finite time, what
It is shown above and illustrated in Figure 3.6. The solution of the problem can
To substitute condition (ai-bi)  (ai-bi) 0 condition (zy-ai)  (ai-bi) -
Where  is in the range of [0,1 / 2].
This means that the correction of the approximation of the point a would only take place if,
When for zj the condition (ai-bi)  (ai-bi)  (ai-bi)  (ai-bi), ie when the point zj invades in
Area right of the hyperplane A sufficiently deep, deeper than |  (ai-bi) |.
Koziniec [Koziniec, 1973] recommends the use of the parameter  accepting values ​​from
0.1 to 0.25. In this work there is no formal proof of the convergence of the algorithm.
Although it has been explicitly stated that for > 0 and > 0 the algorithm will stop after
Finite number of steps of approximation of the vectors a and b. Algebra of the algorithm
It is obvious because the generated vectors pair pair (ai, bi) corresponds to a string
Distance d (ai, bi), which is non-additive, infinite, and bounded from below.
Geometric interpretation of the above-mentioned condition (zy-ai)  (ai-bi) <-  (ai-bi)  (ai-bi)
Obviously, if this condition is presented in another form: | zj-ai |  | ai-bi | cos () <-  | ai-
Bi |  | ai-bi | where  is the angle between the vectors (zj-ai) and (ai-bi). Hence after
To simplify the aforementioned condition, the form will be: | zj-ai | cos () <-  | ai-bi |
The vector of zj-ai vector on ai-bi vector is the bi-ai vector, maximally its
Half, because 0≤≤1 / 2 and cos () is negative, as can be seen by analyzing the situation
Shown in Figure 3.5. If  = 1/2, then the new vector ai + 1, unlike the predecessor ai,
It will only be determined when the point zj belonging to X1 is to the right of
Hyperplane E shown in Fig. 3.5.
Similar considerations could be made for the situation where the point zj will be from
Set X2. The roles of ai and bi would then be converted. A new vector bi + 1,
Different from its predecessor b, will be determined only when the point zj belongs
Until X2 is to the left of the hyperplane E shown in Figure 3.5. For  = 1/4
The correction of pair (ai, bi) will take place only when the point zj belongs to X1 and it will be
To the right of the hyperplane C or when it is from the set X2 and it will lie to the left
From the D. hyperplane

Page 33
In the general case, when sets X1 and X2 are linearly separable, it is discussed
The algorithm running with parameter > 0 will find a pair of vectors a and b, as determined by
No hyperplane with equation (a-b) x-0,5 (a-b)  (a + b) = 0 will divide the sets X1
And X2 with clearance ps = (1-2)  | A-b |. For the situation as in Fig 3.5 and  = 1/4, guaranteed
The clearance would be between the hyperplanes C and D. When  = 0, the clearance
Would be between the hyperplanes A and B, and if  = 1/2, then the algorithm would find
A dividing hyperplane that does not guarantee any clearance.
The above explanation of the principle of the algorithm should do below
Given a formal description of an understandable algorithm. Symbol ": =" means that value
The expression to the right of this symbol must be substituted under
A variable on its left side.
Definition of the iterative algorithm
1. a1 = z1; B1 = z2; I: = 1; J: = 2; Link: = 2; M: = 5;
2. If d (ai, bi) , then {write: The sets X1 and X2 are not linearly separated,
Jump to 7};
3. If j <m, then j: = j + 1 otherwise j: = 1; Link: = link + 1;
4. If zjX1 and (ai-bi) ° (zj-ai) <-  ° (ai-bi) 2, then
{Bi + 1: = bi; Ie: = (zi-ai) ° (bi-ai) / (zi-ai) 2; If i <1, then ai + 1: = ai + tj ° (zj-ai);
Otherwise ai + 1: = zj; I: = i + 1; Link: = 0; Jump to 2};
5. If zjX2 and (bi-ai) ° (zi-bi) <-  ° (bi-ai) 2, then
{Ai + 1: = ai; Ie: = (zi-bi) ° (ai-bi) / (zi-bi) 2; If tj <1, t o bi + 1: = bi + tj ° (zj-bi);
In the opposite case bi + 1: = zj; I: = i + 1; Link: = 0; Jump to 2};
6. If l = m, then {write: The sets X and Y are linearly separated, vectors a and b are designated
Optimal spreading hyperplane; Jump to 7}, otherwise
Jump to 3;
7. End.
Example
 The operation of the algorithm will be illustrated by a simple example of
The five objects shown in Figure 3.7.
Solution
X1 = {x1, x3, x5} = {[2, 1], [3, 2], [1,3]}. X2 = {x2, x4} = {[5, 1], [5, 4]}. Collection Z created
Based on the sets X1 and X2, there are 5 numbered items as in Fig.3.7,
Objects derived from the set X1 have odd indexes and indexes from the set X2
Z = {zj} = {[2,1], [5,1], [3,2], [5,4], [1,3]}, where X1 = {z1, z3, Z5}
And X2 = {z2, z4}. 51j

Page 34
The infinite sequence (zj) formed from the set Z has the form: {z1, z2, ..., z5, z5 + 1,
..., z2 • 5,. . . }, Where zk = 5 + j = zj, for any positive integer i.
 
                       
Fig. 3.7. Collections X and Y to the numerical example for the iteration algorithm
 
The ordering of the set Z does not have to be alternating, it only matters
The first two words of this string contained objects from different classes. On
The base of the string (zj) generates a sequence (ai, bi) approximating pairs of vectors
(Ai, bi), but there is no correspondence between the indexes of elements zj and ai.
The algorithm for the above data is as follows:
1. a1 = z1 = [2,1]; B1 = z2 = [5,1]; I: = 1; J: = 2, l = 2; {Indicator is the sequence number
Approximation of the sought vectors a and b, j is the current number
The object being served, and the link means the guaranteed number of objects located
Get on the right side with respect to the belt designated by
Dividing hyperplanes}
2. d (ai, bi) = d (a1, b1) = d ([2,1], [5,1]) = 3> 0; {Function d (ai, bi) is the urban distance
Between points ai and bi}
3. j: = 3, ld: = 3;
Z3 = [3,2] X, (a1-b1)  (z3-a1) = ([2,1] - [5,2])  (3,2] - [2,1] = [- 3,0]  [1,1] = - 3 <0;
 B2 = b1 = [5,1];
I = [3,1] - [2,1]) (t1 = a1) / (z3-a1) / ([3,2] -2,1]) 2 =
= ([1,1] [3,0] / [1,1] 2 = 3/2> 1; hence it follows that a2 = z3 = [3,2];
I: = i + 1 = 2; L = 0; Jump to 2;
2. d (ai, bi) = d (a2, b2) = d ([3,2], [5,1]) => 0;
3. j: = j + 1 = 4; L = 0;
X
X
 
about
0 1 2 3 4 5
X
4
3
2
1 1
3
2
4
 b a
X 5
Y
about
X = 4 1j 1j 1i 5

Page 35
4. z4 = [5,4] X1; {Other instructions for this step are not executed}
5. z4 = [5,4] X2; (B2-a2)  (z4-b2) = ([5,1] - [3,2])  [5,4] - [5,1]) = [2, -1]  [ ] = - 3 <0;
A3 = a2 = [3,2];
  I = t4 = (z4-b2)  (a2-b2) / (z4-b2) 2 = ([5,4] - [5,1])  ([3,2] - [5,1] / ([5,4] - [5,1]) 2 =
= ([0,3]  [-2,1] / [0,3] 2 = 1/3 <1;
B3 = b2 + t4 (z4-b2) = [5,1] +  ([5,4] - [5,1]) = [5,1] +  [0,3] = [5,2] ];
I: = i + 1 = 3; L = 0; Jump to 2;
2. d (ai, bi) = d (a3, b3) = d ([3,2], [5,2]) = 2> 0;
3. j: = j + 1 = 5; Link: = 1;
 (1,3) - [3,2])  (3,3] - [5,2])  (1,3] - [3,2]) = [- 2.0]  [-2,1] = 4> 0;
 {Other instructions for this step are not executed}
5. z5 = [1,3] X2; {Other instructions for this step are not executed}
6. connect <m; Jump to 3;
3. j: = 1; Link: = link + 1 = 2;
4. (z1-x3 and (a3-b3) ° (z1-a3) = ([3,2] - [5,2]) ([2,1] - [3,2]) = 0] ° [-1, -1] = 2> 0;
5. z1 = [2,1] X2; {Other instructions for this step are not executed}
6. connect <m {bo l = 1 and m = 5}; Jump to 3;
3. j: = j + 1 = 2; Link: = link + 1 = 3;
4. z2X1; {Other instructions for this step are not executed}
5. if z2X2; (B3-a3) ° (z2-b3) = ([5,2] - [3,2]) ([5.1] - [5,2]) = [2,1] 1] = 0;
{Other instructions for this step are not executed}
6. link <m {bo l = 3, m = 5}; Jump to 3;
3. j: = j + 1 = 3; Link: = link + 1 = 4;
4. z3 = [3,2] X1; (3-a3) = ([3,2] - [5,2]) ([3,2] - [3,2]) = [- 2,0] 0] = 0;
 {Other instructions for this step are not executed}
5. z3 = [3,2]  X2; {Other instructions for this step are not executed}
6. connect <m {bo l = 4 and m = 5}; Jump to 3;
3. j: = j + 1 = 4; Link: = link + 1 = 5;
4. z4 = [5,4] X1; {Other instructions for this step are not executed}
5. z4 = [5,4] X2; (B3-a3) ° (z4-b3) = ([5,2] - [3,2]) ([5,4] - [5,2]) = [2,0,0] ] = 0;
 {Other instructions for this step are not executed}
6 = l = m = 5, "The sets X1 and X12 are linearly separated", vectors a = a3 = [3,2] and b = b3 = [5,2]
Designate a dividing hyperplane, jump to 7; 3
1 3
1

Page 36
7. End.
 
 
3.3. Recursive Algorithm for Linear Resolution
The subject of the considerations of this subsection will be poor linear resolution
Sets X1 and X2. Instead of the inequality 3.1, this time it will suffice
Fulfilled layout:
G (x) ≥ 0 when xX1
And (3.19)
                                                      G (x) ≥ 0 when x X2,
Where g (x) = + wn + 1x = [x1, x2, ..., xn].
The idea of ​​the proposed algorithm is illustrated in Fig. 3.8.
 
 
    2 5
1 x 8 o
X o 7
                about
         4 6
    3 x o
    X
                     
                     
 
 
Fig. 3.8. Idea of ​​a recursive algorithm
 
H7 hyperplane, in the figure it represents a straight line, splits correctly
Points numbered from 1 to 7. The arrows indicate the positive sides of the hyperplanes.
Crosses, as usual, have been selected objects in class 1, and circles objects with
Class 2. This is not strict resolution, because the hyperton H7 goes through 2
Points, one from class 1 and the other from class 2. Another presented object number 8
Is located on the wrong side. You can prove that by point 8 you can
Make a H8 hyperplane that distributes correctly, in the sense of separation
Weak, all 8 points, if only there is a hyperplane properly them
Separating. Such hyperplanes can be many. In a two-dimensional situation like
In Figure 3.8, this will be a whole bunch of straight lines between straight H8A and H8B.
Similar law is valid in a general n dimensional case for a set
X = X1X2 = {xj}. If the sets X1 and X2 are linearly separable and some hyperplane
H7 H8A H8B H8 
N
Jjjxw1 mj1

Page 37
Hi separates correctly points from set {xj} and point xi + 1 lies on the wrong one
On the hyperplane Hi side, the hyperplane Hi + 1 passes through the point xi + 1
Correctly separating all points of the set {xj}. This law will be continued
Parts of work formulated in another form and proven. It allows you to lower by
One dimension of the space in which the vector of weights is searched, ie coefficients
Hi + 1 hyperplane. The fact that the Hi + 1 hyperplane will pass through the point
Xi + 1 allows you to create an equation from which one can calculate one of n + 1 weights and thanks
This will remain until you find n weights.
 Further considerations relating to this algorithm will be more convenient
Using, as in the case of the error correction algorithm, from the mapping h (, )
Defined by the formulas (3.3). Using the mapping h (X) of the set X = X1X2 in the set Y,
The layout (3.19) will be transformed to the following form:
V0y0 when yyY, (3.20)
Where v = [w1, w2, w3, ..., wn, wn + 1] and Y = h (X) = {yj}.
The vector v that satisfies the inequality (3.12) will be termed as
Solution for the set Y. Finding algorithm v of the system (3.20) for
The set Y will be recursively defined. Its basis is as follows
claim.
Theorem 3.3
Let vi be the solution of 3.20 for the set Yi = {yj} YEn + 1
In a certain (k + 1) dimensional subspace Pk + 1 of the space of features En + 1 and let not
It will be for the set Yi + 1 = {yj} Y. If in the subspace Pk + 1 exists
Solution v0 for set Y, then in subspace Pk = Pk + 1 {yi + 1}  there exists a solution
Vi + 1 for the set Yi + 1.
Evidence
 Solutions for the Yi + 1 collection can be sought in the convex combinations
Vi + v0, ie vi + 1 = tvi + (1-t) v0. Since vi + 1yi + 1 = 0, then after
Substitution for vi + 1 expressions tvi + (1-t) v0 will yield an equation from which
Calculate t: [tvi + (1-t) v0] yi + 1 = 0, ie t (viyi + 1-v0yi + 1) = - v0yi + 1, hence it follows that
T = -v0yi + 1 / (viyi + 1-v0yi + 1). The product v0yi + 1 is nonnegative, and the product viyi + 1
negative. From a t e m m the value of t in the interval [0,1]. Value 1 is
Excluded, because then there would be no adjustment of the vector vi, because there would be a relation
Vi + 1 = vi.
It is easy to see that for every t value in the range [0,1] vector vi + 1 = tvi + (1-
T) v0 gives a non-negative scalar product with every element of the set Yi, and hence for this
Calculated above, because it also falls within this range. Ij1 11ij mj1 ij1 11ij

Page 38
Actually: vi + 1yj = [tvi + (1-t) v0] yj = t (viyj) + (1-t)  (v0yj) 0, dlaj = 1,2 ,…and,
Because theorem0 of v0yj00, under the assumptions of the theorem. And for the contrast for
Calculated value t of scalar product vi + 1yi + 1 = 0, because of that equation
Coefficient t was calculated. The zero value of this scalar product means that
The vector vi + 1 belongs to the subspace Pk = Pk + 1 {yi + 1} , which completes the proof of the theorem.
  The set Y for which the solution is sought v was created as a result
Transform h (, ) of the set X = X1X2. For the description of the algorithm is no longer necessary
Information that Y contains points that are derived from object transformations,
Derived from sets X1 and X2, as reported by the last constituent elements
The set Y, that is components yn + 1 = 1, because they are derived from the transformation of objects with
The set X1 and yn + 1 = 1, if they are the result of mapping objects from the set X2.
So the proposed algorithm will be designed to be more general formulated
Ie, to search for a solution v for a certain set Z = {zj} En + 1 w
The subspace Pk + 1En + 1, where m is the size of the set Z. If in the subspace
Pk + 1 solution for the set Z exists, then the algorithm should be able to determine them for
The set Z as well as for any subset of it. In further considerations occur
There will also be subsets of the set Z denoted by indices not exceeding the number
N + 1. For this reason, it is convenient to accept one more symbol on it
Denotation, ie that Zk + 1 = Z.
Given that the algorithm is supposed to ultimately lead to the designation
The solution for the set Y is then enough to substitute Z = Y and Pn + 1 = En + 1.
Theorem 3.3 is also true for a set Z whose points do not have
The coordinates of zn + 1 are equal to +1 or -1, as is the case for set Y.
Before defining an algorithm definition, it is necessary to establish meaning
Used symbols.
 The symbol (v, lg): = A (Zk + 1, Pk + 1) will denote the find algorithm
Solution for the set Zk + 1 in the subspace Pk + 1. The lower index at the set Zk + 1 does not
Is necessary, but it is a double marking of the dimension of the space in which
You are looking for a solution search solution that will greatly facilitate tracking
The algorithm illustrates the operation of the examples shown hereafter
This monograph, after the definition of the algorithm. The result of the algorithm is a compound pair
With the v vector and the scalar lg assuming the value 0 if, as a result of application
The algorithm proves that the obtained vector v is a solution for the set Zk + 1 in
The subspace of Pk + 1 or scalar lg assuming -1 if it turns out that
The resulting vector v is not the solution you are looking for. To find a solution
The system of equations (3.20) should be called with the proposed algorithm with the data: Zn + 1 = Y and
Pn + 1 = En + 1, ie (v, lg): = A (Y, En + 1). After entering the aforementioned alerts the algorithm can
Already defined. Mj1

Page 39
Analysis of algorithm instructions will be more convenient to follow
At the same time the appropriate algorithm steps in the given two given
Examples illustrating its effect. The first of the examples given contains
Data in two-dimensional space and refers to a pair of sets X1 and X2 linearly
Separate. The data set is shown in Figure 3.9. The second example concerns
Linearly-inseparable sets and was illustrated using data in
One-dimensional space.
Definition of a recursive algorithm
Call (v, lg): = A (Zk + 1, Pk + 1);
K.1 v: = any non-zero and orthogonal vector of the set Zk + 1
On the subspace Pk + 1, if such a projection exists;
K.O == If in Zk + 1 there is no vector whose orthogonal projection on Pk + 1
Would be nonzero, then take lg: = -1 and jump to k.12;
K.3 DZk + 1: = {zZk + 1: v z0};
K.4 If DZk + 1 is empty, then the base lg: = 0 and jump to k.12;
K.5 if k = 0 and DZk + 1, tolg: = -1 and jump to k.12;
K.6 Zk: = Zk + 1-DZk + 1;
K.7 J: = n-k + 1; Bj: = any vector of DZk + 1;
K.8 Call Ev, lg): = A (Zk, Pk), where Pk: = Pk + 1bj} ;
K.9 If lg = -1, jump to k.12;
K.10 DZk + 1: = {zZk + 1: v z0};
K.11 If DZk + 1, to jump to k.6;
K.12 If lg = -1, then write: Solution does not exist, sets X1 and X2 are not linear
Separate if lg = 0 and k = n, then write: sets X1 and X2
They are linear and the components of v are coefficients
Divider plane.
 
Example 1 - Linear distribution sets
X1 = {[1,1,1], y2 = [-1,4] 2, -1], y3 = [- 2, -4, -1]}.
Solution: k: = n = 2; Zk + 1 = Z3 = Y; Pk + 1 = P3 = E3;
Call (v, lg): = A (Z3, P3);
2.01 v: = y1 = [1,1,1]; Because y1 is from Z3 and is already in P3, it is also a roll on
Subspace P3;
2.02 Condition is unfulfilled;

Page 40
                              
Fig. 3.9. Illustration of an example of a recursive algorithm
 
2.03 DZ3: = {y2, y3}; Because v1 y1 = 3, vy2 = -7 and vy3 = -7;
2.04 Condition is not fulfilled;
2.05 Condition is not fulfilled;
2.06 Z2 = Z3-DZ3 = {y1}; Step 2.05 is inactive
2.07 j: = n-k + 1 = 1; B1 = y2;
2.08 Call (v, lg): = A (Z2, P2); P2 = P3 {b1}  = {b1}  = {y2} ;
 1.01 v: = y1- (b1y1) / (b1b1) b1 =
= [1,1,1] - ([- 4, -2, -1]  [1,1,1]) / ([- 4,2, -1] 2)  [-4, -2 , -1] =
= [- 1,1,2] / 3 [-1,1,2]; {Constant multiplier for convenience, leave} =
 1.02 Condition is not fulfilled;
 1.03 DZ 2: = {empty}; Step 1.1 is not active;
 1.04 Lg: = 0; Jump to 1.12
 1.12 Do not do anything;
2.09 Condition is not fulfilled;
2.10 DZ 3: = {y3}; Because v1y = 2, v2y = 0 and vyy3 = -4;
2.11 Jump to 2.06;
2.06 Z2: = Z3-DZ3 = {y1, y2};
2.07 j: = n-k + 1 = 1; B1 = y3;
2.08 Call (v, lg): = A (Z2, P2); P2: = P3 {b1}  = {b1}  = {y3} ;
 1.01. V: = y1- (b1y1) / (b1b1) b1 =
= [1,1,1] - ([- 2,4, -1]  [1,1,1]) / ([- 2,4, -1] 2)  [-2, -4 , -1] =
X
 
ABOUT
0 1 2 3 4
4
3
2
1 1
2
3
Y
ABOUT
X
Y

Page Skipped page 41


Page Skipped page 42


Page 43
Theorem 3.4
Let vectors v1, i = 1,2, ..., k be the solution of the system (3.20), and let z be
The subsets of the set Y, whose elements give the vectors scalar vectors,
Ie Zi = {yY: viy = 0}. Then any positive combination of v * v vectors is
The solution of the system (3.20) and gives zero scalar products only with those
Elements of the set Y which belong to the set Z * = Z1Z2 ... Zk.
Evidence
Since the vector v * is a positive combination of vectors v, it means that v * = ivi. After
Multiply the multiplication of this relation by the vector y, which does not belong to the set Z *
The relation: v * y = iviy, in which at least one of the components iviy,
Is positive, as evident from the definition of the set Z *, and this means that v * y> 0. If v is
The element of the set Z *, then each of the components iviy is zero, and therefore v * y = 0, which is
Ends the proof.
The advantage of this assertion is that it gives the possibility of reducing the number
Points for which v * y = 0, which means that the corresponding v *
The hyperplane that separates sets X1 and X2 contains less points from sets X1 and
X2. If such points were not at all, ie Z * = , then this hyperplane is strictly
It would separate collections X1 and X2.
Of course, the possibility of using this assertion for that purpose is
Real, only if the sets X1 and X2 are strictly linear. The idea of ​​this
The claims [Jóźwik A., 1981] were used in the dissertation
[Sturgulewski Ł. 2008] for the determination of a strictly separable hyperplasia.
Study of different types of linear distributions of two sets, using
Recursive algorithm, also devoted to another dissertation [Cendrowska
D., 2007]. Presentations of the solutions contained in the aforementioned works would take too much
Places and for this reason will not be included in this monograph.
 Another solution for getting a hyperplane straight
Dividing the examined sets X1 and X2, if there exists, is an enlargement of those sets
In such a way that from the weak linear division of the enlarged sets resulted
Strict linear separation of original collections. This idea is illustrated in Fig. 3.10.
The advantage of such an approach can be addressed if at the investigator's disposal
There is a low linear resolution algorithm.
 As a result of applying a recursive algorithm to sets X1 and X2 obtained
Will be hyperplane H0. The set X1 is enlarged in such a way that z
Each point x of this set creates an additional point xv0 where v0 is
A normal vector of the H0 hyperplane delineating the sets X1 and X2. 

K
I1 

K
I1

Page 44
                
Fig. 3.10. Method of enlarging original collections
 
 Likewise, every point x of the set X2 generates a new point x + v0. if
X1 and X2 contain m points, ie one session, ie, operation, zooming
Increases the number of points in enlarged sets by m. In Fig.3.10 points
The original sets X1 and X2 were represented by larger crosses and circles,
And the points added less. You can zoom in again
Use a recursive algorithm to obtain a new H1 divisor from
Normal vector v1. The described operation of adding new points can be
Repeat by attaching to newly enlarged sets new character points: xv1 to
The set X1 and the points of form x + v1 to X2. Similarly, you can generate another
A dividing hyperplane whose distance from the nearest object from the set
X = X1X2 will be greater than it was in the previous case
Hyperplanes. The designation of further hyperplanes can be dispensed with
Differences in distance between consecutive hyperplanes from the closest object from the set X
They will already be small enough.
In the example shown in Figure 3.10, there is already one new addition operation
The points obtained were the strict linear distributions of the studied sets.
Hypnotic H1 weakly separating enlarged collections strictly distributes the collection
Original, ie X1 and X2.
 Below is an algorithm based on the above
Approach to study the linear resolution of two finite sets in space
Characteristics.
Algorithm for linear resolution with specified clearance
 Linear resolution with specified clearance is multiple
Use one of the algorithms outlined in the author's articles
X x
X
X
X
X
about
about
about
about
about
V0
Xv
X + v
H1 H0
X
X
X
X
O o
about
O o
V1 Collection X1 Collection X2

Page 45
Monograph [Jozwik A., 1983a, Jóźwik A., 1998a], for the convenience of the present
Considerations will be saved as:
        (W, wn + 1, lg) = A (X1, X2), (3.21)
Where w = [w1, w2, ..., wn] and w = 1.
It returns a vector v = [w1, w2, ..., wn, wn + 1] with real components and the number lg
Taking the values ​​-1 or 0. If lg = -1, then the sets X1 and X2 have turned out to be
Linearly inseparable, and when lg = 0, these sets are linear and the equation
G (x) = wx + wn + 1 = 0 describes the equation for the divider hyperplane H.
Definition of the algorithm
1. Specify the ps = 2 prze clearance, where  is the real number and the stop parameter 
Depending on the accuracy of the calculation;
2. Call the procedure (w0, w0, n + 1, lg): = A (X1, X2);
3. If lg = -1 then jump to 10;
4. Basic Y1: = X1 and Y2: = X2;
5. Create a set Z1 = {x-w: xX1} and Z2 = {x + w: xX2};
6. Create Y1: = Y1Z1 and Y2: = Y2Z2;
Call the procedure (w, wn + 1, lg): = A (Y1, Y2);
8. If lg = -1, jump to 10;
9. If | d (H, X) - | <, jump to 10, otherwise jump to 5;
10. If lg = -1, sets X1 and X2 can not be separated from the given ps clearance,
Otherwise when lg = 0, then it should be assumed that the hyperplane
Defined by the equation g (x) = wx + wn + 1 = 0 separates the sets X1 and X2
With ps clearance, where g (x)> 0, where xX1 and g (x) <0, when xX2.
It is worth noting that the algorithm (w, wn + 1, lg): = A (Y1, Y2) is called for ever
Larger sets Y1 and Y2.
Proof of convergence of the algorithm
 Let sets X1 and X2 be strictly linear and let H0 denote
The hyperplane defined by the equation g0 (x) = w0x + w0, n + 1 = 0, ie obtained in step
2 algorithm. The subsequent hyperplanes Hi, i = 1,2,3, ..., are obtained as a result
Calls in step 7 and creates an infinite sequence (Hi). Hi Hi planets arise
As the result of dividing the sets Yi, 1, and Yi, 2, ie, from the calls (wi, wi, n + 1, lg): = A (Yi, 1, Y and, 2) and
They are defined by the equations gi (x) = wix + wi, n + 1 = 0. Infinity of thrust (Hi) has been
Accepted only for the purpose of this proof. This means that immediately after step 9
There is a jump to step 5, ie the algorithm never ends. For each pair
The sets Yi, 1, and Yi, 2, have a set Si of hyperplanes separating these sets. Easy 
1i

Page 46
Note that Yi, 1Yi + 1.1 and Yi + 1,2Yi, 2, and hence show that Si + 1Si. Let Gi be
Hyperplane, from the set of hyperplane Si, whose distance from the set X = X1X2
Is minimal. The hyperplane Hi also belongs to the set Si. It is easy to see,
For every i, i = 1,2,3, ... there is an inequality: d (Hi, X) ≥d (Gi, X). Because
Si + 1Si, then d (Gi + 1, X) ≥d (Gi, X) (minimum on larger set is not bigger). String
D (Gi, X), i = 1,2,3, ..., is unbound, infinite, and bounded from above,
D (Gi, X) ≤. So it must be convergent. The method is not easy to show that it can not
To converge to less than . So d (Gi, X) , and since
D (Gi, X) ≤d (Hi, X) ≤ , also d (Hi, X) , which was evident.
 
3.5. Editing the learning set for linear distribution of sets
One of the simpler ways to construct a linear discriminative function
For the case of two sets, we define an orthogonal hyperplane to
The section connecting the center of gravity of the classes and passing through its center.
Classifier using the discriminative function defined by this hyperplane
Is equivalent to the minimum-distance classifier in which each of them
Classes are represented by the center of gravity. The object is assigned to
Class whose representative is located at a shorter distance.
The minimum distance separator can be very bad. That depends
From the distribution of objects, as shown in Figure 3.11.
 
                
Fig. 3.11. Illustration of the defect of the minimum-distance classifier
 
Large class sizes have been highlighted for the classes
Smaller objects of learning set. The H0 hyperplane determines the separation
Objects according to the minimum-distance classifier. Objects from set X1
Should lie to the left of it, as indicated by the arrow, ie the normal vector, a
Therefore, three objects in class 2 are on the wrong side of this hyperplane.
X o x
X
X
X
X x
about
about
O o
about
about
about
about
O o o
O o o
Collection X1 Collection X2
H0
H1

Page 47
Among the hyperplanes parallel to the H0 hyperplane there is more
Beneficial hyperplan H1, which perfectly separates sets X1 and X2, and its
Determination is a very simple task, since its equation differs from the equation
H0 hyperplane is just a free expression.
The idea can also be used in the case of linear harvesting
Inseparable. To explain how to use it will be convenient to use
Illustrating the example shown in Figure 3.12.
 
 
        
Fig. 3.12. Illustration of the method of editing a learning set for linear resolution
 
The first computational operation is to determine the center of gravity of classes. On
Fig.3.12 are the points a and b. By the point b, a hyperplane can be led
Passing through this point and orthogonal to the section joining points a and b, that is
Orthogonal to the vector a-b. The equation for this hyperplane is:
GB (x) = (a-b)  (x-b) = 0. The point c1 from the set X1 for which the function is defined is now set
GB (x) reaches the minimum and point d1 from the set X2 for which the function gB (x) achieves
maximum.
By the parallel displacement of the HB hyperplane, the first time to the point c1 and
Once again, a pair of HC1 and HD1 hyperplanes will be made to point d1. In waist
Between these hyperplanes are three objects of class 1 and 2 objects of
Class 2
Since in this lane, including points located at the boundary of it
Hyperplanes are more objects of class 1, it is assumed that of two
The extreme points c1 and d1 into the class 1 space invade the point d1, so it is
Thrown out of set X2. Again, a new pair of edge points c2 and
X
about
X
X
X
X x
X
X
X
X
X
about
O o
about
about
about
about
C1 = c2 = c3
D1
D2
and
B
HD1 HD2 HC1 = HC2 = HC3 HB
Collection X2
d3
HD3
Collection X1

Page 48
D2. Point c2 will be the same as point c1 since it is defined as an object from a set
X1 for which the function gB (x) reaches the minimum, and the set X1 has not changed.
Updates only require point d1, ie a new point d2 is set for which
The function gB (x) achieves the maximum on the set X2 minus the point d1, ie on
Set X2 - {d1}.
In the new lane between the HC2 and HD2 hyperplanes,
The equations gC2 (x) = (a-b)  (x-c2) = 0 and gD2 (x) = (a-b)
Two objects in class 1 and one in class 2, so the deletion is subject to point d2. Next
An update of point d2 must now be made, ie, point d3 must be found
C3 = c2 because the set X1 has not changed.
 After performing this operation, in the waist between the HC3 and HD3 hyperplanes,
There is no object outside the objects on these hyperplanes.
This can be recognized by the fact that gB (c3)> gB (d3). The fulfillment of this inequality is
Algorithm stop condition. During operations similar to the above described may
It happens that in the waist between the HCi and HDi hyperplanes are located
There will be equal number of objects in each class. Then, both points di and di are
Thrown out and both + 1 and di + 1 upgrades have to be calculated. Removal principle
Objects, including in tie situations, can be illustrated by example
One dimensional, as in Figure 3.13.
 
Value of feature = 1 2 3 4 5 6 7 8 9 10 11 12 13 14
0. Object class x x x  x
X
X
O o x o o o o
1. Object class x x x x
X
X
o o x o o o o
2. Object class x x x x
X x o o o o o
 
Fig. 3.13. An example of a learning set for illustrating the removal policy
 
The extreme points, ie defining the overlap of the classes, are the points o
The attribute values ​​d1 = [4] and c1 = [8] are marked with a "znak" before the object
(Row number 0). Between them there are 4 crosses and three wheels, including
Points with coordinates c1 and d1. The point d1 must therefore be dropped. In another
The extreme points will be: circle d2 = [6] and cross c2 = [8] (line with number
1). In the waist between the restriction points, including the extremities of the waist
C2 and d2, there are now two crosses and two circles. So the cross c2 = [8] and the circle
D2 = [6] are thrown out. Finally, the belt between the cross will remain
C3 = [6] and circle c3 = [7]. The dividing hyperplane will be described by the equation
H (x) = gC3 (x) + gD3 (x) = (6-x) + (7-x) = 13-2x = 0. Objects for which h (x) 0 will be counted
To class 1, and objects for which h (x) <0 to class 2.

Page 49
Removing objects from the learning set to construct a classifier
It will be referred to as editing a collection.
Definition of the algorithm
1. Determine the gravity centers a and b of class 1 and class 2 respectively;
2. Define a hyperplane gB (x) = 0 passing through point b and perpendicular to
A vector with the beginning of b and the end of a, ie a-b is its normal vector.
3. Among the points in class 1 find the point c for which the function gB (x) achieves the minimum,
And among points in class 2, point d for which the function gB (x) reaches its maximum, a
Then define two hyperplanes gC (x) = 0 and gD (x) = 0 passing
Respectively, by the points c and d, parallel to the hyperplane gB (x) = 0,
Have a vector a-b as a normal vector.
4. If point c lies on the side of point a and point d on the side of point b,
GB (c)> gB (d), then jump to step 8.
5. If gB (c) gB (d), then find the number l1 of such points x of class 1 and the number of such l2
Points x in class 2 that gC (x) 0 and gD (x) 0.
6. If l1> l2, then remove point d and designate point b again. When l2> l1, then remove
Point c and redefine point a. In the case where l1 = l2, then remove both
Point c and d.
7. Jump to subsection 3.
8. Determine the function h (x) = gC (x) + gD (x).
 
     The discriminative function h (x) defines a classifier for pairs of classes 1 and 2. If
H (x) ≥0, then point x is qualified for class 1, otherwise class 2.
 As a result of the operation of editing the learning set X, that is, removing part of it
The objects will be created two new linearly separated sets Y1 and Y2, where
Y1X1 and Y2X2. One can therefore apply to this set, the error correction algorithm,
Iterative algorithm or recursive algorithm supplemented with the possibility of testing
Linear separation with margin. As a standard, step 8 in the definition of the algorithm
It would be wise to replace iterative algorithm.
If its result indicated a lack of separation, what could be
As a result of a numerical error, one can resort to a recursive algorithm in
Complemented version for linear margins. Benefits of
Such a combination of suggested algorithms will be explained by example
Shown in Figure 3.14.
 By applying a set of learning sets for linear resolution obtained
There will be a pair of HC and HD hyperplanes and the resulting HCD hyperplane.
All these three hyperplanes are perpendicular to the vector joining points a and b,

Page 50
         
Fig. 3.14. Example of using the iteration algorithm in step 8
 
That is, to the vector a-b. Collections Y1 and Y2 are certainly linearly separated, but
The obtained HCD resulting hyperplane separates these collections from a small one
Margins. Using an iterative algorithm would make it possible to achieve no less
Margins, because this would be the optimal margin, ie the largest possible.
HAB's resulting hyperplane would run between the hyperplanes
HA and HB.
 This algorithm, for several years now, is presented by the author of this
Monographs, on the first and second degree studies, and in the application to the task
Two-decision, was the subject of a master's thesis [Rychlik T., 2012]. His
The correctness was verified on the Iris set already mentioned in Chapter 3
Date, separately for each of the 3 possible pairs of classes. Also, in another job, this time
Engineering [Tomaszewski W.P., 2013], applying to the problem of multi-
It was compared with the 1-NS classification on Iris Data and on
Other collections available on the Internet (http://archive.ics.uci.edu/ml/datasets.html).
The classifier used was in parallel with Figure 1.2. In some cases,
The presented algorithm with editing for linear resolution offered smaller
Fractions of errors than Classifier 1-NS.
Chapter summary
 There are four types of algorithms for determining hyperplanes
Separating. However, one can not say that any of these algorithms could
Eliminate from other applications. The simplest of them, the error correction algorithm,
It can be applied independently of linear non-separation of sets. It is similar
Situation with algorithm for editing sets for linear resolution. They are so
Algorithms more universal than the other two algorithms. Algorithms: iterative and
X
X x x
X
X
X x
X
X
about
about
about
about
about
about
about
HD HC
about
about
about
about
about
about
X o a
B
about
HA HB HAB
HCD
Collection Y2
Collection Y1

Page 51
The recursive concern is the study of linear distributions of collections, and in case it turns out
No, it does not allow you to construct an acceptable linear
Discriminatory function. However, it is worth investigating whether it is not special
The case is the strict linear resolution, because then this can be true
Use to improve the quality of the classification, measured by the correct percentage
Decision.
The iteration algorithm allows to find the optimal hyperplane. But in
If the sets are linearly separated, but very close to each other, it can be misleading
Answer due to numerical error. Hyperplanetary as a place
Geometric points equally spaced from points a and b given by
The use of an iterative algorithm may not be sufficiently determined
Exactly what was shown in Figure 3.15.
 
                                 
 
Fig. 3.15. Collections difficult to separate for the iteration algorithm
 
In this drawing, the larger characters, crosses or circles have been
Selected objects from the learning set, and smaller points found by
Iterative algorithm, ie points a and b. The convex Co (X1) and Co (X2) of the sets X1 and
X2 are strictly linearly separated, since there are no hyperplanes A and B
No points from the learning set X = X1X2. The algorithm should designate two
The closest points to the opposite shells, points a and b
Due to the limited accuracy of the calculations, these points will not actually be themselves
The closest.
H0 hyperplane passing through the center of the connecting section a and b i
An orthogonal to it may not properly divide the sets X1 and X2, and the algorithm da
The answer is that the tested collections are not linearly separated. By contrast, the algorithm
Recursive can give the answer that these sets are linearly separated in the sense of weak
X o o
about
X
X
X
B
H0
about
about
X
X
B a
H1
Collection X1 x
about
about
AND
Collection X2

Page 52
Resolution, and if you apply the magnification of sets X1 and X2,
Suggested in subsection 3.4, a strict linear resolution is detected
These collections. This algorithm, in comparison with the iteration algorithm, however
More complex and much more difficult to software. It is possible that
Also, the error correction algorithm will find a strictly separable hyperplane,
However, this feature can not be guaranteed because there are no known steps,
What you need to do and the information that it is finite is practically small
importance.
 
 
4. MINIMUM-DISTANCE METHODS
The previous chapter was devoted to two-decision classifiers in which
The discriminatory function was linear. In geometric interpretation
This meant the separation of the hyperplanes of the two sets, in such a way that
As many objects as possible lie on its proper side, objects from class 1 after
Positive side or on this hyperplane, and objects of class 2 after negative.
The two-decision limit is not a problem, because the two-
Decision-makers can be built multi-decision classifiers, as suggested
The structure shown in Figure 1.2 in Chapter 1. The essence of this approach
There was a differentiation of classes. Objects on one side of the hyperplane
Distributions were treated as coming from a different class than the objects lying after
The other side.
The following next subsection will consider methods based on
The similarity of objects than the differentiation based on their location
To hyperplanes. As a function of the similarity of objects to the classes under consideration
The discriminative functions implemented in the structure shown in Figure 1.3 are accepted
From Chapter 1. For two classes, two discriminative functions g1 (x)
And g2 (x) whose values ​​are respectively measures of the similarity of objects x to class 1
And 2, respectively. It is much more convenient to use one differentiated function
Discriminative form of g12 (x) = g1 (x) -g2 (x).
 
4.1. Minimum distance separator
The similarity of an object to a class can be measured with the function of distance, just enough
Each class i was represented by one point pi i = 1,2, ..., nc, in space
Features, with the smaller distance of the classified object x from the point pi
Corresponds to the similarity of this object to class i
The distance also falls within the general pattern of the classifier in Fig.1.3, which becomes
It is understandable to assume that the function g (x) = - d (x, pi) or gi (x) = 1 / [1 + d (x, pi)]. Points pi,

Page 53
I = 1,2, ..., nc, may be centers of gravity of classes or points whose coordinates
There are medians of qualities or even another point set on the basis of the set
The instructor. This type of classifier is known as a minimum-distance classifier.
The most commonly used distance function is the Euclidean measure
D (x, y) = (2) 1/2 or city distance d (x, y) =, where x = [x1, x2,
..., xn]. A list of other applicable distance measures can be found in [Jajuga K.,
1990].
If using the Euclidean distance, the minimum
The distance can be reduced to the structure shown in Figure 1.3, with linear
Discriminatory functions, as will be shown below. Instead of the distance d (x, pi)
You can use the square:
      (X, pi) = (x-pi) 2 = x2-2pipi x + (pi) 2 = x2- [2pix-
Where gi (x) = 2pix- (pi) 2.
  The distance d (x, pi) reaches the minimum for the same indicator value and for
Whose function gi (x) accepts the maximum. The functions g (x) can therefore serve as
Discriminatory functions and implemented in the structure blocks shown in Fig.
1.3. These are linear functions. Substituting x = [x1, x2, ..., xn] and pi = [pi, 1, pi, 2, ..., pi, n]
It can be written as:
Gi, x, 2, ..., pi, n,  [x1, x2, ..., xn] - [pi, From where it follows that
Gi (x) = 2pi, jxj-. Discriminatory function for the class and thus has the form:
Gi (x) = wi, jxj + wi, n + 1, (4.2)
Where wi, j = 2pi, j and wi, n + 1 = -.
The above considerations show that the minimum classifier
Distance can be built as a linear machine [Nilsson. N., 1965].
In the case of two classes, a linear machine can operate with one function
It is discriminatory and requires only n multipliers, while the original version
The minimum distance separator would require 2 multipliers. If the class is
More than two, then this advantage does not take place, because then you should use so many features
Discriminatory how many classes are already discussed in subsection
1.3.
 In the above-mentioned work, another advantage of the linear machine was not taken into account,
It is related to the problem of standardization of features. Taking into account the standardization
Features in the minimum-distance classifier, there is no need for standardization
The entire set of learners. It is enough that only points pi,  will be standardized

N
J
Jjyx
1
) (

N
J
Jjyx
1
|| D2 
N
J1 
N
J1 pji
2

N
J1 
N
J1 pji
2
,

Page 54
I = 1,2, ..., nc, being representative classes. This is a one-time operation, but however
Every newly classified object must also be standardized. The linear machine
It gives you the opportunity to include standardization on the scales of discriminatory functions and then
Classification of objects should not be standardized.
Classifier minimum-distance if standardization is included
Attributes, assign object x class and, if d (xs,) = d (xs,), where
And are standardized means of gravity classes, and xs = [ ...,] is
Classified object after standardization. Discriminatory function (4.2), in
If the standardization of features is taken into account, it will take the form:
Gi (xs) = wi, j + wi, n + 1, (4.3)
Where w, j = 2 and wi, n + 1 = -2.
In the formula (4.3) variables can be substituted by = (xj-mvj) / sdj, where mvj and sdj are
Respectively the mean value and the standard deviation of the j or median and
Median deviation of this feature. Then the function will be received
Discriminative gi (xs) = xj + wi, n + 1-mvj, in which no longer exists
Standardized characteristics values. It can be saved as:
 G (x) = xj +, (4.4)
Where =, a = wi, n + 1-mvj.
Function (4.4) no longer contains standardized values ​​of qualities, because standardization
It was included in its weight.
 The minimum distance separator does not give a good overall rating
Classification results, as illustrated in Chapter 3 in Figure 3.11, but in
Specific tasks can offer you an acceptable quality of classification, and at
This is very fast. It also takes up little memory because of the classification phase
It is enough to remember nc centers of gravity or other nc points representing
Considered classes. Such points are the reference set for classification. He must
Be kept in the computer memory in the classification phase. Because
For the simplicity of the minimum-distance discriminator, it is beneficial to examine whether in
The specific task is not sufficient, since such an experiment does not require it
A lot of work.
 Of course, as with any other classifier it is worthwhile
Consider the selection of features. Also, because of its small isp jmin ps
J isp ps
J xs1 xs2 xsn 
N
J1 xsj ps
Ji, 
N
J1) (ps
Ji xsj xsj 
N
J1 j
Ji
Sd
W, 
N
J1 j
Ji
Sd
W, s
And 
N
J1 wsji, ws1,  wsji, j
Ji
Sd
W, ws1,  
N
J1 j
Ji
Sd
in,

Page 55
Flexibility, ie a small ability to adapt to the learning set, a fraction of errors
Can be evaluated on the basis of the learning set, without the need for a set
Testing, or resort to a method minus one element.
 
4.2. Rule k nearest neighbors
The rule of k nearest neighbors (k-NS) was proposed more than 60 years ago
In the work [Fix E., Hodges J.L., 1952]. Classifier acting according to this rule
Assigns a class to the object from which most of the k is derived
Its objects in the learning set. If the designation of k-th nearest neighbor does not
It is unambiguous, ie at the same distance from the classified object as k-th
The nearest neighbor is still other objects, they all should participate in
Vote. In other words, k-th closest neighbor defines the smallest hyper-
Which contains at least k objects from the set of learners closest
Classified object. All objects that are included in the vote
In this hypercube will find themselves. This principle is illustrated in Fig. 4.1, which shows that in this
Same distance as every fourth closest neighbor, no matter which object it is
There will be two other objects.
 
                              
Fig. 4.1. Illustration to explain the rules of operation of rule 6-NS.
 
There is no reason why all objects that are in the radius of hyperkinesis
Equal distance to fourth closest neighbor was not to participate in
Vote.
Despite its simplicity, the classifier acting on the basis of this rule offers
High quality of classification, in the sense of the probability of correct decision, in
Compared to many other known algorithms [Carpenter G., Grossberg
S., 1996]. Experiments involving 6 classes were conducted, 36
Numerical features, a set of learners with a total of 4435 objects and a set of testers
Containing 2000 objects. Classifiers, known as k-NN, Fuzzy Artmap, RBF,
Neural networks with reverse propagation and logistic regression offered
Respectively 91, 89, 88, 86 and 83 percent of errors.
*
X
X
X
about
about
X
X
X - class
1
O - class
1

Page 56
The k-NS rule directly approximates conditional probabilities of classes
P (j / x) referred to in Chapter 1 of formula 1.1. Estimation
The probability p (j / x) is the ratio kj / k, where kj is the number of nearest
Neighbors of class X objects from class j among k closest neighbors.
 The k-NS classifier can be treated as an approximation of the classifier
The probability p (j / x) is not estimated directly
But using the Bayesian formula, ie they are calculated on the basis of
Probabilities p (j), probability density distribution f (x / j) and density
Probability distribution f (x). If there is only a need to decide which
With the probabilities p (j / x) reaching the maximum value, then, as already noted
In Chapter 1, estimation f (x) is not necessary.
The environment of a class X object, containing k closest neighbors can
Be used to approximate the density function f (x / j) and f (x) distribution
Probabilities needed to estimate the probabilities p (j / x). Well
The probability p (j) of the appearance of an object of class j can be estimated
Directly from the learning set, as long as it was collected according to frequency
Occurrence of classes. Using the neighborhood of a classed object containing k
Object-oriented features of the Bayesian formula are as follows:
P (j) = mj / m, f (x / j) = kj / mj, f (x) = k / m. Replacing them with the Bayesian formula results in:
                               P (j / x) = p (j) f (x)
Ie the same estimate was obtained as before.
The presented approach is not the only one for estimating the density function f (x / j)
And f (x) probability distribution. With non-parametric methods, worth paying attention
Are so. The windows of Parzena [Duda R., O., Hart P.E., Stork D., G., 2001]. They are detailed
Also presented in book publications [Koronack J., Ćwik J., 2005; Stąpor
K., 2005].
Unlike the classifiers in which they are used
Distributing hyperplanes, classifiers acting according to the k-NS rule may be
Used for any number of classes. So there is no need, resort to
A parallel network of two-decision classifiers, which is illustrated in Fig.
However, you should consider whether such a structure does not offer higher
Quality of classification in case the classifiers are two-decision
Classifiers k-NS. Two factors can affect the quality of classification
Offered by a parallel structure compared to the quality of classification
Obtained for the standard k-NS classifier. The first is the designation
A separate number of nearest neighbors for each classifier, a
Second, separate selection of features. K
K
K
M
K
K
M
Mjjj

Page 57
Parallel classifier k-NS
Classifier, defines in the space features decision areas. Point in space
The attributes belong to the decision area of ​​the class, and if they would be classed by that class
Classifier. For the standard k-NS classifier on the boundary line
Third-class objects are also influenced by the two decision areas.
If the task concerns more than two classes. This is done through dependency
The number k and the selected set of features from the objects of all classes
Represented in the teaching set. In case of parallel structure, each of
Component classifiers are constructed for pairs of classes (i, j), i, = 1,2, ..., nc-1,
J = i + 1, i + 2, ..., nc, regardless of objects in other classes. Objects from these
The rest of the classes can affect the boundary between classes and only in phase
Vote constituent constituents, not voting objects, ie closest
Neighbors of the object currently classified. So it is worth to verify whether
A classifier with a parallel structure does not offer a lower probability of misleading
Classification. Classifier of parallel structure composed of two-pronged
The k-NS classifiers were probably presented for the first time in
Publication [Jóźwik A., Vernazza G., 1988]. A few years later the task of comparison
Classifier parallel to standard was taken at work [Jóźwik A.,
1994], using artificially generated learning sets
A two-dimensional feature space containing three classes.
In this work two series of experiments have been carried out for different purposes
The number of learning sets, but only the results of the series will be shown below
Experiments with more learning sets.
Classes occupy the area of ​​squares with vertices whose constituents were
Positive integers:
Class 1: a1 = [0, 0], b1 = [100,0], c1 = [100,100], d1 = [0.100],
Class 2: a2 = [100, 0], b2 = [200,0], c2 = [200,100], d2 = [100,100].
C3 = [100,200], d3 = [100,200], d3 = [100,200], d3 = [100,200], d3 =
In each of these squares, 1000 points of distribution were generated
Evenly distributed. The coordinates of x and y of points in class 1 are given the values ​​of numbers
Real in the range [0.100]. Points in class 2 assumed numerical values
The real ones with the x coordinates in the range [100,200], and the y coordinates z
Interval [0,100]. On the other hand, the x-coordinate of the points in class 3 assumed values
Real numbers in the range [0.100] and the y-coordinate of the real numbers with
Interval [100,200]. Coordinates of points were randomized to 0.01.
Classes are illustrated in Figure 4.2.

Page 58
                                   
Fig. 4.2. Class arrangement in a comparative experiment
 
Numbers k closest neighbors were determined experimentally, with
Use as a criterion for error fractions calculated by the minus one method
Element. It is obvious that the standard k-NS classifier must use both
The features of x and y and the parallel network of two-decision classifiers k-NS also. But
Each component classifier can use only one feature that could be
Selected as a result of feature selection. And so for a classifier designed for steam
Classes 1 and 2 will suffice for feature x, for the decisive classifier between classes 1 and 3
Sufficient for the pair of classes 2 and 3 would suffice
Both the x and y traits, but in the experiments only x was chosen for the experiments
A series of experiments consisted of generating 1000 points for each class
And calculation of error fractions for three k-NS classifiers: standard
Classifier, parallel network without feature selection and parallel network with
Conduct selection of features. Such series were 10. Fractions of errors were counted for
All possible values ​​of k. The smallest values ​​of these fractions have been selected
And the corresponding k values. The results of these experiments are shown in
Tab. 4.1.
 
Tab. 4.1. Error fractions for three different classifiers
 
Number
Experiment
Classifier
Standard
Parallel network
Without the selection of features
Parallel network with
Selection of features
1 0.0067 0.0057 0.000000
2 0.0030 0.0027 0.0000
3 0.0047 0.0043 0.000000
4 0.0017 0.000000 0.0000
5 0.0053 0.0027 0.0000
6 0.0000 0.000000 0.0000
7 0.0057 0.000000000000
8 0.0030 0.000000 0.0000
9 0.0027 0.0007 0.0003
10 0,0043 0,0000 0,0000
Class 3
Class 1 Class 2

Page 59
Designated numbers of nearest neighbors, for standard classifier,
They ranged from 1900 to 2100, but it should be noted that in cases
The largest numbers of k are chosen ambiguously. Numbers k fixed for
The parallel grid of classifiers was already much smaller, because they assumed values
From 1 to 230.
 In these experiments, there was a fine tuning effect of a classifier under a particular one
Data set, however, it was only using one parameter, a number
K. The author of this monograph intends to repeat in the future
Experiments with more abundant collections and with the evaluation of error fractions by both methods
Minus one element and using test sets.
The results of experiments suggest a parallel network of two-
Decision making. The above mentioned parallel network of two-decision classifiers was
The subject of deliberations in the work [Siedlecki W., 1994], where the author showed on the road
Analytical advantage of the parallelizer over the standard, from the point
View quality classification.
 
4.3. Construction of the k-NS classifier in the case of missing characteristic values
Biomedical studies often contain incomplete data sets, including
Also the data sets used to construct diagnostic rules, ie,
Classifiers. The easiest way to deal with such cases is
Removing either objects with missing attribute values, or removing features from
Missing attribute values. Considerations of the current chapter will be limited to
The first of the aforementioned methods. Number of objects rejected from the set
The learner depends on the selected features. It was assumed that only those objects would be rejected
There are gaps in value, but only in the qualities that will ultimately be
chosen.
In the following discussion, it was assumed that
The probability of wrong decision is estimated using the minus method
One element, discussed in subsection 2.1.
Character selection routines are usually used to select features
Either a further rejection of features or, possibly, a combination of these procedures. For each
Of the reviewed feature combinations is estimated classification error and finally
A combination of features is selected that offers the smallest classification error. number
Objects that need to be removed so that the remaining set of the learner is free of features from
Missing values ​​depend on the combination of features used.
If the procedure for the next rejection of the features is applied, then the initial rejection of the features
The learning set phase after removing objects with missing attribute values ​​will be

Page 60
Smaller, and as more features are discarded, these collections will have more
Abundance. On the other hand, in the case of the procedure of the next attachment of features, at the beginning
This procedure will work with larger learning sets that as you add
Further features will be reduced. Intuitively, it seems that the procedure
The next attaching feature should be more efficient than the next one
Rejection of traits.
Regardless of the feature selection procedure used, the number of rejected objects
The learning set may change, so the changes will also be proportional
Classes in the reduced learner sets. These changes make impossible
Reliably compare the combination of features for the faction they offer
Wrong decisions. But before any objects are removed can be assessed
Probability of occurrence of classes, assuming that the learning set was
Collected in accordance with the incidence of classes, as already mentioned in
Previous chapter.
If class counts are m1, m2, ... mnc, where nc is the number of classes and m is
The size of the learning set, you can estimate the probability of pk
Occurrence of classes taking pk = mk / m. Changes in the proportion of classes in a set
Teachers interfere: standardization of characteristics, counting fractions of errors, as already mentioned
The above, the calculation of the malfunction matrix, and also the decision rule that it has
Operate the classifier. However, knowing the actual proportions of pk multiplicity classes can
Correct each of the aforementioned stages of construction and assessment of the classifier.
Standardization of characteristics
 Considerations of this subsection will be devoted to the classical standardization which
It needs to calculate the mean values ​​of the characteristics and their standard deviations, as this
It results from the relation 1.5. Influence of the violation of the proper proportion of classes of values
Mvj and sdj, ie the mean and standard deviation of the characteristics, can be corrected. Including
Express the standardization parameters as mvk, j, sdk, j, and pk, that is, the mean and
Deviations of standard features computed for each class of k separately, as well
Probabilities pk, known a priori or designated by the formula:
    Pk = mk / m. (4.6)
Let objects of the classes under consideration have indices I (k), k = 1,2,3, ..., nc. So indexes
All objects are included in the set:
   I = I (k). (4.7)
The values ​​mvk, j count on the formula:
        Mvk, j = (xi, j) / mk, (4.8) nc
K1
) (kIi

Page 61
hence
      Xi, j = mk mvk, j. (4.9)
Taking into account (4.9), the calculation of the mean value of the jth characteristic allows the formula:
                Mvj = (xi, j) / m = (xi, j) m = (mk / m)
and so
                                                   Mvj = pkmvk, j (4.10)
Slightly more complex is calculating sdj as sdk, j and pk, because
Formula has character
2) / m = (- 2xi, jmvj +) / m =
= (- 2mvj +) / m = (- 2m + m) / m, that is
                                                   = -. (4.11)
The sum in formula (4.11) should be expressed as a function of standard deviations
Calculated for each class separately and probabilities pk. By similarity
To the formula (4.11) the standard deviations for each class are expressed
By:
                                                  = -, (4.12)
 From which it follows that
                                                 = Mk + mk. (4.13)
The sum needed to use formula (4.11) can be expressed as the sum of sums
Particularly defined relations (4.13) taking into account the relation (4.6) at the same time:
== (mk + mk) = (pkm + pkm) =
= M (pk + pk), ie,
                                              = Mpk (+), (4.14)) (kIi Ii 
Nc
K1 ) (kIi 
Nc
K1 
Nc
K1 sdj2 Ii
Ii
 xji2, mvj2 Ii
 xji2, Ii
 xji, Ii
 mvj2 Ii
 xji2, mvj2 mvj2 sdj2 m
1Ii
 xji2, mvj2 Ii
 xji2, sdjk2, km
 1) (k and xji2, mvjk2,) (kIi xji2, sdjk2, mvjk2, Ii
 xji2, Ii
 xji2, nc
K1) (kIi xji2, nc
K1 sdjk2, mvjk2, nc
K1 sdjk2, mvjk2, nc
K1 sdjk2, mvjk2, Ii
 xji2, nc
K1 sdjk2, mvjk2,

Page 62
From substitution (4.14) to (4.11) it follows that
= - = mpk (+) - = pk (+) -.
Ultimately,
  Sdj = (pk (+) -) 1/2. (4.15)
Calculating fractions of errors and misrepresentation matrices
Let the number of errors as a result of classification mk objects of class k is ek, then
The error fraction for class k will be bk = ek / mk. The total number of errors e will therefore be
E = ek and the fraction of errors for all classes
B = e / m = (ek) / m = (bkmk) / m, meaning on the basis of (4.6):
B = pkbk. (4.16)
From the relation (4.16) it follows that knowing the fractions of the classification errors for each class
You can determine the total fraction of classification errors.
Another task is to count the matrix P = probabilities that
An object from the k-th class will be assigned to the jth class. This matrix belongs
Determine from the matrix R = where rk, j is the number by rk
Objects of class k classed as j. If the number of classes k was consistent with
The shares of this class in the set of learners are the number of objects sk, j of class k
Included in class j would have been
      Sk, j = (rk, j / mk) lk, where lk = pkm. (4.17)
The number lk = pkm is the expected number of objects of class k if the class of this class was
Consistent with the statistics of the occurrence of this class in the teaching set. Probability
Pk, j can be calculated as: pk, j = sk, j / lk, which after substituting for sk, j, and lk gives:
           Pk, j = rk, j / mk. (4.18)
Relation (4.18) means that the probabilities pk, j are computed as if
Class frequencies in the learning set were consistent with the probability pk.
Definitely more complex is the calculation of the matrix qk, j probabilities,
The object that belongs to class k is actually derived from class j. Let sk be a number
Objects that would be classed as k if the classes were represented in
Learning set according to the statistics of their occurrence, as defined by the shares of pk,
K = 1,2,3, ..., nc, these classes in the learning set. Probability qk, j is calculated from sdj2 m
1Ii
 xji2, mvj2 m
1 nc
K1 sdjk2, mvjk2, mvj2 nc
K1 sdjk2, mvjk2, mvj2 nc
K1 sdjk2, mvjk2, mvj2 nc
K1 nc
K1 nc
K1} {, 1, jk
Nc
Jkp} {1, jk
Nc
Jkr

Page 63
The pattern qk, j = sj, k / sk. The numbers sj, k are calculated from the formula (4.17) and the number s using
p, jm, hence qk, j = (rj, k / mj) pjm / (rj, k / mj) pjm . Ultimately
When shortened by m, you get:
 Qk, j = (rj, k / mj) pj / (rj, k / mj) pj. (4.19)
The corrected rule k-NS
Presented ways of solving the task of correcting standardization
The characteristics, counting fractions of errors, and the matrices of the errors refer to any
Type of classifier. However, it is particularly convenient to use them in classifiers
Acting according to the k-NS rule, because for this rule a method is very convenient
Minus one item. In addition, the k-NS rule can be easily corrected for
Circumstance of disturbance of proper proportions between class frequencies in the set
Learners.
In the smallest hyperkalemia Hk containing k nearest neighbors is located
A number of ki objects in class i. The fraction ki / m is the fraction of objects in class i,
Which are among the nearest neighbors. Had the class share and in the collection
The instructor was pi, it was in the size of a learning set equal to m in class and it would not be me
But I'm writing objects. Hence, the number of objects in the hyperkalemia Hk in the class i would be:
Qi = (ki / mi) pim.
It is easy to see that the multiplier m has no influence on the choice of the class i corresponding
Maximum value of qi. So the object to be classified should be
Assigned class and for which number
              Qi = (ki / mi) pi (4.20)
Achieves the maximum. This is how the classifier's function is defined
Corrected rule k-NS.
Required form of input data
The first line of the file with the learning set should contain 3 numbers: the number of classes
Nc, the number of features n, and the number of objects m. Each of the remaining lines contains
The first column of the class number, and the other columns of that row are values
Object characteristics, as illustrated in Table 4.2.
In place of missing feature values ​​any number can be entered
Real. Information on missing features is included separately
A set of m lines and n columns and values ​​0 or 1. The number zero in i-th
The row and jth column denotes the missing jth value for the i-th object. Yeah
Notation allows you to easily identify which of the objects of the learning set are for a given
Combination of active features. For example, multiply the binary values ​​of the corresponding nc
J1 nc
J1 nc
J1 nc
J1

Page 64
Row from the missing plate. The zero value of this product will denote that given row
Is not active, ie the corresponding object was temporarily removed from the learning set.
 
Tab. 4.2. Form of data preparation (without the first line of the learning set)
No. Class Feature 1 Feature 2 Feature 3 Absence Table Activity
1 1 3.4 2.1 5.3 1 1 1 1
2 1 2,3 3,1 2,5 1 1 1 1
3 1 1.0 4.4 3.2 1 1 1 1
4 2 5,2 4,5 7,4 0 1 1 0
5 2 4.8 5.5 6.7 1 1 1 1
6 1 2.7 4.1 4.0 1 0 1 0
7 1 1.9 3.9 2.7 1 1 1 1
8 2 6,7 4,7 6,0 1 1 1 1
9 2 5.0 6,4 6,5 1 1 1 1
10 2 5,8 6,1 6,9 1 0 0 0
 
In Figure 4.3. A simple example has been shown to illustrate the counting method
Fractions of errors and malfunctioning matrix. In the simplest of Fig.4.3a, it can be established that p1 = 4/8 = 1/2
And p2 = 4/8 = 1/2.
(and)
X1 x2 x3 x4 x5 x6 x7 x8
X x x o o x o o
(B)
 Teaching set x1 x2 x3 x4 x5 x6 x7 x8
 Table of deficiencies 1 1 1 0 1 1 1 1
 
Fig. 4.3. Example for illustrating the operation of the corrected 1-NS rule
 
On the other hand, Fig.4.3b indicates that the value of the feature for object x4 is missing. Course
The method minus one element is as follows. After removing the object x4 each of
Objects, except x5 and x6, have as their closest neighbor the object of the same as he
Class. That means that only x5 and x6 are misclassified.
In class 3 the objects were correctly classified during the run
The procedure minus one element and one object is wrong, and in the second class two
The objects were correctly assigned and one was wrong.
The results of the method minus one element are shown in Tab.4.3.
It contains the I, II, and III type matrices, as defined in subsection 2.2.
Fraction of errors for the first class is b1 = 0.25 and for the second b2 = 0.33, therefore
The error fraction for both classes is b = p1b1 + p2b2 = 0.50,25 + 0,50,33 = 0,290.

Page 65
 
Tab.4.3. The error matrix for the data in Fig.5.1 and the corrected 1-n rule
 Matrix Matrix Matrix
 1 2 1 2 1 2
1 3 1 1 0.75 0.25 1 0.69 0.31
2 1 2 2 0.33 0.67 2 0.27 0.73
 
No correction due to lack of data, error fractions
It would be b = 2/7 = 0.286 (two mistakes for seven objects, because one would have been
Removed).
This difference is very small, because the teaching set is very small and is lacking
Only one data. If the correction to the absence of a given matrix P was canceled
It would remain unchanged. Conversely, the matrix Q, in the case of resignation from the correction to
The absence of a given would be identical in this case to the P. matrix. This means that
Applying the corrected rule 1-NS changes in this case the matrix Q
Significant.
 
4.4. Blurring the k-NS rule
Affiliation of object x to class j can be written as a membership vector
Vo = [01, 02, ..., 1j, ..., 0nc], where nc is the number of classes and the index "o"
As opposed to fuzzy decision. There is no need for such a record
Count the numbers k nearest neighbors, just calculate the average vector of the vectors
Affiliation of closest neighbors, then the component of the resulting vector
The highest value is set to 1 and the remainder to be assigned
Values ​​equal to zero. If the three closest neighbors x1NS, x2NS and x3NS, in the case
The two classes, v1NS = [1,0], v2NS = [0,1] and v3NS = [0,1], belong to vectors
The average of these vectors will already be blurred and will be vR = [1 / 3,2 / 3], and after changing 2/3 to
1 and 1/3 to 0 produce a sharp vector of membership vO = [0,1]. Voting result
The closest neighbors are measured in this approach, not numbers, but fractions
Votes cast for each class. In the example given, these fractions amounted to
1/3 and 2/3. This implementation of the k-NS rule is more general because it also covers
The case when the affiliation of objects in a set of learners is described as blurred
Vectors of affiliation. Components of membership vectors accept values ​​from
Range [0,1] and add up to unity.
If the classification concerns only acute decisions, then the point of the characteristic space, in
Who is i-th nearest neighbor from k nearest neighbors, belongs to
Each of the classes j with the unknown probability pj, iNS = p (j / xiNS), j = 1,2, ..., nc. His} {, ji
Nc
JirR} {, ji
Nc
JipP} {, ji
Nc
JiqQ

Page 66
Affiliation can be described with the fuzzy vector viNS = [p1, iNS, p2, iNS, ..., pj, iNS, ..., pnc, iNS].
So, i-th nearest viNS neighbor should not cast his whole vote on
One of the classes, but divide its voice among all classes by share
Indicated by the pj, iNS components of the viNS vector. These components are not known, however
They can be estimated.
The closest neighbor of a classified object can be any object of a set
The learner must be assigned a new learning set for each object xi in the learning set
Vi = [p1, i, p2, i, ..., pj, i, ..., pnc, i]. Its components can be
Estimate according to the following rule.
Rule I blurring affiliation
If xi is from class l,
Then pj, i = p (j / xi) = kj / (k + 1) when jl, (4.21)
And j = 1, where j = 1, for j = 1,2, ..., nc,
Where k is the number of closest neighbors of the object xi, of which kj belongs to the class j.
 Such a rule of blurring of belonging results from the perception that object xi,
Who is of class l gives voice to his class, that is to class l
Acute classification problem with learning set of objects whose affiliation
It is defined by class numbers. The rule (4.21) can only be used for a learning set
Once, because once it is used once every object xi i = 1,2, ..., m, set
The teacher has assigned a new vector of membership vi = [p1, i, p2, i, ..., pj, i, ..., pnc, i], which
In general, it is already a vector of fuzzy affiliation. Just in case
All his closest neighbors came from the same class, eg j, it would be:
Vi = [01, 02, ..., 1j, ..., 0nc].
If the affiliation of objects in a set of learners is marked not through
The number of classes, but vectors of membership of the form v = [p, p, ..., p, ..., p], then
Another rule II should be considered, which can be repeatedly applied
For the learning set, hence the upper index indicating that the vector of affiliation has arisen
As a result of repeated use of the rule of blurring affiliation. In case of
For acute classification, for r = 0, all components of membership vectors are binary,
Ie for each and vector vi = [01.02, ..., 1j, ..., 0nc] if xi is from class j. In other words,
R = 0 means that the vectors xi have an original attribution, not yet
Not washed.
Rule II blurring affiliation
            V = [p, p, ..., p, ..., p] = (vi, hNS + v) / (k + 1),
And ri, 1 ri, 2 rij, rinc, 1r
And 1,1ri 1,2ri 1, rij 1, rinc 
K
H1 r
and

Page 67
Where the lower index "i, hNS" means the vector of affiliation of h-th in turn
Closest neighbor of object xi. The division by k + 1 in formula (4.22) implies that
The resulting vector of belonging v is formed as the mean vector of belonging to
Vectors of their k neighbors and their own.
 The problems that need to be resolved are: choosing k nearest neighbors and
Criterion of its choice. The k-NS rule is supposed to work with the learning set whose objects
Will have new vectors of affiliation. Natural seems to be the hypothesis that
Components of vi vectors should accept such values ​​to get on them
The basis of error fractions were as small as possible. Hence the simple conclusion that as k belongs
Accept the value for which the error fraction, denoted by, for example, the minus method
One element is the smallest. The k-number chosen so far is defined clearly
Components of fuzzy vectors of affiliation.
Since the above vector vi belongs to the i-th object of the learning set
Has nc components, and the learning set has m objects, then these vectors can be
Create a membership matrix of the learning set W0 containing m lines and nc
Columns. The teaching set may consist of objects that have been blurred
Vectors of affiliation whose constituents may be less than 1. Then the error
The single classification counts according to formula 1.4 given in subsection 1. Worth
Note that the components of the fuzzy vectors of affiliation need not have
Interpretation of probabilities of belonging to classes. They can be the components
Generally understood degrees of class membership, eg shares of different metals in the alloy,
If the purpose of the classification was recognition of alloys. In the further
By default considerations will be taken that the components of the vector belong
There will be degrees of belonging to each class.
If the object of interest is classification with acute decisions, objects
In the learner set they have class membership, then describe them
The vectors of character belonging to vo = [01.02, ..., 0nc] arise in binary
Matrix of affiliation W0. Based on the II rule of blurring affiliation
Defined by relation (4.22) and methods minus one element (or another method
You can create an infinite sequence of triangles:
             (W0, k0, e0), (W1, k1, e1), )
Within this range kr is the number of nearest neighbors for which the classifier acting according to
The k-NS rule with Wr matrix achieves the smallest fraction of er of classification errors. Mother
Affiliation W1, where class objects of the learning set are marked
The number of classes should be determined by applying the rule (4.21) of divergence.
In contrast, in other cases, the Wr + 1 matrix is ​​used for the II
Blurring rule (4.22). Rule I can be avoided if 1r
and

Page 68
Assignment of objects belonging to learning set with class numbers will be
Transformed into binary vectors of affiliation and a matrix will be created
W0.
 The string creation (4.23) continues as long as er + 1 <er. With the moment,
When er + 1 er, string generation (Wr, kr, er) is aborted. Finally the classifier
It will work with the Wr's matrices matrix and the kr-NS decision rule. Relation
(4.23) defines the fuzzy learning scheme of the k-NS rule [Jóźwik A., 1983b].
This algorithm was analyzed in detail by Bezdeek and co-authors [Bezdek
J.C., Chuah S.K., Leep D., 1986].
The method of determining the identity matrix W1 will be illustrated at
One-dimensional set of two classes, shown in Fig.4.4.
Example 1
The following example applies to 10 objects, of which 5 are marked with crosses
It belongs to class 1 and the other 5 is marked with circles to class 2.
Value of feature 0 1 2 3 4 5 6 7 8 9 10 11 12 13
Object class x x x x o x O o o o
Object x1 x2 x3 x4 x5 x6 X7 x8 x9 x10
Fig. 4.4. Data for action illustration and blurring rules for affiliation.
 
Applying the minus one method to a classifier acting as
The 1-NS rule gives you three errors. The x4, x5 objects will be mistakenly recognized
And x6, because each of these objects has as its nearest neighbor
Object from the opposite class. And for rule 2-NS and 3-NS object x4 will already be
Properly classified. There is no difference in the classification of this object
Between the 2-NS and 3-NS rules, because the same objects are in the vote as
Closest neighbors The second and third closest neighbor of this object are from x4 in this
So far, so anyway in rule 2-NS takes locally the share of the three closest
Neighbors.
For an optimal number of nearest neighbors you can assume k = 3. Odd number
K in two-decision tasks is a reliable security against
Inconsistent voting results of k neighbors. After establishing the number of coming
Neighbors can apply I or II rule blurring affiliation.
The x4 object has in its vicinity three nearest neighbors, objects x3, x5, and x6,
Hence k1 = 2, k2 = 1 and belongs to class l = 1. Thus, according to the rule I, ie the relation (4.21)
P1,4 = p (1 / x4) = (k1 + 1) / (k + 1) = 3/4; p2,4 = p .
However, using rule II seems to be more convenient, because according to (4.22): 
0r

Page 69
V = [p, p] = v4.1NS + v4,2NS + v4,3NS + v = ([1,0] + [0,1] + [1,0] + [1,0] [3 / 4,1 / 4].
The upper index at vectors, in case Rule II was introduced because the rule
This can be used as many times as previously.
Similarly, you can designate new vectors for affiliation
The remaining 9 objects are shown in Tab.4.4.
 
Tab.4.4. Illustration of blurring of acute membership matrix
Matrix of affiliation W0 Matrix of membership of W1
01: [1, 0] 06: [1, 0] 01: [4/4, 0/4] 06: [2/5, 3/5]
02: [1, 0] 07: [0, 02] 02: [4/4, 0/4] 07: [1/4, 3/4]
03: [1, 0] 08: [0, 03] 03: [4/4, 0/4] 08: [0/4, 4/4]
04: [1, 0] 09: [0, 04] 04: [3/4, 1/4] 09: [0/4, 4/4]
05: [0, 1] 10: [0, 05] 05: [3/5, 2/5] 10: [0/4, 4/4]
 
 In the example presented in the voting, mostly four objects were involved,
Ie, the object for which a new vector of affiliation was created, and three of them
Closest neighbors. An exception occurred for objects x5 and x6, for which the third
The nearest neighbor was in the same distance as the fourth.
 In 1985, three other fuzzy rules appeared in the literature
K-NS proposed by Keller et al. [Keller J.M., Gray M.R., Givens J.A.,
1985], the second of which described in the aforementioned work refers exclusively to two-
The decision-making task of classification and therefore will be omitted in this monograph.
The other two methods differ from each other by ways of determining the initiating degrees
The membership of pj, and objects xi from the learning set to each class j.
The first rule uses directly from the binary matrix W0, ie pj, i = 1, when the object xi
It belongs to class j and pj, i = 0, when it does not belong to it. The second rule, described by the relation
(4.24), initiates the already fuzzy matrix of objects belonging xi from the learning set.
Components of vectors consisting of matrix W0, this time not binary, counted
Are according to the formula:
                                            Pj, l = 0,49 + j, l0,51, (4.24)
Where j, l assumes value 1 when j = l and value 0 when jl, where l is a class
Object xi. Degrees of affiliation of newly classified objects are determined by
Fairly complex rule:
             2 / (l-1)], (4.25) 1 / d (x, xhNS)
4 14.1 14.2 0
4 k
Kj 
K
H1 
K
H1

Page 70
Where pj, hNS is the degree of affiliation of the h-th nearest classifier
The object x to the class j, d (x, xhNS) is the distance between the object x and its h-th
Nearest neighbor, k is the predetermined number of nearest neighbors and l is
The rule parameter (4.24) greater than unity.
The values ​​of degrees of belonging pj, hNS nearest neighbors, occurring in the formula
(4.25), are elements of the matrix W0, irrespective of whether its elements are binary,
Or are they defined by the relation (4.24).
Example 2
 For a better understanding of the pattern (4.24) the degree of belonging of object x5, z
Example 1 will now be calculated from this formula, assuming k = 3. In the same
The distance from the object x5, as its third closest neighbor, are two objects and this
From opposite classes, ie object x3 and object x7, this means that it should be applied
4-NS rule. Among these 4 nearest neighbors, k1 = 3 is from class 1 (crosses) and k2 = 1
With class 2 (circles). The object x5 is of class 2, and therefore l = 2. According to the formula (4.24) object
X5 receives new degrees of affiliation:
P1,2 = 0,49 + 00,51 = 0,3675, p2,2 = 0,49 + 1,51 = 0,6325, ie v5 = [0,3675; ],
That means that predominates membership in class 2. In Example 1, as is apparent from
Table 4.4. The object x5 has been assigned another blurred vector affiliation, i.e.
V5 = [0.6, 0.4], ie the degree of belonging to class 1 is higher, although the object
It actually comes in class 2. Formula 4.24 will always give the object a higher grade
Belonging to the class from which it actually originates, while formulas (4.21)
And (4.22), referring to the fuzzy k-NS rule proposed by the author
This monograph [Jóźwik A., 1983b], can give the object the highest grade
Belonging to a class other than the class from which it originated. According to the author
This monograph is a serious flaw of the fuzzy k-NS rule described in the paper
Keller et al. [Keller J.M., Gray M.R., Givens J.A., 1985] as a single point
Representing any one class, even surrounded by very dense points
Representing another class will always have a higher grade assigned
Belonging to the class from which it originated.
Because in the formula (4.25) the functions of distance appear as their inverse,
This closer neighbor has a greater effect on the value of pj, x. With the increase in parameter l
Distances from neighboring neighbors are less affected. As parameter l
Will be closer to 1, closer neighbors will have a greater impact on value
Pj, hNS. Unfortunately, in the above-mentioned publication, Keller et al
A classified object will coincide with one of its nearest neighbors. Distance
D (x, xhNS) will then take a value of zero and the formula (4.24) will be 4
3 4
1

Page 71
impossible. The exclusion of such a near neighbor would be a falsification
Situation. In the extreme case, all the closest neighbors can cover up with
Classified object.
  
4.5. Classification using class areas
In some applications, estimated error fractions, regardless of type
The proposed classifier, may be large enough that the construction of the classifier
Loses its meaning Exit from this situation can be to designate from the space features of the areas
Classes, and thus the overlap and coverage of classes. The boundaries of these areas do not
You must describe analytically. It's enough to have an algorithm that will tell you,
Does the object belong to the specified class? If it should be at the same time
To two or more classes, that is, it belongs to the overlapping area
These classes. The classifier can make decisions only if the object finds it
Get exactly in the area of ​​just one class. For the purposes of this chapter more convenient
It will change the existing symbols for the teaching set and its subsets
Representing classes.
Let the disjoint sets U1, U2, ..., Unc divide the set of the learning set and their sum
Multiplication generates as a result the set U for the subsets of these subsets can be defined
As follows, some numbers ei, and areas Ai:
 
        Ei = max d (Ui {uj}, {uj}), (4.26)
                                        Uj Ui
                                    
        Ai = {x: d (Ui, {x})  ei}. (4.27)
                                                                  
Where d (, ) is a function of the distance between two sets, understood as
The distance between the nearest points, one of which belongs to one and the other
To the second set. It is easy to see that ei is the greatest distance between
A class object and its closest neighbor from the same class. And Ai area
Geometric location of points distant from U and at most ei. Illustrative example
Class areas are shown in Figure 4.5.
Classifier defined by the above mentioned classes areas can be
Characterize by giving two fractions: the fraction of the wrong decision and the fraction of the missing
Decision (answer: do not know). The correct decision will only take place if,
When the object is from class i and will be in the area of ​​this class. Wrong happens
Only when the object will be in the class and will fall into the area only one but another
Class j, ij. If the object is in the Ai area, the overlapping classes or outside
The area of ​​each class, the response of the classifier will be a decision I do not know.

Page 72
                       
Fig. 4.5. Illustration of class areas for Euclidean metric and two classes
 
There are several solutions to reduce the number of non-type decisions
I know. One of them is the use of objects in the area of ​​overlap of classes,
K-NS rules. However, a better solution is to perform two stages
Selection of features. In the first stage, the selection of features is performed using,
As a criterion, the class overlap fraction. Then, for objects from obtained in
This way of overlapping classes, the second feature selection is performed with
The error fraction as a criterion for k-NS. This approach has been applied to
Aerial photography [Jóźwik A., Sernico S., Roli F. 1998]].
Below is a simple example, shown in Figure 4.6, illustrating
Defining class areas and applying the minus method to one element.
Example
     U1 u2 u3 u4 u5 u6
     X x x o o o
                        
                        
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
Fig 4.6. Example of two classes with marked areas A1 and A2.
 
The objects shown in Fig. 4.6 for convenience are described by features whose values ​​are
Integers In class 1 (crosses) the greatest distance to the nearest
The neighbor has an object u3 and it is e1 = d (u3, u2) = | 10-7 | = 3.
Similarly, for class 2 (circle) the greatest distance from the object to its
Nearest neighbor equals e2 = d (u4, u5) = | 16-13 | = 3. The numbers e1 and e2 satisfy the condition
(4.26). It is easy to now designate A1 = [2,13], A2 = [10,21] and the area A1,2 = [10,13].
The wrong decision fraction and the type response fraction I do not know is estimated
By the minus one method, and the method of estimation is explained in Fig. 4.7. While,
Detailed results obtained by the minus one method are given in Table 4.5.
The minus method of one element is sequentially taken from the learning set
Object ui and classed on the basis of the classes of classes defined for the set
Other objects, ie for the set U- {ui}.
 
A1 A2

Page 73
 
 
 
U1) u1 u2 u3 u4 u5 u6
     ■ x x o o O
                        
                        
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 
U2) u1 u2 u3 u4 u5 u6
     X x x o o O
                        
                        
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 
U3) u1 u2 u3 u4 u5 u6
     X
                        
                        
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 
U4) u1 u2 u3 u4 u5 u6
     X x x * * o O
                        
                        
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 
U5) u1 u2 u3 u4 u5 u6
     X x x o ■ O
                        
                        
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 
U6) u1 ​​u2 u4 u5 u6
     X x x o o ■
                        
                        
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 
Fig. 4.7. How to calculate fractions of the wrong decisions and fraction of answers do not know
  Method minus one element
 
In the example given, two selected objects are mistakenly misrepresented
Ie objects u3 and u4. These objects are close to the border between
Classes and each of them fell into the class of the opposite class. None of the six objects did
He was outside the class areas, nor did any object fall into the coverage area
Get classes.
The numbers ei, i = 1,2 ..., nc, do not have to be in the running of the run method
Method minus one element designated each time before each classification
Another object according to the formula (4.26). They can be fixed on a one-off basis
A1 A2
A1
A1
A1 A2
A1
A2
A2
A1 A2
A2

Page 74
The entire set of learners. Of course, interpretation of the results would have been then
Different.
 
Tab. 4.5. The results of the method run minus one element for the example of Fig. 4.7
 Object Class E1 e2 A1 A2 A1.2 Classification
U1 = [5] 1 3 3 [4,13] [10,21] [10,13] 1
U2 = [7] 1 5 3 [0.15] [10.21] [10.15] 1
U3 = [10] 1 2 3 [3,9] [10,21] 2
U4 = [13] 2 3 2 [2,13] [14,20] 1
U5 = [16] 2 3 5 [2,13] [8,23] [8,13] 2
U6 = [18] 2 3 3 [2,13] [10,19] [10,13] 2
 
 It is expected that as the multiplicity of the learning set increases, the value is increased
The numbers e and will be less diminished. Occurrence of outlying objects (noise) may occur
Make the use of relation 4.26 to designate e and lose meaning, because
Areas of overlapping classes may contain too many objects from a set
The instructor. In this case, the experimental selection of these may be more favorable
Numbers.
It is worth noting that during the course of the method minus one element changed
Undergoes only the class of the object from which the object intended to be taken
Classification.
The idea of ​​class areas can be used to construct a classifier
Multi-stage. Objects from the area of ​​overlapping classes in the stage with the index and can
Be a teaching set for stage with number (i + 1).
The mode of operation of such a classifier was explained on the example in which
The Iris Data set was used, and the results are given in Table 4.6.
In the first stage, column I, the class overlap area contained 46 objects,
Apart from all the areas there are 3 objects, no object was wrong
Classified and 101 correctly classified. Objects in the area
The overlap of classes in the first stage, which was 46, was a teaching set
Second stage. In the second stage, column II, the overlap of the classes contained 22
Objects, one object found outside the areas of both classes, no one was wrong
Classified, and 23 objects were correctly classified. Content
The remaining columns, whose numbers are also the stage numbers, are interpreted
similarly.
The stages were as much as it was possible to calculate, though from a practical one
The point of view is the sensibility of using more than three stages in this task
Is at least questionable.

Page 75
Tab. 4.6. Results of multi-stage classification for the Iris Data collection.
Description of the situation - rows / stage number - columns I II III IV V Total
Number of points in the teaching set 150 46 22 12 6 -
Number of points in total coverage areas 46 22 12 6 0 -
Number of points with a decision I do not know 3 1 0 2 0 6
Number of badly classified points 0 0 1 1 1 3
Number of points properly classified 101 23 9 3 5 141
 
Multi-stage classifier
 A classified object may be in different locations with respect to
Class areas defined at each stage. Depending on this
Positions are made stepwise decisions according to the following scheme.
1. The classified object x does not lie in any of the Ai areas. Then the decision is:
I do not know. Areas Ai may be less than the number nc of the classes in question.
Some or all of them may be in particular a zero measure, ie contain
One point or several points overlapping. From a mathematical point of view
Views would not be areas.
2. The classified object is located exactly in one of the Ai areas. Point is left
Assigned to class i.
3. At the current stage, the overlap of the classes in which it is classified
The object contains exactly the same objects as the corresponding overlap area
Classes from the previous stage. The object is then assigned to the selection:
A) The most represented class in the area under consideration
Overlap classes, and when this step does not settle, follow the same principle
For the area overlapping with the step by two preceding steps (because step
The previous contains the same objects) and possibly take the appropriate area
From the stage of three degrees earlier, etc., possibly finishing all over
The original teaching set. In case of non-settlement is taken
Random decision
B) Class selected with the k-NS rule.
4. The object belongs to the overlap area of ​​the ncp classes, and the contents of this area are different
From the content of the corresponding area of ​​the previous stage. Points of this
The area is then set up for the next stage. They are built
New areas Ai, they are ncp this time. The number of ncp may be less than the number
Nc considered classes. In the area of ​​overlapping classes, at the current stage, they can
Not all classes are represented. For a classified object will thus
Next, cases 1, 2, 3 and 4 are considered from the beginning.

Page 76
Definition of the algorithm (slightly modified) and examples illustrating its operation
They come from the publication [Jóźwik A. and Stawska Z., 2000].
 
4.6. Reduce reference sets
A collection of objects that must be remembered in the classification phase is called
Reference set. It may be a subset of a learning set or a new set of assemblies
From the points artificially generated from the learning set. In the case of a classifier
The minimum-distance reference set is the most commonly used set of nc measures
Gravity or median means of the classes under consideration, as already mentioned.
Classifiers using the k-NS rule need to be stored in memory
The computer of the whole learning set as a reference set. Size of the set
Reference has a significant effect on the speed of classification.
A classifier can, for example, be used to analyze optical images, in
The recognized objects are the pixels described by the attributes set on
An example based on grayscale pixels of the neighborhood [Jóźwik A., Kieś P., 2005].
The number of learning sets can then be several dozen or more
Thousand. Classification speed, using the k-NS rule, could not be
Acceptable. Exiting from the situation could in this case apply
Minimum-distance classifier or classifier proposed in
Subsection 2.8, constructed on the basis of the set of learners edited for
Linear separation and utilization of the parallel structure of Fig. 1.2. Quality
However, classification in these solutions may not be sufficient.
Another solution might be to use the nearest neighbor's classifier,
As a special case of using the k-NS rule for k = 1. This classifier also requires
Normally remember the whole set of learners as a reference set, but
It works much faster than the k-NS classifier for k> 1, but it will not
Fast enough for a collection of dozens or hundreds
Thousand objects. Therefore, many reduction algorithms have been developed for Rule 1-NS and
Condensation of the reference set. In addition, the k-NS rule can be approximated by the rule of 1-
NS. In order to make this approximation the binary matrix of membership of W0,
Using one of the formulas 4.21 or 4.22, transform into a fuzzy membership matrix
W1, and then each of the fuzzy membership vectors composing it
Convert to the class number corresponding to its largest component. E.g,
The object x5 in Fig. 4.4 would change the membership of class 2 to class 1 as it appears
From Table 4.4.
Another solution would be to develop a fast-paced algorithm
The nearest neighbor, which was proposed, eg in the trial [Grabowski S.,
2003].

Page 77
The subject of further consideration will be the reduction and condensation algorithms
Reference set for rule 1-NS.
The Hart algorithm
 The Hart Method [Hart P.E., 1968] works as follows. First object u1 z
The original reference set O, ie the set U, O = U = {ui},
Is qualified for the reduced reference set Z. Another object ui
It is classified according to the rule 1-NS operating with the current set
Reduced Z. If it is misclassified, it is included
To the current reduced set, ie Z: = Z {ui}. After the classification of the last of
Objects of the set O are subsequently classified in this way objects of the set O,
From object u1 to object um. Such a classification cycle is repeated until
The next m of this type of classification will be correct. The resulting Z collection
Is the resulting reduced reference set.
This algorithm is defined as consistent (with the original set of references),
Because the 1-NS rule working with it as a reference set properly allocates
Class objects from set O. The disadvantage of this algorithm is the recruitment to the set
Reduced, in its initial phase, objects lying far from class boundaries.
Such objects are probably redundant. That is why [Gates G. W., 1972]
Suggested removal of such objects. If you remove the object from the reduced
The reference set does not violate the compatibility of this set with the original set, it is
The object is permanently deleted from the object. Another solution, suggested by the author
This monograph is a repeat of the Hart procedure, but this time with the modification
This means that first objects are presented from the set Z in order
In reverse to the one in which they were recruited and then made
Presentation of other objects, ie objects from the set O-Z.
Tomeka algorithm
In the publication [Tomek I., 1976] two reduction algorithms have been proposed
Reference collections, however, only one of them has gained the interest of readers.
This algorithm works as follows. For each pair of objects x and y belong
For different classes, a hyperkernel with center at (x + y) / 2 and radius is constructed
R = d (x, y) / 2, where d (, ) is a Euclidean metric. This is a hyperketula spread on the points
X and y, whose diameter is the connecting section x from y. If the interior of this hyperkinel does not
Contains any object from the reference set O, then both objects x and y go to
Reduced reference set Z. The author of the algorithm gave the theorem for
The fact that the received collection is reduced is consistent, but it turned out to be false
[Toussaint G.T., 1994], as shown in Fig. 4.8.
 M
I1

Page 78
X
about
about
about
X1
X2 x3
X4
 
 
 
 
 
 
 
 
Fig. 4.8. Counterpart for Tomek's assertion
 
 Objects x1 and x2 form a pair of tome, because the inside of the ball spanned on them is not
Contains no object. And the ball spanned on objects x1 and x3 has in its interior
Object x2, and the ball spanned on objects x1 and x4 contains within it an object x3. Set
The reduced contains only objects x1 and x2. Objects above
The straight line H is closer to the object x1 than to the object x2. The x4 object will therefore be wrong
Classify and spoil the compatibility of the reduced set. Join the collection
The reduced x4 object will make it a consistent set.
This algorithm is easily repaired. Just attach additional
Objects using the Hart algorithm. Some pairs of objects in the set
Reduced may be unnecessary.
Modification of Gowda and Krishna
Interesting proposal to modify the algorithm Hart was proposed several years
Later in the publication [Gowda K.C. And Krishna G., 1979]. This modification can be done
Much simpler to describe than the authors of the aforementioned work. For this purpose
It is worth introducing the concept of the position measure mp (x) of the object x. Let y be
Closest neighbor of the object x coming from the opposite. Number of objects from this
The same class as x that is no further from the y object than the x object is a value
The position measure of the object x. In other words, if x is the k-th closest neighbor
Object y, from objects of this class from which x originates, is mp (x) = k. For everyone
The object x of the reduced reference set O is measured mp (x) and the distance
D (x, y) to the nearest object of the opposite class. Assembly objects are ordered
By increasing values ​​of mp (x) measures, and objects with equal values ​​of those measures by
The distance and also ascending. For an orderly set of O, we should apply
The Hart algorithm.
It is obvious that objects with smaller measure values ​​mp (x) and distances
D (x, y) lie closer to the class boundaries.
H

Page 79
Modifications using the representativity measure of the object
Another way to use the Hart algorithm, after the previous one
The original reference set was proposed in the dissertation
Doctoral dissertation [Raniszewski M., 2009]. The author introduced a measure of representativity
Rm (x) of object x measured by the number of objects from the learning set for which this object
Is the nearest neighbor. Teaching set objects are sorted first by
Decreasing rm (x) values, followed by objects with the same values
Rm (x) are sorted ascending according to the mp (x) measure proposed by Gowda and Krishna.
Both of these measures can be used in reverse order.
Apart from describing the aforementioned approaches, the above-mentioned treatise contains a description of many
Known reduction algorithms, including several other than the aforementioned,
Proposed by the author and description of the state of knowledge in this subject.
Method of objects intertwined.
 This is the second algorithm [Jóźwik A., Kieś P., 2005], beyond the already described algorithm
Tomeka, in which the pairs of objects of different types are recruited into the reduced set
Classes, and the resulting collection is completed using the Hart procedure.
However, in this algorithm, pairs of qualified objects are reduced to a set
They are defined in another way, which is illustrated in Figure 4.9.
 
 
 
 
 
 
 
Fig. 4.9. Method of designating objects coming closest to each other
 
 Each object from the original reference set is assigned
Pair of objects intertwined, coming from two classes, class 1 and class 2.
The procedure must be performed for all the objects of the set to be
reduced. For any object x, eg from class 1, it starts with designation
Nearest neighbor s1 from the opposite class, ie from class 2. Then, for object s1
Looking for his closest neighbor s2 from class 1. Then, for s2 is determined
Closest neighbor in class 2, etc., until we finally get a loop, ie starting from
One will be dealing with sl + 2 = sl. To avoid a loop that covers more than two
X
X x
about
about
about
X1
X2
X3
X5
X4
X6

Page 80
Objects, it should be assumed that in the case of ambiguous objects is selected from
Lower index.
In the situation shown in Figure 4.9 for object x = x2 from class 1 was found
The nearest object is s1 = x4 from class 2. For object s1, the closest object with
The opposite class is the object s2 = x3 from class 1, and the closest to s2 is the object s3 = x5 z
Class 2. Object s3 has as its nearest neighbor object s5 = x3.
Termination of the task of designating pairs of objects intertwined for
Object x2 has been fulfilled. Objects x3 and x5 represent pairs of objects mutually
Closest to the x2 object.
 The presented algorithm applies only to two classes, and compared to
Like his Tomeka algorithm, pairs of objects intertwined are much
Less than tome pairs. This collection should be completed using the procedure
Harta.
Modification of the Tomeka algorithm
 Tomek's algorithm sets pairs of objects a and b from different classes. Based
Such a pair can be constructed of a hyperplane passing through the middle of the segment
Linking points a and b and orthogonal to it. With this hyperplane you can
Bind the new artificial feature of the object x, which will accept only 3 values: +1,
When the object x is located at the side of the point a, 0, where x will lie on that
Hyperplanes and -1 when it is on the side of the point b. Zero can be
Eliminate if the objects lying on the hyperplane align
+1 value instead of zero. This conversion can be done for each object
Original reference set O.
So each tome pair of objects corresponds to each other unambiguously
New artificial feature. The use of the selection of artificial features will therefore be associated with
Reduction of tome pairs. Objects that should be in the set
Reduced, so they will be indicated by the characteristics, because each of the characteristics specifies a pair
Tomekowa, from which objects will come into the reduced set. A collection will be given
Reduced artificial features in the space. In the new feature space, in the 1-
NS, you can apply both the square of the Euclidean distance and the distance
Urban. Newly classed objects must be converted to new ones
Character spaces using only those hyperplanes that correspond
Selected features. The selection of new features is made through
Apply the next join, next rejection, or procedure
Combined, discussed in subsection 2.3. As a criterion you can accept
Fraction fractions determined by the minus method of one element.

Page 81
The hyperplanes that were the basis for the definition of artificial features share
Space of original features on convex areas. But not in each of them finds
The object of the learning set included in the reduced set is designated
Tomeka algorithm. Some of these areas may be empty. As a result of selection
Artificial features, number of pairs of objects corresponding to these features and number
The hyperplanes associated with these features will usually be reduced. number
Areas in the original space will also decrease. However, the areas are still empty
Can occur. If the classified object x is in a non-empty area
The original feature space is its distance to the nearest neighbor in the new
Space will be zero because the values ​​of the artificial properties of both these objects will be
identical. The idea of ​​the above mentioned artificial characteristics, though otherwise used, has been
Proposed in the publication [Jóźwik A., Chmielewski L., Skłodowski M., Cudny W.,
2001].
Example
 The method of applying the artificial feature method is illustrated on the set
The data in Tab.4.7 described by the two characteristics c1 and c2.
 
Tab. 4.7. An example collection for illustration of how to use artificial features
Object Class Feature c1 Feature c2 Feature c3.2 Feature c5.6 Feature c7.6 Feature c1,4
1 1 1,0 5,0 1 1 -1 1
2 2 4,0 2,0 -1 1 -1 -1
3 1 3,0 2,0 1 1 -1 0
4 2 6.0 4.0 -1 -1 -1 -1
5 1 3,0 4,0 1 1 -1 -1
6 2 4,0 5,0 -1 -1 -1 -1
7 1 4.0 7.0 -1 -1 1 0
 
The pairs of points (x3, x2), (x5, x6) and (x7, x6) are pairs of tome pairs because they are spanned on them.
The spheres do not contain any objects in their interiors. So all objects from
These pairs enter the reduced set. A broken line (1) defined by
The 1-NS rule, however, will consist of four straight lines, because
In addition to the above mentioned pairs of objects, para (x5, x2) will also enter its epoch
As a geometric place of points equilateral to objects x2 and x5.
This collection is shown in Figure 4.10.
Based on the selected Tomek algorithm, three pairs of objects (x3, x2), (x5, x6) and
(X7, x6) can be defined by hyperplanes and based on three artificial features: c3,2,
C5,6 and c5,6.

Page 82
 
 
Fig. 4.10. Illustration of an example explaining the method of artificial features.
 
These hyperplanes have been oriented in such a way that, in the corresponding
For pairs of objects, the object of class 1 (cross) was on its positive side, a
Object from class 2 (circle) after negative. Values ​​of new artificial features c3,2, c5,6 and c7,6
See Table 4.7.
Using the minus method one element for a set of 7
Objects and 3 artificial features, you can count the fraction of errors that it is
E1 = 2/7, because misaligned objects will only be objects x2 and x7 (for 1-NS and
Metric city). If selection of created artificial is made
Characteristics, then the selected features will be selected two characteristics: c3,2 and
C7,6, which means that you only need to store four in the computer memory
Objects: x2, x3, x6, and x7. The error fraction for these selected features will be e2 = 1/7,
Because the only misleading class object is object x7. To class 2 (circles)
Only objects x, for which c3,2 = -1 and c7.6 = -1, ie x = [-1, -1], and class
1 (crosses) objects described by the remaining combinations of attribute values, ie, when x = [-
1,1] or x = [1, -1]. The dotted line in Figure 4.10 divides the feature space into two
Decision regions, left or above dashed line (2) class 1 region, and
Right and below this line the class 2 region.
  It is worth noting that the use of the minus method of one element for a set
A learner with primitive features c1 and c2 produces a fraction of errors e0 = 5/7, that is
X
X
X
X
about
about
about
X1
X2 x3
X4 x5
X7
X6
(thirty
1
2
3
4
5
6
7
8
01234567
(2)
(1)

Page 83
Much higher than both fractions e1 and e2 mentioned above and counted for artificial ones
Characteristics. Fractions e1 and e2 can not be used as measures for a reliable 1-NS rating
Acting in the new feature space.
 In the case of a small set, the Tomek algorithm could be dispensed with
Create artificial features for all possible pairs of objects from the opposite classes.
Such couples in the example under consideration are 12. After selection
Only one feature c1,4 would be selected for the artificial features, the values ​​of which are given in
Tab. 4.7. It corresponds to the hyperplane (3) in Figure 4.10. As was already above
The mentioned zero values ​​of the artificial features can be changed to values ​​of 1.
Thus, in the example in question, only one artificial feature would suffice
A consistent set reduced to 1-NS rule, in the original feature space, could
Only contain pairs of objects, objects x1 and x4, with new objects in
Equal distances from each of these two objects should be
Qualified for class 1 (crosses).
Bubble reduction algorithm for reference set
 The essence of acceleration of classification by reference reduction methods is based on
In that the nearest neighbor of the classified object is designated from the smaller
Set of objects, that is, from a reduced set. Presented will be different now
An approach that requires the change of the decision rule itself. Original collection
Reference O can be covered with homogeneous disconnects, in the sense of weak disconnection,
Hyperkids that contain objects of only one class. So these hyperglycics have
Assigned affiliation. The common points of these hyperkineses can only be
Points lying on their spheres. If the classified object x is inside
One hyperkalemia or only on its sphere, it will be attributed to this class of hyperkalemia.
And while he will be lying on the spheres of several hyperkids, he will take the class
Hyperlinked previously created, with a smaller index. Each of the created
Hyperkids may contain multiple or only one object. Where classified
The object will be outside of each of the hyperkines, which will be assigned to the nearest class
Hyperkalemia.
Creating a hyperkernel begins with a randomly selected object x1 for which
The distance r1 to the nearest neighbor y1 from another class is found. The first of
Hyperkul K (x1, r1, l1), where l1 denotes the number of objects contained within it
Or on its sphere, has therefore already been defined. How to create more hyperkids
Can be determined inductively. You should now show how, with already created hypercapes
K (xj, rj, lj), j = 1,2, ..., i, create a hyperkalle K (xi + 1, ri + 1, li + 1). Out of objects
The hypercapes that are not yet covered by the previously created hyperlinks are randomly selected
Object xi + 1, the distance d + 1 is calculated to the nearest neighbor from another class than xi + 1 and
The distance qi + 1 is determined to the nearest hyperkalemia. Radius ri + 1 created

Page 84
Hyperkinesis takes the lower of two values: di + 1 and qi + 1. So the restriction at
Creating another hypercube is the closest object from another class or sphere of another
Hyperkalemia.
The algorithm described above, as one of the few, has been implemented within the framework
Ph.D. thesis [Sierszeń A., 2009]. He did not give satisfactory results.
They were similar to the results of the original rule 1-NS, only in the case of very
Large teaching sets, number of tens of thousands of objects (2 classes + 13 traits).
However, in terms of speed classification this approach has been very much
Effective, also thanks to and that the resulting hyperglycols were descending sorted by
The size of their rays and in that order they were verified or not
Classified object. So the suggestion is that it would be better to sort
Hyperglycemia obtained by the size of the objects contained in them.
The reason for this unsatisfactory effect is in a great part of the fact that this algorithm
It does not result in a consistent set of hyperkinesis. Some objects from the learning set
It may be on the hyperkull spheres representing opposite classes. Otherwise
It is not clear, ie its results are not reproducible.
Hyperkinetic rule
 The algorithm described can be easily modified to remove both
Ambiguity, equip with respect to the original reference set O and
The sequence of produced hyperkinesis is ordered in such a way that the next hyperkalemia in the sequence
It contained as many objects as possible outside the set of objects already covered by
Previously designated hyperglycemia.
 For each xO object, the distance d must be determined to the closest neighbor of
Another class and the distance q to the farthest object of the same class as x, but w
The distances do not exceed d. The hyperkines K (x, r, Z) are created now, of which
Each is defined by the center x, the radius r = (d + q) / 2, and the set of objects Z
They were inside the hypercube or on its sphere. How to create such hyperkalemia
It is shown in Fig. 4.11.
Objects in class 1 (crosses) are located on or inside the sphere with center point
X and radius q. And objects of class 2 (pin) on or inside the ball about the same
The middle and the radius d. Of these hyperkinesis, the first is the hyperkaloma
K (x1, r1, Z1) in which, together with its sphere, there are most objects from the set O,
Ie the hyperglycum with the most abundant Z.
Create another hypercube, just like with a bubble algorithm
Reducing the set of references can be defined recursively, assuming that hyperglycols
K (xj, rj, Zj), j = 1,2, ..., and, have already been created. As a hyperkline K (xi + 1, ri + 1, Zi + 1)
It is chosen from the remaining hyperkines K (x, r, Z) for which the set Z contains

Page 85
 
                        
   Fig. 4.11. Illustration of how to determine the radius of hyperkalemia
 
Maximum number of objects outside the set Si = Z1Z2, ...,  Zi. If by the way
Dialing will detect that one of these remaining hyperkines K (x, r, Z) is not in
Its set From objects outside of the set Si, it is permanently deleted. Number nk
The hyperkull K (xj, rj, Zj) covering the set O will for obvious reasons considerably
Less than the number m of sets O. It is worth noting that hyperglycery containing
Objects of the same class may overlap. The received hyperkinetic string is yes
Ordered to maximize the chance of finding a classified object
In the hyperglyph with the smallest index. If the object is not in any of the
Designated hyperkinesis, it is assigned to the class of hyperkalemia closest.
The distance from the point of hyperkalemia is understood as the distance from this point to the center
Hyperkalemia minus its radius.
 
4.7. Condensation of reference sets
The algorithms presented in the previous section allowed to designate sets
References with smaller numbers than learning sets and their subset.
Reduced reference collections need not be subsets of the learning set, but
Can be points in the feature space created on the basis of the learning set,
Artificial objects. An example of a set of references may be a set of measures
Gravity or median measures for the minimum classifier
Distant. This is also an example of the strongest condensation.
This condensation differs from the reduction that the condensed reference set is not
A subset of the condensed learning set. However, the minimalist classifier
Distant, as already explained in subsection 3.5, may not offer
Satisfactory quality of classification. Classifier 1-NS can have a high overhead
The advantage when it comes to the quality of classification and is more universal because it creates
X r
about
X
R
Q
D
X o
about
O x

Page 86
More complex separation hyperfunction. Hence, he will continue to be
The subject of reflection. The distinction between reduction and condensation was introduced
By the author of this monograph. Other authors distinguish these two ways
Obtain reference sets with reduced numbers, but these concepts
They are used interchangeably as a synonym [Ainslie M.C. And Sanchez J.S., 2002],
[Hart P.E. (1968)].
Chang's algorithm
 The first condensation algorithm was proposed in [Chang C.L., 1974]
And it consisted of combining objects from the same class and replacing them with one new one
Artificial object. The condensed set S is initially equal to the whole set
The learner, ie the original reference set O. Both of these collections must be
Remembered because S will gradually decrease and O will be served
Verification of conformity. In S may be only remembered indexes of objects with O, which
Of course, the set S is consistent, because it can not not be compatible with
By itself, that is, with the set O. Then, the next few objects are determined. A
And b, coming from the same class and temporarily creating a new artificial one
Object c = (a + b) / 2. If you throw objects a and b and replace them with c
Does not eliminate the compatibility of the set S, objects a and b are permanently deleted from the set
A similar operation is performed for the set S: = Sc- {a, b}. I w e n e in
The current set S returns the new pair a and b and is checked
The possibility of replacing this pair with the new c object that was left over
Created. This operation is repeated until the current set S does not
There will be no pair of objects that could be replaced by an average object from it
Designated.
Algorithm of cutting planes
Another idea of ​​condensing the learning set was proposed in the materials
Conference [Jóźwik A., Serpico S. B. and Roli F., 1995] and later described also
Publication [Chen C. H. and Jóźwik A., 1996]. It involves the sharing of the learning set
To the desired number of subsets and replace each of those subsets by means
Gravity and its associated class (ie label) such as the class to which it belongs
Most of its objects. Ambiguous situations can be settled in favor
Class more numerous in the whole set of learners, and eventually classes with less
Index. For the definition of the algorithm, the concept of the diameter of the set was introduced,
Understood as the greatest distance between two objects of the set.
Let nd be the required number of condensed sets S. Original
The set of reference O, ie the set of the learner, will be successively divided into increasingly
More number of subsets Oi until they are nd, ie i = 1,2, ..., nd.

Page 87
The initial condition is: O1 = O, which means that the set O is not yet
divided. The center of gravity s1 of the set O1 is a one-element set
Condensed S1. The center of gravity is assigned to the class it has
Most objects in O1.
 The way to create the next divisions of the set O into subsets can be described
Recurrently, assuming that at some stage nc subsets of the set O, we must
Their number is zoomed to nc + 1. Since they are nc, then O = O1O2 ... Onc, and current
The condensed set S consists of the centers of gravity sj corresponding
Oj, j = 1,2, ..., nc, whose affiliation is like most objects
Suitable subset of Oj. From these subsets a subset of Oi is selected,
It contains objects of at least two classes and whose diameter is the largest. If
There are no more Oj subsets containing objects of at least two classes, then
Only a subset of the largest diameter Oi is selected. Ambiguity can be
Resolved in favor of a subset with a smaller index.
The subset Oi will be divided into two subsets and thus the number
The subsets of O, which make up the set O, will increase from nc to nc + 1. In which
The way to do this will be described in the next paragraph. For further
Considerations, Oi collection, intended for distribution, is convenient to mark additional
Symbol, that is, assume that D = Oi. Thus, the set D is divided into two
Subsets D1 and D2. The subsets of Oj, j = 1,2, ..., nc, are removed
The current subset of Oi, and its place occupies a new subset Oi = D1. Also,
This string is magnified by the new Onc + 1 = D2 element. S condensed collection
Also updated. The former point si is removed, and in its place
The new point si is the center of gravity of the set D1. In addition to this conversion, to
The condensed set S is connected to the center of gravity of the set D2 as the point
Snc + 1.
In the recursive procedure described above, the volume of the condensed set S
It also grows at the same time as the number of subsets of Oj that make up the collection
A. It is possible to control the quality of the classification after each update
Condensed set and stopping the procedure before the size of set S reaches
The given number of nd elements if the fraction of the wrong decision proves to be acceptable. IN
If you give up the quality control of the classification on a regular basis, you just need the items
The set S is determined only after obtaining the subsets Oj, j = 1,2, ..., nd, as means
The gravity of these subsets. Quality control can take place on the basis of
Classifying objects from an O or a separate validation set.
The method of dividing D into subsets D1 and D2 has not been explained yet.
Now, in D we have to find a pair of objects p1 and p2 that are far distant from
each other. On the basis of these objects is constructed hyperplane passing

Page 88
By the center of the section connecting points p1 and p2 and orthogonal to it. With points
The set D, located on this hyperplane or closer to the point p1 formed
Is the set D1 and from the points closer to the object p2 the set D2.
The formal description of the proposed algorithm is as follows:
Definition of the algorithm
1. Determine the desired number nd of objects (points) in the S set;
2. Accept S =  and nc: = 1, nc - current number of subsets of set O;
3. Accept O1: = O;
4. Calculate s1 = center of gravity of the set O1, assign it a class, join it to S and
Accept i = 1;
5. In the set D = Oi find the two most distant points p1 and p2;
6. Divide the set D into two subsets D1 and D2, where d (, ) is the distance function:
 = {PD: d (p, p1) d (p, p1)},
 D2: = {pD: d (p, p1)> d (p, p1)};
O n: = nc + 1, Oi: = D1, Onc: = D2, remove from the current set of condensed S;
8. Calculate the gravity of si and the sets of sets Oi and Onc, assign them classes and join S;
9. If nc = nd, then jump to 14;
10. Assume J1: = {j: jnc and Oj contains objects of at least two classes},
 J2: = {j: jnnc} -J1;
11. Accept J: = J1 if J1 is not empty, otherwise J: = J2;
Determine a pair of points p1 and p2 most distant from each other in each
 From the subsets Oj for jJ and remember the index iJ of the largest Oi
 Diameter;
13. Jump to 5;
14. End.
The presented algorithm can be modified in many ways,
Due to the criterion of typing a subset of O and for division (step 12).
So you can choose the largest diameter collection or the most abundant collection of Oi.
The number of n subsets, that is, the volume of the condensed set does not have to be
Asked. You can continue the splitting procedure from the sets until
Existing subsets containing representatives of at least two classes will be exhausted.
In this case, the volume of the condensed set will be determined
automatically.
The next modification may involve recruiting into a condensed set
S not single center of gravity Oi, but at the same time the center of gravity of each
From the classes represented in the Oi collection. All of the aforementioned options
The modification of the cutting edge algorithm was considered and compared

Page 89
Experiment with each other and with the original version of the 1-NS rule on the nine available
Online data sets [Ainslie M.C. And Sanchez J.S., 2002; Sanchez J.S.,
2004]. It turned out that the best result was obtained when the division is chosen
The most common subset contains objects of at least two classes, and the algorithm is
Continue until the subsets that meet these conditions are exhausted.
Yet another modification [Jóźwik A., Kieś P., 2005], intended to accelerate
The algorithm consists in replacing the split diameter of the set by the distance between
Pair of objects mutually extreme. The objects x and y are mutually extreme, if
Y is the furthest object from x and at the same time x is the furthest object
With respect to y. The method of defining mutually extreme objects is illustrated in Fig
Figure 4.12.
 
         
Fig. 4.12. Illustration of how to design objects that are far farthest away
 
Several points are mutually distant for each object
Appropriate subset. By specifying such a pair for object x2 in Fig
The farthest object, the x4 object, is the object closest to it
Object x4 is object x1, further for x1 the furthest object is x3, and for x3 the furthest
Is x1. A loop has formed, which embraces mutually extreme objects.
The course of the algorithm in its original version is illustrated in Tab. 4.8,
The size of the result set was not set and the procedure ended after gain
Subsets, each containing only one class.
 
Tab. 4.8. Example of a condensation of a reference set
I 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37
II x x x x x o o o
III 1
IV 2 3
V 4 5
VI x x o x o o
 
 




X1
X2
X3
X4
1
2
3
4

Page 90
If the set volume of the condensed set was nd = 6, then the run
The algorithm would be identical. In line I, the coordinate axis scale is given to
You could read the values ​​of the features. The teaching set, shown in row II, which
Is a primitive, ie non-condensed reference set O, consists of five
Objects in class 1 (crosses) and four objects in class 2 (circles).
In the first step of the algorithm, the set O consists of only one subset
O1 = O and the condensed set S contains only one element, ie the center of gravity
S1 = (1 + 4 + 7 + 13 + + 17 + 23 + 27 + 35 + 37) / 9 = 18.2 of the set O1. The class will be assigned to it
Because most of the objects of this collection come from class 1. Now you have to accept
D = O1 and divide the set D into two subsets D1 and D2. Maximum distance
Between objects of set D is 37-0 = 37, which means that the division will occur in
Median numbers from 1 to 37 (coordinates of extreme points), and so on values ​​19.
It is symbolized by the number 1 placed in column 3. The subset of D1 forms objects on
Left from the dividing point, that is 19 on the coordinate axis, and on the subset D2
Consists of objects lying to the right of the point of division.
 The position of the number 1 in the third row indicates the value of the characteristic of the divisor 1.
The reference set O consists of two subsets O1 = D1 and O2 = D2. Current
The condensed set S now has two new elements: s1 = (1 + 4 + 7 + 13 + 17) / 5 = 8.4 and
S2 = (23 + 27 + 35 + 37/4 = 30.5, which are the centers of gravity of sets O1 and O2.
It is assigned class 1, and point s2 is class 2. In the subset to the left
The distance between the two most distant objects is 17-1 = 16, and for the set
Right is 37-23 = 14. So the subdivision should be left subset as
The current set D. The values ​​of the features of its objects extend from 1 to 17, so divide
There will be a median number between 1 and 17, which is 9. The position of the division point indicates
Number 2 in line IV Tab.4.8. The current set of D1 creates objects to the left of the value
9, and the set D2 consists of objects between values ​​9 and 19. Set O1
Is now upgraded and equal to D1 (objects to the left of value 9),
The set O2 remains unchanged (objects to the right of value 19), and the set O3 is equal
Now collect D2.
The condensed collection consists of the following points: new s1 = 4 (center of gravity
Set D1) with assigned class 1, unchanged s2 = 30.5 with class 2 and new
S3 = (13 + 17) / 2 = 15 from class 1, because this class prevailed in Supernatural D.
In a similar way you can continue to divide the set O into smaller and smaller ones
Subsets. The next 3 divisions (row IV, Tab.4.8) would concern the set of objects o
Coordinates to the right of the value 19, and the split value would be the number 30. Breakdown
4 (row V of the table), realized with value 15, would refer only to a subset
Composed of two objects with coordinates 13 and 17. Last division 5 (marked
Also in line V), would be a feature value of 25. Resulted content

Page 91
The condensed set is shown in the VI row of Tab.4.8.
Comparison of both types of algorithms for reducing the reference set, ie reduction
And condensation suggests a solution that is a combination of both approaches. Combination
This produces better results than self-contained condensation when size is required
The set of references does not exceed the predefined number of objects.
Experimental results included in the works [Jóźwik A., Kieś P., 2005] and
[Jóźwik A., 2006] show that it is preferable to use a combination in which
The first stage is condensation and the second is reduction. This combination in
The experiments gave 2 to 3 times smaller error fractions than
Use only condensation alone. The reverse order is much higher
Fractions of the wrong decisions.
The experiments used a data set of 80800 objects, 2
Class and 13 characteristics, of which 4000 were drawn to the learning set and the others
40800 was a test set. Condensation was performed using
Algorithm of hyperplanes cutting and reduction in the first two works mentioned above
Algorithms: Harta and Gowda and Krishna. In the second of the mentioned works
The Gowda and Krishna algorithms were abandoned.
 
4.8. Method of potential functions
The distance measure can be used to define the function K (x, uj) defined as
The potential generated by the point uj at the point x. The potential term has been
Borrowed from physics by association with the electrostatic potential of the charge
Electric. The function K (x, uj) should take the maximum value in the point
X = xj and its value should fall asymptotically to zero as x moves away from
Point xj. The simplest form of such a function has the form: Ka (x, uj) = 1 / [1 + d2 (x, ui)] or
Kb (x, uj) = 1 / [1 + d (x, ui)], where  is the parameter chosen experimentally
During the tuning and testing of the classifier using the functions K (x, uj).
The potential functions K (x, uj), so Ka (x, uj) as well as Kb (x, uj), are shown on
Fig. 4.13.
They can be used to define the discriminative functions gi (x),
I = 1,2, ..., nc, where nc is the number of classes, according to the scheme of Fig.1.3,
Construct a classifier. Discriminatory functions can take on the potential
Summative, or gi (x) = Ki (x) = jK (x, uj), where I (i) is a set of indices
Objects in class i, 1 inc, and j is an integer indicating how many potentials
The k (x, uj) of the object ui consists of the summation potential.
 
 ) (ij

Page 92
  
 
 
  
  
 
 
 
Fig. 4.13. Illustration of the shape of the potential function:
    A) (x, uj) = 1 / [1 +
 
The classified object will belong to the class that corresponds to the largest
Value of gi (x). This is a version of the classifier without learning.
This classifier can be learned by adjusting the potentials
Summative gi (x). The initial summation potential can be identically equal
Zero When verifying whether this classifier correctly assigns a class to each
The object from the learning set proves that for a certain class u object and potential
Summative gi (u) does not get the greatest value, then it can be enlarged by K (x, u)
Ie, make a correction of its magnification: gi (x): = gi (x) + K (x, u). If in
The learning set has no objects of different classes that would be in the feature space
Represented by the same points, then these corrections will lead to total
Separation of objects from the learning set. But not always completely separated
Points from the learning set are desirable, as it can lead to
Classifier override. This means that the results of the rating of the classifier on a separate
The test set may be worse than if the above instruction was interrupted earlier.
The separator surfaces designated by the classifier may be too
Complex, eg single object, even densely surrounded by objects of another class,
This will cause all objects from its near vicinity to be mistakenly assigned to
The class from which he himself originated. For this aspect, which also applies to 1-NS rule,
A note has been noted in the book [Tadeusiewicz R., Flasiński M., 1991].
A good solution would be to divide the dataset,
That is, a set of X objects of known class membership, with three disjoint parts:
The teaching part U, the validation part W, and the test part T. After each correction
The summation potentials gi (x) i = 1,2, ..., nc, calculate the fraction of errors on the set
In and constantly keep in memory the set of these gi (x) functions that they offer
Smallest fraction of errors er0. If several consecutive presentations of the entire set U not
It already leads to the smaller fraction of errors computed on the set W than er0,
This learning classifier can be discontinued. Finally, the fraction of errors must be determined
A) b)
 = 10,20,40
 = 10,20,40 0
1
-101
0
1
-101

Page 93
Using the set T. The collection W would be used for such an approach
Many times, and the function of the summation potential would be collected so that the fraction
The errors obtained for objects in this set were minimal. Rating of the classifier, what
Corresponds to the smallest error fractions on the set W it would not be reliable.
A fair assessment of a classifier requires a separate set of T testers that would be required
Used once.
In case the classification refers to only two classes, it is similar to
Linear discriminative functions described in subsection 1.3, two functions
The discriminative gi (x), i = 1,2, which are now the summation potentials, can be
Replaced with one summation potential:
            G (x) = g1,2 (x) = K (x) = K1 (x) (4.28)
The two-decision case, defined by the relationship (4.28), is worth the attention.
Let (uj) be an infinite sequence of objects from the set U, in
Which each object occurs infinite number of times. This string will be
Corresponded to the sequence of summation potentials Kj (x). Teaching decision rule
The classifier can be defined as follows:
   Kj + 1 (x) = Kj (x) + r K (x, uj + 1), (4.29)
 Where r = 0, where Kj (uj + 1) ≥0 and uj + 1class 1 or Kj (uj + 1) <0 and uj + 1class 2,
           R = 1, g d y Kj (uj + 1) <0 and uj + 1 class 1,
            R = -1 when Kj (uj + 1) ≥0, and uj + 1 class 2
And K0 (x) 0.
Sample calculations will be made for potentials K (x, uj) = Ka (x, uj) =
1 / [1 + d2 (x, ui)] and the coefficient  = 1.
Example
 The operation of the learning algorithm according to rule (4.29) can be illustrated for data
One dimensional figures shown in Figure 4.14.
 
Value of feature 0 1 2 3 4 5 6 7 8 9 10 11
Class x x o x o o
The object u1 u2 u4 u3 u5 u6
Fig. 4.14. Data set for illustration of learning by means of the potential function
 
 These potentials for individual objects are as follows: K (x, u1) = 1 / [1+ (x-2) 2],
K (x, u2) = 1 / [1+ (x-5) 2], K 1 + (x-6) 2], K (x, u5) = 1 / [1+ (x-
10) 2] and K (x, u6) = 1 / [1+ (x-11) 2]. Presentation of objects u1, u2, u3 does not change the value of  1 (Ij 

2 (Ij  1j  1j

Page 94
The sum potential, ie K3 (x) = K2 (x) = K1 (x) = K0 (x) = 0. But the potential of K3 (u4) = 0, a
Should be negative because u4 is from class 2. So to the potential of K3 (x) should be added
The potential of K (x, u4) = 1 / [1+ (x-6) 2], that is, K4 (x) = K3 (X-6) 2]. For
Next objects u5 and u6, which are of class 2, potentials K4 (u5) = - 0,059, K4 (u6) = -
0.38 are negative and no correction is needed, ie K6 (x) = K5 (x) = K4 (x). Only the second
The presentation of objects from the learning set will require correction:
K6 (u1) = - 0,059 = <0K7 (x) = K6 (x) 2) 2],
K7 (u2) = - 0.400 <0K8 (x) = K7 (x) + 2] + 1 / [1+ (x-5) 2],
K8 (u3) = - 0.073 <0K9 (x) = K8 (x) + 2] + 1 / [1+ (x-5) 2] +
K9 (u4) = 0,241 <0K10 (x) = K9 (x), K10 (u5) = 0.195> 0K10 (x) = K9 X) -
- 1 / [1 + (x-5) 2] + 1 / [1+ (x-2) 2] + 1 / [1+ (x-8) 2] 1 / [1+ (x-10) 2].
The summation potential K (x) = K10 (x) is already non-negative for objects of class 1 and
Negative for objects in class 2: K (u1) = 1.053, K (u2) = 0.662, K (u3) = 0.727, K (u4) = -
0.300, K (u5) = 0.805, K (u6) = - 0.399. The potential of the u6 facility was unnecessary.
Using the above learning method for functions Kb (x, uj) = 1 / [1 + d (x, ui)] can
Require correction for other objects than using the function Ka (x, uj) = 1 / [1 + d2 (x, ui)].
And so it happens, as illustrated in Figure 4.15.
 
I u u u u u u u u u u u u u u u u u u u u u u
IIa      17
IIb      21
Fig. 4.15. Data set for illustration of learning by means of the potential function
 
In row I, there is a string of objects from the learning set. Content
Line IIa indicates that learning, in this case lasting until correct
The separation of the whole set of learners using the Ka (x, uj) potentials has ended
After five corrections. They were necessary in turn for objects u4, u1, u2, u3, and u5, for
The drawing was marked with an asterisk.
Learning ended only after 17 presentations of objects from the learning set,
Because after obtaining the separation of the set, the algorithm had to perform six more
Presentations that do not require correction to be successful.
The use of potentials Kb (x, uj) also required five corrections, but slightly different,
What can be seen from the content of poem IIb. The number of required presentation objects of the string
The instructor was now 21. Numbers 17 and 21 were in Figure 4.15 shown under
The meaning of objects on which learning was completed.

Page 95
 The shape of potentials K (x) formed once of the potential functions Ka (x, uj) i
The second time from Kb (x, uj) is shown in Figure 4.16.
 
  and)
                  
B)
                  
 Fig. 4.16. Illustration of summation potentials for functions: a) Ka (x, uj) and b) Kb (x, uj)
 
In both cases, the potentials of the same sums up
Five objects, although the course of learning was different, but it does not have to be a rule for others
Data.
The presented version of the potential function method is determined by its
The authors [Ajzierman M.A., Bravierman E.M., Rozonoer A.I., 1970] as the embodiment
Machine. The second of the two described in the aforementioned work of method implementation was named
Perceptronic. It is very similar to the error correction algorithm described in
Subsection 3.1, except that the features in the new space. This implementation does not
It already has a minimum distance character.
The valuable advantage of the potential function method with respect to the k-NS rule is that,
They do not need to choose the nearest neighbors. The closest neighbors do anyway
K (x)
K (x)
X
X
X x
X x
X
X o
O o o
O o
U1 u6
U6 u1 u2 u3 u4
U3 u2 u4 u5 -1,2
-0.8
-0.4
0,0
0,4
0,8
1,2
0,83456789101112131415 -0,8
-0.4
0,0
0,4
0,8
1,2
0123456789101112131415

Page 96
The biggest voice in a more natural way, because their potential will be the strongest
Influence on the value of the summation potential.
In the k-NS rule, the distance between a classified object and all
The objects of the learning set must be calculated. In contrast, in the function method
Potential is not unnecessary. Counted must be distances only to those
Objects whose presentation caused the need for potential correction
Summative. Most learning objects may not participate in
Aggregation potential of the discriminating functions of the classifier.
A serious disadvantage of the potential function method is the dependence of the potential
Summative to the ordering of the learning set.
 Contrary to the k-NS rule for the method of potential learning functions
It would be costly to implement a method minus one element for evaluation
Quality of classification. On the other hand, there is no problem with the minus one element method,
If you use the potential function without learning method.
 There are numerous areas of application recognition methods in which the meeting
A sufficiently large number of objects of known affinity is difficult, so that
Due to the costs and the time needed for its assembly. This is happening
Mainly in biomedical applications and in ecology. That's why these collections are worth
Use in the most effective way possible. Proposed new approach
The construction and quality assessment of the classifier includes the next subsection.
 
4.9. New classification scheme rating scheme
Subsection 2.1 discusses methods for evaluating the quality of classification. On the occasion
Discussing the condition of retention of the classifier's teaching on the basis of
Decision rules using summative potentials have been suggested
An additional set of W called validation. Next summative potentials during
The learning was based on the objects from the learning set U, and the choice was made
One of these potentials was made using the validation set W.
The final rating of the classifier was made using a test set
T, used exactly once. Such a scheme of construction and assessment of the classifier
This is shown in Table 4.9, in Row I. Application of the minus one method
Element, would be very expensive. It would require a lot of learning sessions, that is
Determine the summation potentials Ki (x) for each of the classes i, i = 1,2, ..., nc, what is
The size of the learning set.
This defect does not occur in the case of the k-NS rule, because it is quite easy
Implemented method minus one element. Therefore, the set of objects
With the known affiliation it is enough to divide into two parts, the part that teaches U = {uj} mj1 wyst

Page 97
 And the test part T. To determine the number k k closest offering neighbors
The smallest fraction of errors, such fractions count for all possible
The values ​​k, k = 1,2, ..., m-1.
 
Tab. 4.9. Summary of selected design schemes and rating of classifiers
Poem
Table
Quality assessment method
Classification
Set
structural
Collection for evaluation
In the learning phase
Collection of evaluation
Final
And Met. Validation set U W T
II Met. Minus one U U T element
III Met. Minus one element X U T
 
Normally, in the designation k phase, ie learning the k-NS classifier, each
The object ui is classed with the k-NS rule with the set Oi = U- {ui} as
This requires the method minus one element. The nearest neighbors are searched in
Oi collection. Once the k-value has been determined, the classifier's error fraction can be evaluated
Classifying each object from the test set T with the k-NS rule with the reference set
O = U, ie the nearest neighbors are designated from the set U. Such behavior has been in
Tab.4.9 marked as scheme No II.
 The next scheme, proposed by the author of this monograph, marked in
Tab. 4.9, as a schematic no. III, will more conveniently be described using the set X = UT. This
Together, the fraction of errors is also evaluated on the set U, but in the procedure minus one
Each element of uiU is classed as a k-NS rule with a reference set
Oi = X- {ui}. Final testing of the classifier is currently underway
Method minus one element, which did not take place in Schemes I and II. Otherwise
Saying, if T = {tj}, then each tiT object is classified according to the k-NS rule with
The set of references Oi = X- {ti}. This is therefore an estimate of the fraction of errors for
The classifier that will be used with the reference set X rather than its
This is a subset of U. This should translate into a higher classification quality.
  
5. Summary and perspectives
Preferred properties of classification methods
 The wish of every author is to make his work as profitable as possible
The largest number of people. And if it contains a description of his methods, then
Also great satisfaction will be found when they are implemented, a
Then applied. These premises had a decisive influence on directions
Research conducted by the author, and consequently, and on the selection featured in
Monographs of methods. In order to meet the request can meet, in addition to simplicity, mtj1

Page 98
The most important is the effectiveness of the proposed methods. Both of these properties do not have to
Mutually exclusive. The simplicity of the methods makes it easier for the author to collaborate with
Professionals from other fields where they can find application. Well
The associated ease of implementation makes it easy to write a program that will
Free from serious mistakes, and if you do such a program will happen, then
It will be easier to locate. The author hopes that suggested in
The monograph of the method, which is the result of his research, is characterized by a large degree
Above mentioned properties. Below, the most important will be described
Features of some of the methods employed in the work.
Methods already published by the author
Of the methods already published by the author and included herein
Monographs, most applications have found the fuzzy rule k nearest neighbors
[Jóźwik A., 1983b], as a result of his cooperation with chemists
Physical [Lesiak B., Jabłoński A., Zagórska M. and Jóźwik A., 1988; Lesiak B.
And Jóźwik A., 2004; Lesiak B., Biliński A., Jóźwik A., 2005]. One of its advantages is
Classification quality, which is often higher than the standard k-NS rule.
The dubbed version of the k-NS rule generates a standard k-NS rule. Sam
The idea of ​​this rule was created through collaboration with people involved in spectroscopy
Electronically. There were jobs that could not be standardized
The k-NS rule, or any other method of acute classification, ie not fuzzy,
Recognition of metal alloy shares, as already mentioned in the chapter
Fourth.
The dubbed version of the k-NS rule was also frequently used in the issues
Biomedical co-author with biologists and doctors [Sokołowska B.,
Jóźwik A., Pokorski M., 2003]. Motivation for its use instead of version
The standard, was the higher quality of classification. In addition, in all
Applications, particularly biomedical, were the problem of selection of features.
It was carried out using the error fractions, as a criterion, calculated by the method
Minus one item. This method is very convenient to use in conjunction with
The k-NS rule, determining the optimal number for each of the combination combinations evaluated
K. The rule discussion was thoroughly analyzed by other authors in the publication
[Bezdek J.C., Chuah S.K., Leep D., 1986], who also noticed its usefulness,
Especially in the case of small learning sets. It is worth emphasizing that the fan
The tasks in which both versions of the k-NS rule, standard and fuzzy, may be useful
It is very broad, and their assumptions are rather weak.
The next published method proposed by the author, having the character
Universal, that is, can be used in many different classification tasks, is

Page 99
Method of classifier construction consisting of class areas and
Areas of overlapping classes [Jóźwik A., Serpico S. and Roli F., 1998]. Same as
The k-NS rule, classifier using class areas, can be important
Practical, mainly in situations where other classifiers, including the k-NS classifier,
They offer a high probability of misleading decisions, making themselves useless.
Separating the overlap of classes makes it possible to build in this situation
A useful classifier, because some objects outside the class overlap
Will be classified with acceptable credibility [Jóźwik A., Stawska Z.
Grabowski M., Filipiak K., Rudowski R., Opolski G., 2003].
Multi-stage classifier [Jóźwik A. and Stawska Z., 2000], built from
The use of class areas also allows for accelerated classification. Much of it
Classifying objects that will be within only one class will remain
A quick classification, and a slightly more complex rule k-NS or its fuzzy version,
It will only apply to more difficult objects at the final stage of classification.
Acceleration, as compared to the standard k-NS rule,
That the optimal k is limited to the overlapping area
Classes. In addition, it can offer a better quality of classification, because the parameter k,
That is, the number of nearest neighbors will be selected only for the part of the learning set,
Contained in the area of ​​overlap of classes.
Further published methods and in this monograph,
Developed by its author, referred to the reduction and condensation of collections
Reference for Rule 1-NS. Among the reduction algorithms, this was an object algorithm
Mutually Coming [Jóźwik A., Kieś P., 2005]. In contrast to the condensation of the harvest
Reference was made to the algorithm of cutting planes [Jóźwik A.,
Serpico S. B. and Roli F. 1995; Chen C. H. and Jóźwik A., 1996]. Both reductions
As well as condensation are particularly useful for large data sets and were
Already used by the author in the case of teaching collections of the order number
Several thousand, and some of them even for collections containing tens of thousands
Objects.
This monograph also contains a proposal already published by the author
[Jóźwik A., 2004], more efficient use of a collection of known objects
Affiliation to classes. This is a new grading scheme that can
Be particularly useful for small learning sets. Calculated according to
He fractions of errors should be more accurate because they are set to
The basis of the more numerous reference sets, including the teaching and testing set,
Ie their sums.
The methods mentioned above are universal, ie they are not required
Meet rigorous assumptions by learning sets, as is the case in

Page 100
Case of algorithms that use separating hyperplanes.
Determination of dividing hyperplanes will not be useful if distributed
Objects from the learning set will be too complex, eg when objects of one class will be
To be inside a certain hyperkernel, and objects of another class evenly
Spread out of this hyperkalemia. But in spite of this, the designation of hyperplanes
Separation is worth the interest, because often there are learning sets,
For whom this approach may prove to be effective.
Methods already published by the author of this monograph should also be used
Recursive algorithm for studying the linear distributions of two sets proposed in
Two different versions, once using the solution of the system of equations
Linear method of elimination of variables, and the second time using orthogonalisation. Both of these
Versions [Jóźwik A., 1983a; 1998a] differ in the ways of designating the first
Approximation of the solution sought, ie the starting hyperplane.
Methods not previously published by the author
The monograph also contains new methods, not yet published. To them
The corrected k-NS rule, which may be of great practical significance, especially in
The case of the simultaneous application of standardization of features standardization, error counting
Classification and matrices. The main application of this rule may be
Construction task of the classifier, not only in the case of learning sets with
Missing attribute values, but also tasks in which the proportion of class size
The teaching set does not correspond to reality.
The new idea is the hyperkinetic rule, as a method of reducing the set of references along with
Simultaneous change of decision rule, defined in this paper as a rule
Hyperkin It is based on covering the set of learners with hyperkines that may overlap
Over each other if they belong to the same class. All balls are homogeneous, that is, each of them
They contain only one class. Area of ​​feature spaces covered by
One class hyperglyc is disjointed with the area of ​​the feature space covered by
Hypergroups of any other class. Its effectiveness is very dependent on the distribution of classes in
The set of references and certainly is very sensitive to outlying objects, ie
Individual objects surrounded by objects from another class. You can do this by editing
The teaching set, for example, removing all objects that are closest to it
The neighbor has an object from another class. This operation can be repeated many times. Set
In this way the instructor will be purified from standing objects.
The previously recursive algorithm of two linear distributions has been developed
The harvest was completed with a new stage. He himself is the first step and allows
Find a hyperplane that corresponds to a low resolution or detect that such
Hyperplanetary does not exist. If the first step gives an affirmative answer, it is added

Page 101
The stage will allow you to investigate whether the examined collections are separate with the specified clearance.
Assuming a very small value of the clearance, you can decide whether the examined collections are
Strictly linear. By contrast, using only the second stage several times and selecting
Different ground clearance values ​​can be defined by the proximity hyperplane
Optimal.
The new result is also to propose an algorithm for constructing a classifier
Based on a set of learners edited for linear resolution.
 This monograph presents a modified idea
Use artificial features in application to reduce harvest size
Reference for the nearest neighbor rule. In the previous version, selection was used
Artificial features to reduce the reference set used in the original
Space of qualities. In the new version the classifier works in the space of artificial qualities,
That is, any newly classified object must be converted to a new one
Space of qualities. The advantage of this approach is the higher quality of classification at
A slight computational cost resulting from classed conversions
Objects to a new feature space.
Planned further study
1. The problem of the construction of the classifier, in the case of a learning set with the missing
Character values ​​can be better resolved if applied
A parallel structure composed of two-decimile classifiers, each of which
Will work according to the corrected rule k-NS. Expected to be fewer
Objects from the learning set that would not participate in constructing the classifier.
Improving the quality of classification seems to be intuitively obvious, but it's worth it
Plan a series of experiments, several available on the Internet and varied
Data sets. These experiments could be carried out for different ratios
Number of objects in which there were missing characteristic values ​​in relation to size
Learning set.
2. In the parallel structure of Fig. 1.2, the voting of the component classifiers may
Rely on counting votes as an outcome of acute or fuzzy classification in which
The voices of the closest neighbors in the classifiers may be dispersed
Between all the classes considered. It has not yet been resolved which of these
Two types of voting, assuming that it is ultimately a classifier
Producing sharp decisions, offers smaller fractions of error.
3. The method of determining class areas based on relations 4.26 and 4.27 with certainty
Can be improved. It is worth thinking about other, though similar, rules
Defining class areas. It is not excluded that instead of calculating ei values ​​according to the formula

Page 102
4.26, which is equal to the largest distance to the nearest neighbor of the same class,
It would be better to use the mean of such values ​​calculated for k most
Distant closest neighbors, which would reduce the size of the area
Classes and sizes of areas of overlapping classes, and consequently, and numbers contained in
Their objects. Besides, this way of defining class areas would be less sensitive
For the presence of atypical objects. Of course, also 4.27 should be dropped
Similar modification. Verification of the effectiveness of this possible modification
Patterns 4.26 and 4.27 could also be carried out experimentally on collections
Internet, so that you can compare the obtained results with already existing for those
Collections in literature.
4. As for the reduction of the reference set with the use of artificial features, then
You might want to consider other procedures than the Tomek algorithm for extraction of artificial features.
It would also be useful to consider whether in the case of a newly classified object
It will be in an empty area, as you can tell if the distance to it
The nearest neighbor will be bigger by zero, not worth the same class,
What is the nearest non-empty area.
5. Another subject of research may be the use of artificially created
Features in the algorithm of cutting planes, where this algorithm will be
Used with the automatic result set option.
Each of the subsets obtained would be homogeneous, because it would contain objects only
One class. The hyperplanes used in this algorithm can be used to
Define artificial features in the same way as it was suggested in
Subsection 4.6. Cutting divisors divide the space of features into homogeneous or
Empty areas in the feature space. The object would then be assigned such a class,
The origin of the area in which it is located. If it is in
The empty area should be assigned to the class of the nearest area. IN
An existing version of the algorithm of the class decides the closest center of gravity
Appropriate subset.
6. Another modification of the cutting edge hyperplane algorithm, which should be investigated,
It may consist in applying another way of dividing the created subsets into
The result of dividing the cutting planes. For two classes
Cutting edge hyperlinks could be constructed not based on pairs of objects
The farthest or most distant (which is not the same) but a couple of means
The gravity of the classes computed for the objects inside the target
To split another subset. In tasks with more classes, hyperplanes
Cutting operators could separate the largest class of a subset from the other shared classes
Subset, in the same way as in the situation of two classes.

Page 103
7. The rule of hyperkineses, first described in this monograph, as already mentioned above
It can be used to simultaneously reduce the set of references and select features.
Its criterion would be the number of designated hyperkids covering the entire set of instructors,
After rejecting unnecessary hyperkids, in relation to the size of the learning set.
Worthy experiments is to consider the dependence of the number of received hyperkids from
The kind of distance measure used.
8. There are also possibilities for modifying a recursive resolution analysis algorithm
Linear two sets in such a way that in the case of strict linear resolution
The studied sets were designated by a pair of separating hyperplanes
Far from each other distant, that is, ultimately the hyperplane spreading from
Maximum margins. Likewise, it is worth exploring the possibility
Create such modifications so that you can test the linear resolution with the overlay,
As was the case with the error correction algorithm discussed in
Subsection 3.1. While the correction algorithm could be used rationally independently
Whether the examined sets are linear or not, the recursive algorithm would be
He could fulfill his task, he must ensure that he got the right solution,
That is, a separating hyperplane, if any. So, they are not a problem
Definitions of both the above suggested modifications, but the proof of them
Convergence when actual linear resolution with ground clearance and with overlay respectively
takes place.
9. Algorithm for editing the learning set for linear separation from subsection 3.5
It sequentially selects the objects of the learning set to be removed in the end
Possibility of determining the dividing hyperplane, but among the hyperplanes
Perpendicular to the section joining the centers of gravity of classes already calculated for
Minor collections. It can not be ruled out that it would be better to study separation
After every reduction, any of the studied collections, even o
One object. This would lead to a reduction in the number of removals
Objects.
10. Fractional errors, defined by the minus one method, can be used to
Determine the optimal number k in the k-NS rule. In this procedure every object u
The learning set U is classed on the basis of the reference set O = U- {u}
Composed of other objects. This increases the chance for you to come in
Neighborhood was an object from one of the opposite classes. The error fractions can thus be
Inflated. The experiments described in [Jóźwik A., 2002] have shown that
Optimal k values ​​in the k-NS rule, based on the error fractions
Determined by the validation set method was higher than in the case of fractions
Errors determined by the minus one method. One of the reasons for this

Page 104
The phenomenon may be the above fact of overestimating the fraction of errors, which makes in
Method minus one element smaller values ​​of error fractions were obtained
For smaller k values. Indeed, fractions of errors received for the method
Minus one element was higher. The experiments were carried out on small
Artificially generated learning sets, two-dimensional and containing in
One series of 10 objects and the other of 40 objects of the class. Teaching sets and
Validation was equivocal. Doubts are aroused by the fact that the proportions between
The average k-values ​​determined by the validation set method and the method
Minus one element were almost the same for both of the aforementioned series of experiments. Ta
The issue still requires reflection and re-experimentation verification
With the use of available online collections.
 
 
6. Quoted literature
Ainslie M.C. And Sanchez J.S. (2002), Space partitioning for instance reduction in lazy
Learning algorithms, In 2nd Workshop on Integration and Collaboration Aspects of
Data Mining, Decision Suport and Meta-Learning, pp. 13-18
Ajzierman M.A., Bravierman E.M., Rozonoer A.I. (1970), Menopause
Features in the masturbation tie, Science
Bayes T. (1763), An essay towards solving a problem in the doctrine of chances,
Philosophical Transactions of the Royal Society, vol. 53, pp. 370-418
Bezdek J.C., Chuah S.K., Leep D. (1986), Generalized k-NN rule. Fuzzy sets and
Systems, vol. 18, pp. 237-256
Bobrowski L., Niemiro W. (1984), A method of synthesis of linear discriminant
Function in the case of nonseparability, Patterns Recognition vol. 17, no. 2, pp. 205-
210
Carpenter, G., Grossberg, S. (1996), Learning, categorization, rule formation, and
Prediction by fuzzy neural networks, in the book "Fuzzy logic and neural network
Handbook, edited by C.H. Chen, McGraw-Hill Series on Computer Engineering,
New York, pp. 1.3-1.45
Cendrowska D. (2005), Construction of object classifiers using
Algorithm for studying the linear distribution of two sets, PhD thesis
Defended at the Institute of Fundamental Technological Research of the Polish Academy of Sciences, promoter: W.
Kosiński
Chang C.L. (1974), Finding prototype for nearest neighbor classifiers, IEEE
Transactions on Computers, vol. 23 (Corresp.), Pp.1179-1184
Devijver P.A., Kittler J. (1982), Pattern recognition: A statistical approach, Prentice
Hall, London.

Page 105
Duda R.O., Hart P.E., Stork D.G. (2001), Pattern classification, Wiley Interscience,
New York
Fix E., Hodges J.L. (1952), Discriminatory analysis: nonparametric discrimination
Small sample performance, Project 21-49-004, report number 11, USAF school
Of aviation medicine, Randolph Field, Texas, pp. 280-322
Gates G. W. (1972), The nearest nearest neighbor rule, IEEE Transactions
 On Information Theory, vol. 18 (Corresp.), Pp. 431-433.
Chen C. H. and Jóźwik A. (1996), A sample set of condensation algorithm for the class
Sensitive artificial neural network, Pattern Recognition Letters, vol. 17, pp.
819-823
Gowda K. C. and Krishna G. (1979), The condensed nearest neighbor rule using the
 Concept of mutual nearest neighborhood, IEEE Transactions on Information
Theory, vol. 25 (Corresp.), Pp. 488-490.
Grabowski S. (2003), Construction of the minimum-distance classifiers
 About network structure, dissertation dissertated by AGH, promoter: D.
Sankowski
Hart P.E. (1968), The condensed nearest neighbor rule, IEEE Trans. On Information
Theory, vol. 14 (Corresp.), Pp. 515-516
Ho Y.C., Kashyap R.L. (1965), An algorithm for linear inequalities and its
Application, IEEE Transactions on Electronic Computers, v. 14, pp. 683-688
Jajuga K. (1990), Statistical Theory of Image Recognition, PWN, Warsaw.
Jóźwik A. (1981), A double stage algorithm for the investigation of linear separability
Of two sets in pattern classifying problems, IV Polish-Italian Symposium, Ischia,
1978, Quaderni de La Richerca Scientifica, v. 108, pp. 41-45
Jóźwik A. (1983a), A recursive method for the investigation of linear separability
Of two sets, Pattern Recognition, vol. 16, no. 4, pp. 429-431
Jóźwik A. (1983b), A learning scheme for a fuzzy k-NN rule, Pattern Recognition
Letters 1, pp. 287-289
Jóźwik A. (1994), Pattern recognition method based on k nearest neighbor rule,
XLV, July-August, pp. 27-29
Jóźwik A. (1998a), Algorithm for studying the linear distribution of two sets
And its perspective on the use of classifiers, VI Conference
"Networks and Information Systems - theory, projects, implementations", Łódź, October
1998, Conference Materials, pp. 311-316
Jóźwik A. (2002), Examination of the properties of two methods of assessing the quality of classifiers
Type of k-closest neighbors, "Materials X Network Conferences and Systems
Informatics, pp. 537-548
Jóźwik A. (2004), New Classifier Testing Scheme, Materials XII
Network Conference and Information Systems, Łódź, pp. 425-430

Page 106
Jóźwik A. (2005), Minimal-distances and other methods of construction of classifiers
Linear-line, Inst. Biocybernetics and Biomedical Engineering No. 64.
Jóźwik A. (2006), Combining reference set of condensation and reduction algorithms
For controlling the compromise between the set reference and size
Quality, Materials XIV Conference Network and Information Systems, Łódź, p. 213-
215
Jóźwik A., Chmielewski L., Skłodowski M., Cudny W. (2001), A proposition of the
New space and its use to construction of a fast minimum distance classifier,
In the book Computer Recognition Systems (II Conference papers
KOSYR2001), Wrocław, pp. 381-386
Jóźwik A., Janecki J. and Demczuk M. (1998), A multistage NN type classifier based
On class overlap minimization and its application to cardio-circulatory events
Prediction, Proceedings of the IV National Conference on Biocybernetics
And Biomedical Engineering, Zwierzyniec, September, pp. 47-51
Jóźwik A., Kieś P. (2005), Reference set reduction for 1-NN rule based on finding
Mutually nearest and mutually furthest pairs of points, Advances in Soft
Computing, Computer Recognition Systems, Springer-Verlag, Berlin-Heidelberg,
Pp. 195-202
Jóźwik A., Serpico S., Roli F. (1998), A parallel network of modified 1-NN and k-NN
Classifiers - application to remote-sensing image classification, pattern
Recognition Letters 19, pp. 57-62
Jóźwik A., Serpico S. B. and Roli F. (1995), Condensed Version of the k-NN rule
Remote sensing image classification, Image and Signal Processing for Remote
Sensing II, EUROPTO Proceedings, SPIE, vol. 2579, pp. 196-198
Jóźwik A., Serpico S. and Roli F. (1998), A parallel network of modified 1-NN and k-
 NN classifiers -application to remote-sensing image classification, Pattern
 Recognition Letters 19, pp. 57-62
Jóźwik A. and Stawska Z. (2000), multi-stage classifier closest to the neighbor
 From each class, Materials VIII Conference "Networks and Information Systems", Łódź,
Pp. 339-346
Jóźwik A., Stawska Z. Grabowski M., Filipiak K., Rudowski R., Opolski G. (2003),
Distance based classifiers and their use to analyze related data
Coronary syndromes, in the book Computer Recognition Systems (papers
3rd Conference KOSYR2003), pp. 369-375
Jóźwik A., Vernazza G. (1988), Recognition of leucocytes by a parallel k-NN
Classifier, Lecture Notes of the ICB Seminar, pp. 138-153
Keller J.M., Gray M.R., Givens J.A., (1985), A fuzzy k-nearest neighbor algorithm,
IEEE Trans. On Systems Man and Cybernetics, vol. SMC-15, pp. 580-585.
Kohavi R., (1995), A Study of Cross-Validation and Bootstrap for Accuracy Estimation
And Model Selection, http://robotics.stanford.edu/%7Eronnyk/accEst.pdf

Page 107
Koronacki J., Ćwik J. (2005), Statistical learning systems, WNT.
Koziniec B.N. (1973): Riekurientnyj algoritm convection of two convex
Oblivion, in Algoritma learning about the images, (edited by
W. N. Vapnik), Soviet Radio, Moscow, pp. 43-50 (in Russian)
Kurzyński M. (1997), Object recognition. Statistical methods. Annexe
Publishing House of Wroclaw University of Technology.
Lachenbruch P. A. (1965), Estimation of Error Rates in Discriminant Analysis, Ph.D.
Dissertation, University of California, Los Angeles, Chapter 5.
Lesiak B., Biliński A., Jóźwik A. (2005), Segregation in CuPd alloys studied by x-ray
Photoelectron spectroscopy using lineshape analysis and the fuzzy k-nearest
Neighbor rule, Polish J. Chem. 79, p.1365
Lesiak B., Jabłoński A., Zagórska M. and Jóźwik A. (1988), Identification of
Synthetic metals from the shape of the carbon KLL spectra by pattern recognition
Method, Surface and Interface Analysis, vol. 12, pp. 461-467
Lesiak B. and Jóźwik A. (2004), Quantitative analysis of AuPd alloys from the shape
Of XPS spectra by the fuzzy rule, Surface and Interface Analysis, vol. 36, pp. 793-
797
Mangasarian O.L. (2000): Generalized Support VectorMachines, Advances in Large
Margin classifiers, MIT Press, available at ftp://ftp.cs.wisc.edu/math-
Prog / tech-reports / 98-14.ps, pp. 135-146
Nilsson N. (1965), Learning machines, McGraw-Hill, New York.
Raniszewski M. (2009), Methods of strong reduction and editing of a reference set for a rule
The closest neighbor, PhD thesis, defended at the Technical University of Lodz (KIS),
Promoter: A. Jóźwik
Rychlik T. (2012), Modified two-decision classifier with standardization, Work
MA, defended at the University of Lodz (WFiIS), promoter A. Jóźwik
Sanchez J.S. (2004), High training set size reduction by space partitioning
 And prototype abstraction, Pattern Recognition 37, pp. 1561-1564
Siedlecki W. (1994), A formula for multi-class distributed classifiers, Pattern
Recognition Letters, Volume 15, Issue 8, August 1994, Pages 739-742
A. siege (2009), Methods of condensing the reference set for decision rules
Based on distance functions, Ph.D. thesis, Lodz University of Technology (KIS),
Promoter: A. Jóźwik
Sokołowska B., Jóźwik A., Pokorski M. (2003), A fuzzy-classifier system to distinguish
Respiratory pattern envolving after diaphragm paralysis in cat, Japanese Journal of
Physiology, vol. 53, pp. 301-307
Stąpor K. (2005), Automatic classification of objects, EXIT, Warsaw
Sturgulewski Ł. (2008), Algorithms for the study of strict linear separability of two
Ph.D. thesis, Technical University of Lodz (KIS), promoter: A. Jóźwik

Page 108
 
Tomaszewski W.P. (2013), a face recognition system based on
O Modified multi-descriptive minimum-distance classifier, Work
Engineer, made at WSKSiM, promoter: G. Osiński
Tadeusiewicz R., Flasiński M. (1991), Recognition of Images, PWN, Warsaw
Tomek I. (1977), Two modifications of CNN, IEEE Trans. Systems, Man,
 And Cybernetics, vol. 7, no. 2, pp. 92-94
Toussaint G. T. (1994), A counterexample is Tomek's consistency theorem for
A condensed nearest neighbor decision rule, Pattern Recognition Letters, vol.15,
Pp.797-801
Vapnik V.N. (2000), The Nature of Statistical Learning Theory. Springer, New York.
Vapnik V.N., Chervonenkis A., J. (1974), Theory of image recognition, Science,
Moscow (in Russian)